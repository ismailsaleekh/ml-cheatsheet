{
  "version": "1.0.0",
  "lastUpdated": "2025-12-31",
  "categories": [
    {
      "id": "foundations",
      "name": "1. Foundations",
      "description": "Core ML concepts and learning paradigms",
      "icon": "BookOpen",
      "color": "blue",
      "order": 1
    },
    {
      "id": "data-foundation",
      "name": "2. Data Foundation",
      "description": "Data types, preprocessing, and feature engineering",
      "icon": "Database",
      "color": "green",
      "order": 2
    },
    {
      "id": "learning-problem",
      "name": "3. Learning Problem",
      "description": "Loss functions, bias-variance, and generalization",
      "icon": "Target",
      "color": "orange",
      "order": 3
    },
    {
      "id": "optimization",
      "name": "4. Optimization",
      "description": "Gradient descent and advanced optimizers",
      "icon": "TrendingUp",
      "color": "yellow",
      "order": 4
    },
    {
      "id": "regularization",
      "name": "5. Regularization",
      "description": "Techniques to prevent overfitting",
      "icon": "Shield",
      "color": "purple",
      "order": 5
    },
    {
      "id": "evaluation",
      "name": "6. Model Evaluation",
      "description": "Metrics for measuring model performance",
      "icon": "BarChart",
      "color": "teal",
      "order": 6
    },
    {
      "id": "supervised-regression",
      "name": "7. Regression",
      "description": "Supervised learning for continuous predictions",
      "icon": "TrendingUp",
      "color": "blue",
      "order": 7
    },
    {
      "id": "supervised-classification",
      "name": "8. Classification",
      "description": "Supervised learning for categorical predictions",
      "icon": "Tags",
      "color": "green",
      "order": 8
    },
    {
      "id": "ensemble-methods",
      "name": "9. Ensemble Methods",
      "description": "Combining multiple models for better predictions",
      "icon": "Users",
      "color": "purple",
      "order": 9
    },
    {
      "id": "neural-networks",
      "name": "10. Neural Networks",
      "description": "Foundations of deep learning",
      "icon": "Cpu",
      "color": "orange",
      "order": 10
    },
    {
      "id": "deep-learning",
      "name": "11. Deep Learning",
      "description": "Advanced neural network architectures",
      "icon": "Layers",
      "color": "red",
      "order": 11
    },
    {
      "id": "unsupervised-learning",
      "name": "12. Unsupervised Learning",
      "description": "Learning patterns from unlabeled data",
      "icon": "GitBranch",
      "color": "teal",
      "order": 12
    },
    {
      "id": "generative-models",
      "name": "13. Generative Models",
      "description": "Models that create new data",
      "icon": "Sparkles",
      "color": "pink",
      "order": 13
    },
    {
      "id": "specialized-learning",
      "name": "14. Specialized Learning",
      "description": "Transfer, reinforcement, and self-supervised learning",
      "icon": "Zap",
      "color": "yellow",
      "order": 14
    },
    {
      "id": "structured-prediction",
      "name": "15. Structured Prediction",
      "description": "Sequence labeling and structured outputs",
      "icon": "GitMerge",
      "color": "indigo",
      "order": 15
    },
    {
      "id": "computer-vision",
      "name": "16. Computer Vision",
      "description": "Image and video understanding",
      "icon": "Eye",
      "color": "cyan",
      "order": 16
    },
    {
      "id": "nlp",
      "name": "17. NLP",
      "description": "Natural language processing",
      "icon": "MessageSquare",
      "color": "emerald",
      "order": 17
    },
    {
      "id": "time-series",
      "name": "18. Time Series",
      "description": "Temporal data analysis and forecasting",
      "icon": "Clock",
      "color": "amber",
      "order": 18
    },
    {
      "id": "recommendations",
      "name": "19. Recommendations",
      "description": "Recommendation systems",
      "icon": "ThumbsUp",
      "color": "rose",
      "order": 19
    },
    {
      "id": "practical",
      "name": "20. Practical ML",
      "description": "Feature engineering and model debugging",
      "icon": "Wrench",
      "color": "slate",
      "order": 20
    },
    {
      "id": "mlops",
      "name": "21. MLOps",
      "description": "Deploying and monitoring ML systems",
      "icon": "Server",
      "color": "violet",
      "order": 21
    },
    {
      "id": "ethics",
      "name": "22. Ethics & Fairness",
      "description": "Responsible AI and compliance",
      "icon": "Scale",
      "color": "lime",
      "order": 22
    },
    {
      "id": "rag",
      "name": "23. RAG",
      "description": "Retrieval-Augmented Generation",
      "icon": "Search",
      "color": "fuchsia",
      "order": 23
    }
  ],
  "concepts": [
    {
      "id": "core-concepts",
      "name": "Core Concepts",
      "parentId": "foundations",
      "sectionId": "1.1",
      "level": 1,
      "fullExplanation": "Core concepts form the fundamental building blocks of machine learning, including the definition of ML itself, how it differs from traditional programming, and the theoretical foundations that explain why ML works.",
      "simpleExplanation": "The basic ideas you need to understand before diving into machine learning algorithms and techniques.",
      "example": {
        "description": "Core concepts include understanding what machine learning is, how it differs from regular programming, and the mathematical theory behind it."
      },
      "tags": [
        "basics",
        "fundamentals",
        "introduction"
      ],
      "relatedConcepts": [
        "types-of-learning"
      ],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "machine-learning",
      "name": "Machine Learning",
      "parentId": "core-concepts",
      "sectionId": "1.1.1",
      "level": 2,
      "fullExplanation": "Machine Learning is a subfield of artificial intelligence that develops algorithms and statistical models enabling computer systems to progressively improve performance on a specific task through experience, without being explicitly programmed for that task. It relies on patterns and inference from data rather than explicit instructions.",
      "simpleExplanation": "Instead of telling a computer exactly what to do step by step, you show it many examples and let it figure out the patterns itself. Like teaching a child to recognize cats by showing them thousands of cat pictures rather than describing what a cat looks like.",
      "example": {
        "description": "Email spam filter: Instead of writing rules like \"if contains 'free money' then spam\", you feed the system 10,000 emails labeled as spam or not-spam. The system learns patterns (certain words, sender patterns, formatting) that distinguish spam from legitimate emails.",
        "code": "from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Prepare training data\nemails = [\"Win free money now!\", \"Meeting at 3pm\", \"You won $1000!\", \"Project update\"]\nlabels = [\"spam\", \"not_spam\", \"spam\", \"not_spam\"]\n\n# Convert text to features\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(emails)\n\n# Train the model\nmodel = MultinomialNB()\nmodel.fit(X, labels)\n\n# Predict on new email\nnew_email = vectorizer.transform([\"Claim your prize now!\"])\nprint(model.predict(new_email))  # Output: ['spam']",
        "codeLanguage": "python"
      },
      "tags": [
        "ml",
        "basics",
        "introduction",
        "ai",
        "artificial-intelligence"
      ],
      "relatedConcepts": [
        "traditional-vs-ml",
        "supervised-learning",
        "unsupervised-learning"
      ],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "traditional-vs-ml",
      "name": "Traditional Programming vs ML",
      "parentId": "core-concepts",
      "sectionId": "1.1.2",
      "level": 2,
      "fullExplanation": "Traditional programming follows explicit rule-based logic where humans define input-output mappings through conditional statements and algorithms. Machine Learning inverts this paradigm—instead of programming rules, you provide examples of inputs with their desired outputs, and the algorithm discovers the underlying rules automatically through optimization.",
      "simpleExplanation": "Traditional: You write the recipe (rules) yourself. ML: You show the computer many finished dishes (examples) and it figures out the recipe.",
      "example": {
        "description": "Recognizing handwritten digits:\n- Traditional: Write rules like \"if top loop and bottom loop, it's 8\" — extremely hard to cover all variations\n- ML: Show 60,000 handwritten digit images with labels, model learns to recognize any handwriting style automatically",
        "table": {
          "headers": [
            "Aspect",
            "Traditional Programming",
            "Machine Learning"
          ],
          "rows": [
            [
              "Input",
              "Rules + Data",
              "Data + Expected Output"
            ],
            [
              "Output",
              "Answers",
              "Rules/Model"
            ],
            [
              "Maintenance",
              "Update rules manually",
              "Retrain with new data"
            ],
            [
              "Complexity",
              "Hard for complex patterns",
              "Handles complexity well"
            ]
          ]
        }
      },
      "tags": [
        "comparison",
        "programming",
        "paradigm"
      ],
      "relatedConcepts": [
        "machine-learning"
      ],
      "prerequisites": [
        "machine-learning"
      ],
      "difficulty": "beginner"
    },
    {
      "id": "statistical-learning-theory",
      "name": "Statistical Learning Theory",
      "parentId": "core-concepts",
      "sectionId": "1.1.3",
      "level": 2,
      "fullExplanation": "Statistical Learning Theory provides the mathematical foundation for understanding when and why machine learning works. It addresses the fundamental question: how can a model trained on finite samples generalize to unseen data? Key concepts include VC dimension, PAC learning, and bounds on generalization error that relate sample size, hypothesis complexity, and expected performance.",
      "simpleExplanation": "It's the math that proves ML actually works. It answers: \"If I learn from 1000 examples, why should I expect it to work on example 1001?\" It gives guarantees about how much data you need and how complex your model can be.",
      "example": {
        "description": "Theory tells us: To reliably learn a model with VC dimension d, you need roughly O(d/ε²) samples to achieve error ε. This is why a simple linear classifier needs fewer examples than a complex neural network to learn reliably."
      },
      "tags": [
        "theory",
        "mathematics",
        "foundations",
        "vc-dimension",
        "pac-learning"
      ],
      "relatedConcepts": [
        "no-free-lunch",
        "bias-error",
        "variance-error"
      ],
      "prerequisites": [
        "machine-learning"
      ],
      "difficulty": "advanced"
    },
    {
      "id": "no-free-lunch",
      "name": "No Free Lunch Theorem",
      "parentId": "core-concepts",
      "sectionId": "1.1.4",
      "level": 2,
      "fullExplanation": "The No Free Lunch Theorem states that no single machine learning algorithm performs best across all possible problems. Any two algorithms are equivalent when their performance is averaged across all possible problems. This implies that algorithm selection must be guided by domain knowledge, data characteristics, and problem structure—there is no universally superior approach.",
      "simpleExplanation": "There's no \"best\" algorithm that wins at everything. An algorithm that's amazing for images might be terrible for text. You always have to pick the right tool for the job.",
      "example": {
        "description": "Random Forest might beat Neural Networks on a small tabular dataset with 1000 rows. But for image classification with millions of images, Neural Networks crush Random Forest. Neither is \"better\"—they're suited for different problems."
      },
      "tags": [
        "theory",
        "algorithm-selection",
        "fundamentals"
      ],
      "relatedConcepts": [
        "machine-learning",
        "statistical-learning-theory"
      ],
      "prerequisites": [
        "machine-learning"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "types-of-learning",
      "name": "Types of Learning",
      "parentId": "foundations",
      "sectionId": "1.2",
      "level": 1,
      "fullExplanation": "Machine learning approaches are categorized by how they learn from data. The main paradigms differ in the type of feedback available during training: supervised learning uses labeled examples, unsupervised learning finds patterns without labels, and other paradigms like semi-supervised, self-supervised, and reinforcement learning offer different trade-offs.",
      "simpleExplanation": "Different ways a model can learn: with answers (supervised), without answers (unsupervised), or through trial and error (reinforcement). Each approach is suited for different types of problems.",
      "example": {
        "description": "Supervised: Predict house prices given features. Unsupervised: Group customers into segments. Reinforcement: Train a robot to walk."
      },
      "tags": [
        "learning-paradigms",
        "fundamentals"
      ],
      "relatedConcepts": [
        "core-concepts",
        "supervised-learning",
        "unsupervised-learning"
      ],
      "prerequisites": [
        "machine-learning"
      ],
      "difficulty": "beginner"
    },
    {
      "id": "supervised-learning",
      "name": "Supervised Learning",
      "parentId": "types-of-learning",
      "sectionId": "1.2.1",
      "level": 2,
      "fullExplanation": "Supervised Learning trains models using labeled datasets where each input example is paired with its correct output (target/label). The algorithm learns a mapping function f: X → Y by minimizing the discrepancy between predicted and actual outputs. It's called \"supervised\" because the correct answers guide the learning process, like a teacher supervising a student.",
      "simpleExplanation": "Learning with an answer key. You show the computer examples where you know the right answer, and it learns to predict answers for new examples.",
      "example": {
        "description": "House price prediction:\n- Input features: square footage, bedrooms, location, age\n- Label: actual sale price ($450,000)\n- Model learns: given these features, predict price\n- New house comes in → model predicts $475,000",
        "code": "from sklearn.linear_model import LinearRegression\nimport numpy as np\n\n# Training data: [sqft, bedrooms, age]\nX = np.array([[1500, 3, 10], [2000, 4, 5], [1200, 2, 20]])\ny = np.array([300000, 450000, 250000])  # prices\n\n# Train model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n# Predict new house\nnew_house = np.array([[1800, 3, 8]])\nprint(f\"Predicted price: ${model.predict(new_house)[0]:,.0f}\")",
        "codeLanguage": "python"
      },
      "tags": [
        "supervised",
        "labeled-data",
        "classification",
        "regression"
      ],
      "relatedConcepts": [
        "unsupervised-learning",
        "semi-supervised-learning",
        "training-set"
      ],
      "prerequisites": [
        "machine-learning"
      ],
      "difficulty": "beginner"
    },
    {
      "id": "unsupervised-learning",
      "name": "Unsupervised Learning",
      "parentId": "types-of-learning",
      "sectionId": "1.2.2",
      "level": 2,
      "fullExplanation": "Unsupervised Learning works with unlabeled data, discovering hidden patterns, structures, or relationships without predefined correct answers. The algorithm identifies inherent groupings (clustering), reduces dimensionality, or models data distribution. There's no \"right answer\" to optimize toward—the algorithm finds whatever structure exists in the data.",
      "simpleExplanation": "Learning without an answer key. You give the computer a pile of data and say \"find interesting patterns\" without telling it what to look for.",
      "example": {
        "description": "Customer segmentation:\n- Input: purchase history, browsing behavior, demographics for 100,000 customers\n- No labels—you don't know what groups exist\n- Algorithm discovers: \"There are 5 natural clusters—budget shoppers, luxury buyers, seasonal shoppers, etc.\"\n- You interpret the clusters after",
        "code": "from sklearn.cluster import KMeans\nimport numpy as np\n\n# Customer data: [total_spend, visit_frequency]\nX = np.array([[100, 2], [150, 3], [5000, 15], [4500, 12], [50, 1]])\n\n# Find 2 clusters\nkmeans = KMeans(n_clusters=2, random_state=42)\nclusters = kmeans.fit_predict(X)\n\nprint(f\"Cluster assignments: {clusters}\")\n# Output: [0, 0, 1, 1, 0] - separated high-value from regular customers",
        "codeLanguage": "python"
      },
      "tags": [
        "unsupervised",
        "clustering",
        "dimensionality-reduction",
        "unlabeled-data"
      ],
      "relatedConcepts": [
        "supervised-learning",
        "semi-supervised-learning"
      ],
      "prerequisites": [
        "machine-learning"
      ],
      "difficulty": "beginner"
    },
    {
      "id": "semi-supervised-learning",
      "name": "Semi-Supervised Learning",
      "parentId": "types-of-learning",
      "sectionId": "1.2.3",
      "level": 2,
      "fullExplanation": "Semi-Supervised Learning leverages a small amount of labeled data combined with a large amount of unlabeled data. This approach exploits the structure of unlabeled data to improve model performance beyond what's achievable with labeled data alone. It's particularly valuable when labeling is expensive but unlabeled data is abundant.",
      "simpleExplanation": "You have a few examples with answers and many without. The model uses the patterns in all the data to learn better than it could from just the labeled examples.",
      "example": {
        "description": "Medical image diagnosis:\n- 100 labeled X-rays (expensive—needs radiologist)\n- 100,000 unlabeled X-rays (cheap—just raw images)\n- Model learns structure from all 100,100 images but uses labels from 100 to guide classification\n- Result: Better than using only 100 labeled images"
      },
      "tags": [
        "semi-supervised",
        "limited-labels",
        "unlabeled-data"
      ],
      "relatedConcepts": [
        "supervised-learning",
        "unsupervised-learning",
        "self-supervised-learning"
      ],
      "prerequisites": [
        "supervised-learning",
        "unsupervised-learning"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "self-supervised-learning",
      "name": "Self-Supervised Learning",
      "parentId": "types-of-learning",
      "sectionId": "1.2.4",
      "level": 2,
      "fullExplanation": "Self-Supervised Learning generates supervisory signals from the data itself through pretext tasks, requiring no manual labels. The model learns representations by predicting parts of the input from other parts (masked token prediction, next frame prediction, contrastive learning). These representations transfer well to downstream tasks.",
      "simpleExplanation": "The data creates its own labels. Like covering part of a sentence and asking the model to guess the hidden word—the \"answer\" comes from the sentence itself.",
      "example": {
        "description": "BERT pre-training:\n- Take sentence: \"The cat sat on the [MASK]\"\n- Label is automatically: \"mat\" (from original text)\n- Model learns language understanding by predicting millions of masked words\n- No human labeling needed—labels come from the text itself"
      },
      "tags": [
        "self-supervised",
        "pretext-tasks",
        "representation-learning",
        "bert",
        "gpt"
      ],
      "relatedConcepts": [
        "semi-supervised-learning",
        "transfer-learning"
      ],
      "prerequisites": [
        "supervised-learning"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "transfer-learning",
      "name": "Transfer Learning",
      "parentId": "types-of-learning",
      "sectionId": "1.2.5",
      "level": 2,
      "fullExplanation": "Transfer Learning applies knowledge gained from solving one task to a different but related task. Instead of training from scratch, you start with a model pre-trained on a large dataset, then fine-tune it for your specific task. This dramatically reduces required training data and computation while often improving performance.",
      "simpleExplanation": "Don't start from zero. Take a model that already learned something useful (like recognizing shapes from millions of images) and adapt it to your specific problem (like identifying your company's products).",
      "example": {
        "description": "Building a dog breed classifier:\n- Start with ResNet pre-trained on ImageNet (14 million images, 1000 classes)\n- Model already knows edges, textures, shapes, animal features\n- Fine-tune on 1,000 dog images across 120 breeds\n- Result: High accuracy with small dataset, trains in minutes not days",
        "code": "import torchvision.models as models\nimport torch.nn as nn\n\n# Load pre-trained ResNet\nmodel = models.resnet50(pretrained=True)\n\n# Freeze early layers (keep learned features)\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Replace final layer for our task (120 dog breeds)\nmodel.fc = nn.Linear(model.fc.in_features, 120)\n\n# Only train the new layer\n# Much faster than training from scratch!",
        "codeLanguage": "python"
      },
      "tags": [
        "transfer-learning",
        "fine-tuning",
        "pretrained",
        "efficiency"
      ],
      "relatedConcepts": [
        "self-supervised-learning",
        "meta-learning"
      ],
      "prerequisites": [
        "supervised-learning"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "data-types",
      "name": "Data Types",
      "parentId": "data-foundation",
      "sectionId": "2.1",
      "level": 1,
      "fullExplanation": "Data in machine learning comes in various formats and structures. Understanding data types is crucial for choosing appropriate preprocessing techniques, feature engineering strategies, and model architectures. The main categories are structured (tabular), unstructured (text, images), semi-structured (JSON, XML), and specialized types like time series and graph data.",
      "simpleExplanation": "Different kinds of data need different handling. Spreadsheet data, images, text, and time-based data each have their own rules and best practices.",
      "example": {
        "description": "A retail company might have: structured data (sales records in database), unstructured data (customer reviews, product images), and time series data (daily sales figures)."
      },
      "tags": [
        "data",
        "data-types",
        "preprocessing"
      ],
      "relatedConcepts": [
        "structured-data",
        "unstructured-data"
      ],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "structured-data",
      "name": "Structured Data",
      "parentId": "data-types",
      "sectionId": "2.1.1",
      "level": 2,
      "fullExplanation": "Structured data is organized in a predefined schema with rows and columns, typically stored in relational databases or spreadsheets. Each column has a defined data type, and relationships between tables follow consistent rules. This format enables efficient querying, indexing, and standard analytical operations.",
      "simpleExplanation": "Data in neat tables with rows and columns. Like an Excel spreadsheet where every column has a name and every row is one record. Easy to search and analyze.",
      "example": {
        "description": "Customer database table:",
        "table": {
          "headers": [
            "CustomerID",
            "Name",
            "Age",
            "City",
            "TotalPurchases"
          ],
          "rows": [
            [
              "001",
              "John",
              "34",
              "NYC",
              "$1,250"
            ],
            [
              "002",
              "Sarah",
              "28",
              "LA",
              "$890"
            ],
            [
              "003",
              "Mike",
              "45",
              "Chicago",
              "$2,100"
            ]
          ]
        }
      },
      "tags": [
        "structured",
        "tabular",
        "database",
        "sql"
      ],
      "relatedConcepts": [
        "unstructured-data",
        "semi-structured-data"
      ],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "unstructured-data",
      "name": "Unstructured Data",
      "parentId": "data-types",
      "sectionId": "2.1.2",
      "level": 2,
      "fullExplanation": "Unstructured data lacks a predefined format or organization, including text documents, images, audio, video, and social media content. It doesn't fit neatly into tables and requires specialized processing techniques (NLP, computer vision) to extract meaningful features. Comprises ~80% of enterprise data.",
      "simpleExplanation": "Data that doesn't fit in a spreadsheet. Raw text, photos, videos, audio recordings—things you can't easily put in rows and columns.",
      "example": {
        "description": "Examples of unstructured data:\n- Customer emails (free-form text)\n- Product images\n- Support call recordings\n- Social media posts\n- PDF documents"
      },
      "tags": [
        "unstructured",
        "text",
        "images",
        "audio",
        "nlp",
        "computer-vision"
      ],
      "relatedConcepts": [
        "structured-data",
        "semi-structured-data"
      ],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "data-quality",
      "name": "Data Quality & Preprocessing",
      "parentId": "data-foundation",
      "sectionId": "2.2",
      "level": 1,
      "fullExplanation": "Data quality refers to the accuracy, completeness, consistency, and reliability of data. Preprocessing transforms raw data into a format suitable for machine learning models. Poor data quality leads to poor models—garbage in, garbage out. Key tasks include handling missing values, removing outliers, and cleaning inconsistencies.",
      "simpleExplanation": "Making sure your data is clean and ready to use. Fixing errors, filling in blanks, and removing weird values so the model can learn properly.",
      "example": {
        "description": "Raw survey data might have: missing ages, typos in city names, outlier incomes (negative values or billions), and duplicate entries. All need fixing before training."
      },
      "tags": [
        "data-quality",
        "preprocessing",
        "cleaning"
      ],
      "relatedConcepts": [
        "missing-values",
        "outliers",
        "data-cleaning"
      ],
      "prerequisites": [
        "data-types"
      ],
      "difficulty": "beginner"
    },
    {
      "id": "missing-values",
      "name": "Missing Values",
      "parentId": "data-quality",
      "sectionId": "2.2.1",
      "level": 2,
      "fullExplanation": "Missing values occur when no data value is stored for a variable in an observation. They can be Missing Completely at Random (MCAR), Missing at Random (MAR), or Missing Not at Random (MNAR). Handling strategies include deletion (listwise, pairwise), imputation (mean, median, mode, model-based), or using algorithms that handle missing values natively.",
      "simpleExplanation": "Empty cells in your data. Sometimes data wasn't collected, was corrupted, or the person skipped a question. You need to decide: delete the row, fill in a guess, or use an algorithm that handles gaps.",
      "example": {
        "description": "Survey data with missing values:",
        "table": {
          "headers": [
            "Age",
            "Income",
            "Satisfaction"
          ],
          "rows": [
            [
              "34",
              "$50,000",
              "8"
            ],
            [
              "28",
              "???",
              "7"
            ],
            [
              "45",
              "$75,000",
              "???"
            ]
          ]
        }
      },
      "tags": [
        "missing-data",
        "imputation",
        "data-cleaning"
      ],
      "relatedConcepts": [
        "imputation",
        "data-cleaning"
      ],
      "prerequisites": [
        "data-quality"
      ],
      "difficulty": "beginner"
    },
    {
      "id": "imputation",
      "name": "Imputation",
      "parentId": "data-quality",
      "sectionId": "2.2.2",
      "level": 2,
      "fullExplanation": "Imputation replaces missing values with estimated values. Simple methods use statistical measures (mean, median, mode). Advanced methods use models to predict missing values based on other features (KNN imputation, regression imputation, MICE—Multiple Imputation by Chained Equations). Choice depends on missing data mechanism and feature relationships.",
      "simpleExplanation": "Filling in the blanks with educated guesses. Simple: use the average. Smart: use other information to predict what the missing value probably was.",
      "example": {
        "description": "Missing income value:\n- Mean imputation: Fill with average income ($55,000)\n- Median imputation: Fill with middle value ($52,000)—better if outliers exist\n- KNN imputation: Find 5 similar people (same age, job, location), average their incomes ($58,000)",
        "code": "from sklearn.impute import SimpleImputer, KNNImputer\nimport numpy as np\n\ndata = np.array([[25, 50000], [30, np.nan], [35, 75000]])\n\n# Mean imputation\nmean_imp = SimpleImputer(strategy='mean')\ndata_mean = mean_imp.fit_transform(data)\n\n# KNN imputation (uses similar rows)\nknn_imp = KNNImputer(n_neighbors=2)\ndata_knn = knn_imp.fit_transform(data)",
        "codeLanguage": "python"
      },
      "tags": [
        "imputation",
        "missing-data",
        "preprocessing"
      ],
      "relatedConcepts": [
        "missing-values",
        "data-cleaning"
      ],
      "prerequisites": [
        "missing-values"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "outliers",
      "name": "Outliers",
      "parentId": "data-quality",
      "sectionId": "2.2.3",
      "level": 2,
      "fullExplanation": "Outliers are data points that significantly deviate from other observations. They can result from measurement errors, data entry mistakes, or represent genuine rare events. Outliers can skew statistical measures, distort model training, and violate assumptions. Detection methods include Z-score, IQR, and isolation-based algorithms.",
      "simpleExplanation": "Weird data points that don't fit the pattern. Could be mistakes (typo: age 500 instead of 50) or real rare cases (billionaire in income survey). Need to decide if they're errors to fix or valid data to keep.",
      "example": {
        "description": "Employee salaries: $45K, $52K, $48K, $55K, $2,500K\n\n$2.5M is an outlier. Is it:\n- Error? CEO salary accidentally included in analyst data → Remove\n- Valid? Company includes all employees → Keep but be aware it skews mean"
      },
      "tags": [
        "outliers",
        "anomalies",
        "data-cleaning",
        "statistics"
      ],
      "relatedConcepts": [
        "data-cleaning",
        "missing-values"
      ],
      "prerequisites": [
        "data-quality"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "feature-engineering",
      "name": "Feature Engineering",
      "parentId": "data-foundation",
      "sectionId": "2.3",
      "level": 1,
      "fullExplanation": "Feature engineering transforms raw data into features that better represent the underlying problem, improving model performance. It requires domain knowledge to create informative representations. Includes creating new features, transforming existing ones, encoding categories, and combining features. Often the highest-impact activity in ML projects.",
      "simpleExplanation": "Creating better inputs for your model. Raw data might not be in the most useful form. You transform it, combine it, or create new information that helps the model learn patterns.",
      "example": {
        "description": "E-commerce purchase prediction - from raw timestamps create:\n- days_since_last_purchase\n- purchases_per_month\n- is_weekend_shopper\n- average_order_value\n- days_until_payday"
      },
      "tags": [
        "feature-engineering",
        "features",
        "transformation"
      ],
      "relatedConcepts": [
        "feature-scaling",
        "categorical-encoding"
      ],
      "prerequisites": [
        "data-quality"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "feature-scaling",
      "name": "Feature Scaling",
      "parentId": "feature-engineering",
      "sectionId": "2.3.1",
      "level": 2,
      "fullExplanation": "Feature scaling transforms features to similar ranges, essential for algorithms sensitive to feature magnitude (gradient descent, distance-based methods, regularized models). Without scaling, features with larger ranges dominate the learning process. Common methods: normalization (min-max), standardization (z-score), robust scaling (median-based).",
      "simpleExplanation": "Making features comparable. If one feature is 0-1 and another is 0-1,000,000, the big one dominates unfairly. Scaling puts them on similar footing.",
      "example": {
        "description": "Before scaling:\n- Age: 18-80\n- Income: $20,000-$500,000\n- Feature_1 (Age) has almost no influence compared to Feature_2\n\nAfter standardization (mean=0, std=1):\n- Age: -2.1 to +2.3\n- Income: -1.8 to +2.5\n- Both features contribute fairly"
      },
      "tags": [
        "scaling",
        "normalization",
        "standardization",
        "preprocessing"
      ],
      "relatedConcepts": [
        "normalization",
        "standardization"
      ],
      "prerequisites": [
        "feature-engineering"
      ],
      "difficulty": "beginner"
    },
    {
      "id": "normalization",
      "name": "Normalization (Min-Max)",
      "parentId": "feature-engineering",
      "sectionId": "2.3.2",
      "level": 2,
      "fullExplanation": "Min-Max normalization scales features to a fixed range, typically [0, 1]. Formula: x_normalized = (x - x_min) / (x_max - x_min). Preserves relationships between values and maintains zero values if present. Sensitive to outliers since min/max determine the scale.",
      "simpleExplanation": "Squish all values between 0 and 1. The smallest becomes 0, largest becomes 1, everything else proportionally in between.",
      "example": {
        "description": "Ages: [20, 30, 40, 50, 60]\n- Min = 20, Max = 60\n- 20 → (20-20)/(60-20) = 0.0\n- 40 → (40-20)/(60-20) = 0.5\n- 60 → (60-20)/(60-20) = 1.0\n\nResult: [0.0, 0.25, 0.5, 0.75, 1.0]",
        "code": "from sklearn.preprocessing import MinMaxScaler\nimport numpy as np\n\nages = np.array([[20], [30], [40], [50], [60]])\nscaler = MinMaxScaler()\nnormalized = scaler.fit_transform(ages)\nprint(normalized.flatten())  # [0.0, 0.25, 0.5, 0.75, 1.0]",
        "codeLanguage": "python"
      },
      "tags": [
        "normalization",
        "min-max",
        "scaling",
        "preprocessing"
      ],
      "relatedConcepts": [
        "standardization",
        "feature-scaling"
      ],
      "prerequisites": [
        "feature-scaling"
      ],
      "difficulty": "beginner"
    },
    {
      "id": "standardization",
      "name": "Standardization (Z-Score)",
      "parentId": "feature-engineering",
      "sectionId": "2.3.3",
      "level": 2,
      "fullExplanation": "Standardization transforms features to have zero mean and unit variance. Formula: z = (x - μ) / σ. Unlike normalization, it doesn't bound values to a specific range—outliers remain outliers. Preferred when data is approximately normal or when algorithms assume standardized inputs.",
      "simpleExplanation": "Transform so average is 0 and spread is 1. Values tell you \"how many standard deviations from average.\" A value of +2 means \"2 standard deviations above average.\"",
      "example": {
        "description": "Test scores: [60, 70, 80, 90, 100]\n- Mean (μ) = 80\n- Std Dev (σ) = 14.14\n- 60 → (60-80)/14.14 = -1.41\n- 80 → (80-80)/14.14 = 0.0\n- 100 → (100-80)/14.14 = +1.41\n\nResult: [-1.41, -0.71, 0.0, +0.71, +1.41]",
        "code": "from sklearn.preprocessing import StandardScaler\nimport numpy as np\n\nscores = np.array([[60], [70], [80], [90], [100]])\nscaler = StandardScaler()\nstandardized = scaler.fit_transform(scores)\nprint(standardized.flatten().round(2))  # [-1.41, -0.71, 0.0, 0.71, 1.41]",
        "codeLanguage": "python"
      },
      "tags": [
        "standardization",
        "z-score",
        "scaling",
        "preprocessing"
      ],
      "relatedConcepts": [
        "normalization",
        "feature-scaling"
      ],
      "prerequisites": [
        "feature-scaling"
      ],
      "difficulty": "beginner"
    },
    {
      "id": "one-hot-encoding",
      "name": "One-Hot Encoding",
      "parentId": "feature-engineering",
      "sectionId": "2.3.4",
      "level": 2,
      "fullExplanation": "One-Hot Encoding creates binary (0/1) columns for each category, with exactly one column being 1 per observation. It introduces no ordinal relationship between categories. Drawback: high-cardinality features create many columns (1000 cities = 1000 columns), leading to sparse matrices and potential overfitting.",
      "simpleExplanation": "Make a new column for each category. If there are 3 colors, make 3 columns. Each row has exactly one \"1\" showing which category it belongs to.",
      "example": {
        "description": "Original: City column with values [NYC, LA, Chicago]",
        "table": {
          "headers": [
            "City_NYC",
            "City_LA",
            "City_Chicago"
          ],
          "rows": [
            [
              "1",
              "0",
              "0"
            ],
            [
              "0",
              "1",
              "0"
            ],
            [
              "0",
              "0",
              "1"
            ],
            [
              "1",
              "0",
              "0"
            ]
          ]
        }
      },
      "tags": [
        "encoding",
        "categorical",
        "one-hot",
        "preprocessing"
      ],
      "relatedConcepts": [
        "feature-engineering",
        "label-encoding"
      ],
      "prerequisites": [
        "feature-engineering"
      ],
      "difficulty": "beginner"
    },
    {
      "id": "train-val-test-split",
      "name": "Train/Validation/Test Split",
      "parentId": "data-foundation",
      "sectionId": "2.4",
      "level": 1,
      "fullExplanation": "Data splitting divides available data into separate sets for training, validation, and testing. Training data fits the model, validation data tunes hyperparameters, and test data provides final unbiased evaluation. Proper splitting prevents data leakage and gives realistic performance estimates.",
      "simpleExplanation": "Divide your data into three piles: one to learn from (training), one to tune settings (validation), and one final exam (test). Never peek at the test set until you're done!",
      "example": {
        "description": "10,000 samples split:\n- Training: 7,000 (70%) - model learns from these\n- Validation: 1,500 (15%) - tune hyperparameters\n- Test: 1,500 (15%) - final evaluation, touch once"
      },
      "tags": [
        "data-split",
        "train",
        "validation",
        "test",
        "evaluation"
      ],
      "relatedConcepts": [
        "cross-validation",
        "data-leakage"
      ],
      "prerequisites": [
        "data-quality"
      ],
      "difficulty": "beginner"
    },
    {
      "id": "cross-validation",
      "name": "Cross-Validation",
      "parentId": "train-val-test-split",
      "sectionId": "2.4.1",
      "level": 2,
      "fullExplanation": "Cross-validation repeatedly splits data into training and validation sets to get robust performance estimates. It reduces variance from a single split, uses all data for both training and validation (across folds), and provides confidence intervals on metrics. Essential for small datasets and model comparison.",
      "simpleExplanation": "Don't rely on one random split. Split your data multiple ways, train and test on each split, average the results. Gives a more reliable estimate than one lucky or unlucky split.",
      "example": {
        "description": "5-Fold Cross-Validation on 1000 samples:\n- Fold 1: Train on 800, validate on 200 (samples 1-200) → 85%\n- Fold 2: Train on 800, validate on 200 (samples 201-400) → 87%\n- Fold 3: Train on 800, validate on 200 (samples 401-600) → 84%\n- Fold 4: Train on 800, validate on 200 (samples 601-800) → 86%\n- Fold 5: Train on 800, validate on 200 (samples 801-1000) → 85%\n\nAverage: 85.4% ± 1.1% (much more reliable)",
        "code": "from sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier()\nscores = cross_val_score(model, X, y, cv=5)\n\nprint(f\"Accuracy: {scores.mean():.2%} (+/- {scores.std():.2%})\")",
        "codeLanguage": "python"
      },
      "tags": [
        "cross-validation",
        "k-fold",
        "evaluation",
        "validation"
      ],
      "relatedConcepts": [
        "train-val-test-split",
        "stratified-k-fold"
      ],
      "prerequisites": [
        "train-val-test-split"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "hypothesis-space",
      "name": "Hypothesis Space",
      "parentId": "learning-problem",
      "sectionId": "3.1",
      "level": 1,
      "fullExplanation": "The hypothesis space H is the set of all possible functions the model can represent. It's defined by model architecture—a linear model's hypothesis space contains only linear functions; a neural network's contains complex nonlinear functions. Learning selects the best hypothesis from this space. If the true function isn't in H, perfect learning is impossible (model bias).",
      "simpleExplanation": "All the possible patterns your model CAN learn. A straight line can only represent lines. A neural network can represent almost anything. Your model can only learn functions within its hypothesis space.",
      "example": {
        "description": "True relationship: y = x²\n\nLinear model hypothesis space: {y = ax + b}\n- Best it can do: straight line through parabola\n- Will always have error (underfitting)\n\nNeural network hypothesis space: {any continuous function}\n- Can represent y = x² exactly\n- True function is in hypothesis space"
      },
      "tags": [
        "hypothesis-space",
        "model-capacity",
        "theory"
      ],
      "relatedConcepts": [
        "bias-error",
        "underfitting",
        "overfitting"
      ],
      "prerequisites": [
        "machine-learning"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "loss-functions",
      "name": "Loss Functions",
      "parentId": "learning-problem",
      "sectionId": "3.2",
      "level": 1,
      "fullExplanation": "A loss function (cost function, objective function) quantifies the discrepancy between model predictions and true values. It provides the signal for optimization—the gradient of the loss with respect to parameters tells the model how to improve. Loss function choice depends on task type, desired properties (robustness to outliers, probability calibration), and optimization considerations.",
      "simpleExplanation": "A number that says \"how wrong is the model?\" Lower is better. The model adjusts weights to make this number smaller. Different loss functions penalize errors differently.",
      "example": {
        "description": "Predicting house price = $300,000\nActual price = $350,000\n\nMSE Loss: (300,000 - 350,000)² = 2,500,000,000\nMAE Loss: |300,000 - 350,000| = 50,000\n\nSame error, different loss values. Choice affects how model learns."
      },
      "tags": [
        "loss-function",
        "cost-function",
        "optimization"
      ],
      "relatedConcepts": [
        "mse",
        "cross-entropy",
        "gradient-descent"
      ],
      "prerequisites": [
        "machine-learning"
      ],
      "difficulty": "beginner"
    },
    {
      "id": "mse",
      "name": "Mean Squared Error (MSE)",
      "parentId": "loss-functions",
      "sectionId": "3.2.1",
      "level": 2,
      "fullExplanation": "MSE is the average of squared differences between predictions and targets: MSE = (1/n)Σ(ŷᵢ - yᵢ)². Squaring penalizes large errors more heavily than small ones. MSE is smooth, differentiable, and corresponds to maximum likelihood estimation under Gaussian noise assumption. Sensitive to outliers due to squaring.",
      "simpleExplanation": "Average of (prediction - actual)². Big errors get penalized way more than small errors because of the squaring. A $100 error becomes 10,000; a $10 error becomes 100.",
      "example": {
        "description": "Predictions: [100, 200, 300]\nActuals: [110, 190, 350]\n\nErrors: [-10, +10, -50]\nSquared: [100, 100, 2500]\nMSE = (100 + 100 + 2500) / 3 = 900\n\nThe -50 error dominates the loss.",
        "code": "import numpy as np\nfrom sklearn.metrics import mean_squared_error\n\ny_true = np.array([110, 190, 350])\ny_pred = np.array([100, 200, 300])\n\nmse = mean_squared_error(y_true, y_pred)\nprint(f\"MSE: {mse}\")  # 900.0",
        "codeLanguage": "python"
      },
      "tags": [
        "mse",
        "loss-function",
        "regression",
        "metrics"
      ],
      "relatedConcepts": [
        "loss-functions",
        "cross-entropy"
      ],
      "prerequisites": [
        "loss-functions"
      ],
      "difficulty": "beginner"
    },
    {
      "id": "cross-entropy",
      "name": "Cross-Entropy Loss",
      "parentId": "loss-functions",
      "sectionId": "3.2.2",
      "level": 2,
      "fullExplanation": "Cross-entropy loss measures the difference between predicted probability distribution and true distribution. For classification: -Σyᵢlog(pᵢ) where y is one-hot true label and p is predicted probability. Heavily penalizes confident wrong predictions (predicting 0.01 probability for true class). Standard loss for neural network classification.",
      "simpleExplanation": "Measures how surprised the model is by the correct answer. If model says \"99% dog\" and it's actually a cat, loss is huge. If model says \"51% cat\" and it's a cat, loss is small.",
      "example": {
        "description": "True class: Cat\nModel predicts: [Dog: 0.7, Cat: 0.2, Bird: 0.1]\n\nCross-entropy = -log(0.2) = 1.61\n\nIf model predicted: [Dog: 0.1, Cat: 0.8, Bird: 0.1]\nCross-entropy = -log(0.8) = 0.22\n\nMore confident correct prediction → Lower loss"
      },
      "tags": [
        "cross-entropy",
        "loss-function",
        "classification",
        "probability"
      ],
      "relatedConcepts": [
        "mse",
        "loss-functions"
      ],
      "prerequisites": [
        "loss-functions"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "bias-variance-tradeoff",
      "name": "Bias-Variance Tradeoff",
      "parentId": "learning-problem",
      "sectionId": "3.3",
      "level": 1,
      "fullExplanation": "The bias-variance tradeoff describes the fundamental tension in model selection. Bias is error from oversimplified assumptions (underfitting). Variance is error from sensitivity to training data fluctuations (overfitting). Total error = Bias² + Variance + Irreducible noise. Reducing one typically increases the other—finding the sweet spot is key to good generalization.",
      "simpleExplanation": "Too simple = consistent but wrong (high bias). Too complex = right on training data but wrong on new data (high variance). You want the Goldilocks zone—just complex enough.",
      "example": {
        "description": "Fitting a curve through data points:\n- Linear model: High bias (can't capture curve), low variance\n- Polynomial degree 20: Low bias (fits training perfectly), high variance (wiggles wildly)\n- Polynomial degree 3: Balanced—captures the pattern without overfitting"
      },
      "tags": [
        "bias",
        "variance",
        "tradeoff",
        "generalization",
        "model-selection"
      ],
      "relatedConcepts": [
        "bias-error",
        "variance-error",
        "underfitting",
        "overfitting"
      ],
      "prerequisites": [
        "machine-learning"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "bias-error",
      "name": "Bias",
      "parentId": "bias-variance-tradeoff",
      "sectionId": "3.3.1",
      "level": 2,
      "fullExplanation": "Bias is the error from oversimplified assumptions in the learning algorithm. High bias means the model misses relevant relations between features and target, consistently getting the answer wrong in the same direction. It indicates underfitting—the model is too simple to capture the underlying pattern, regardless of training data quantity.",
      "simpleExplanation": "The error from making wrong assumptions. A simple model might assume everything is a straight line, but real data is curved. No matter how much data you give it, it'll keep getting the curve wrong. It's consistently wrong in a predictable way.",
      "example": {
        "description": "True relationship: y = x²\nLinear model predicts: y = 2x - 1\n\nNo matter how much data:\n- x=2: True=4, Predict=3 (error=-1)\n- x=5: True=25, Predict=9 (error=-16)\n\nModel is fundamentally unable to capture the curve—that's bias."
      },
      "tags": [
        "bias",
        "underfitting",
        "model-complexity"
      ],
      "relatedConcepts": [
        "variance-error",
        "underfitting",
        "bias-variance-tradeoff"
      ],
      "prerequisites": [
        "bias-variance-tradeoff"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "variance-error",
      "name": "Variance",
      "parentId": "bias-variance-tradeoff",
      "sectionId": "3.3.2",
      "level": 2,
      "fullExplanation": "Variance is the error from sensitivity to small fluctuations in the training data. High variance means the model fits training data very closely—including noise—and produces very different models from different training sets. It indicates overfitting—the model is too complex and memorizes rather than generalizes.",
      "simpleExplanation": "The error from being too sensitive. A complex model that fits training data perfectly might be just fitting noise. Train on slightly different data and you get a totally different model. It memorizes instead of learning.",
      "example": {
        "description": "Two training sets from same distribution:\n- Set A trains model that predicts new point as 5.2\n- Set B trains model that predicts same point as 8.7\n\nBig difference = High variance\nThe model is unstable, capturing noise specific to each training set."
      },
      "tags": [
        "variance",
        "overfitting",
        "model-complexity"
      ],
      "relatedConcepts": [
        "bias-error",
        "overfitting",
        "bias-variance-tradeoff"
      ],
      "prerequisites": [
        "bias-variance-tradeoff"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "underfitting",
      "name": "Underfitting",
      "parentId": "bias-variance-tradeoff",
      "sectionId": "3.3.3",
      "level": 2,
      "fullExplanation": "Underfitting occurs when the model is too simple to capture the underlying pattern in the data. Characterized by high training error AND high test error—the model performs poorly even on data it trained on. Caused by insufficient model complexity, too much regularization, or not enough training.",
      "simpleExplanation": "Model is too simple. It can't even do well on training data, let alone new data. Like trying to fit a curved road with a straight ruler—just doesn't work.",
      "example": {
        "description": "Fitting complex data with linear regression:\n- Training accuracy: 60%\n- Test accuracy: 58%\n\nBoth are bad! Model is too simple.\n\nFix: Use more complex model (polynomial, neural network)"
      },
      "tags": [
        "underfitting",
        "bias",
        "model-complexity",
        "training"
      ],
      "relatedConcepts": [
        "overfitting",
        "bias-error",
        "generalization"
      ],
      "prerequisites": [
        "bias-variance-tradeoff"
      ],
      "difficulty": "beginner"
    },
    {
      "id": "overfitting",
      "name": "Overfitting",
      "parentId": "bias-variance-tradeoff",
      "sectionId": "3.3.4",
      "level": 2,
      "fullExplanation": "Overfitting occurs when the model learns the training data too well, including noise and random fluctuations, failing to generalize to new data. Characterized by low training error but high test error—a significant gap. Caused by excessive model complexity, insufficient training data, or inadequate regularization.",
      "simpleExplanation": "Model memorized the training data instead of learning patterns. It knows every quirk of training examples but fails on new ones. Like memorizing test answers instead of understanding the subject.",
      "example": {
        "description": "Training a very deep neural network:\n- Training accuracy: 99.5%\n- Test accuracy: 72%\n\nGap of 27.5% = Overfitting\n\nModel memorized training set, doesn't generalize.\nFix: Add regularization, get more data, simplify model."
      },
      "tags": [
        "overfitting",
        "variance",
        "model-complexity",
        "regularization"
      ],
      "relatedConcepts": [
        "underfitting",
        "variance-error",
        "regularization",
        "dropout"
      ],
      "prerequisites": [
        "bias-variance-tradeoff"
      ],
      "difficulty": "beginner"
    },
    {
      "id": "generalization",
      "name": "Generalization",
      "parentId": "bias-variance-tradeoff",
      "sectionId": "3.3.5",
      "level": 2,
      "fullExplanation": "Generalization is the model's ability to perform well on unseen data from the same distribution as training data. It's the ultimate goal of machine learning—we don't care about training performance, only how well the model handles new examples. Good generalization means low gap between training and test performance.",
      "simpleExplanation": "How well the model works on new data it's never seen. Training accuracy doesn't matter if the model fails on fresh data. Generalization is the whole point of ML.",
      "example": {
        "description": "Model A: Training=95%, Test=93% → Good generalization (2% gap)\nModel B: Training=99%, Test=75% → Poor generalization (24% gap)\n\nModel A is better despite lower training accuracy because it generalizes."
      },
      "tags": [
        "generalization",
        "test-accuracy",
        "model-evaluation"
      ],
      "relatedConcepts": [
        "overfitting",
        "underfitting",
        "bias-variance-tradeoff"
      ],
      "prerequisites": [
        "bias-variance-tradeoff"
      ],
      "difficulty": "beginner"
    },
    {
      "id": "gradient-based-optimization",
      "name": "Gradient-Based Optimization",
      "parentId": "optimization",
      "sectionId": "4.1",
      "level": 1,
      "fullExplanation": "Gradient-based optimization uses the gradient (vector of partial derivatives) to iteratively update model parameters in the direction that reduces the loss function. The gradient points toward the steepest increase, so moving in the negative gradient direction decreases loss. This family of methods includes basic gradient descent, stochastic variants, and adaptive methods.",
      "simpleExplanation": "Find which way is downhill (gradient), take a step that way. Repeat until you reach the bottom (minimum loss). The foundation of how neural networks learn.",
      "example": {
        "description": "Imagine standing on a hilly landscape blindfolded. You can feel the slope under your feet. To find the lowest point:\n1. Feel which direction goes downhill steepest\n2. Take a step that way\n3. Repeat until you can't go lower"
      },
      "tags": [
        "optimization",
        "gradient",
        "training",
        "neural-networks"
      ],
      "relatedConcepts": [
        "gradient",
        "gradient-descent",
        "sgd",
        "adam"
      ],
      "prerequisites": [
        "loss-functions"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "gradient",
      "name": "Gradient",
      "parentId": "gradient-based-optimization",
      "sectionId": "4.1.1",
      "level": 2,
      "fullExplanation": "The gradient is a vector of partial derivatives of a function with respect to each parameter, pointing in the direction of steepest increase. For a loss function L(θ), ∇L(θ) indicates which way parameters should move to increase loss most rapidly. Moving in the negative gradient direction decreases loss—this is the foundation of gradient-based optimization.",
      "simpleExplanation": "Arrow pointing uphill. For each weight, gradient says \"if you increase this weight, loss goes up/down by this much.\" Go the opposite direction (downhill) to reduce loss.",
      "example": {
        "description": "Loss function L(w1, w2) at current point:\n- ∂L/∂w1 = +3.5 (increasing w1 increases loss)\n- ∂L/∂w2 = -2.0 (increasing w2 decreases loss)\n\nGradient = [+3.5, -2.0]\nMove opposite: Δw1 = -3.5, Δw2 = +2.0 (reduces loss)"
      },
      "tags": [
        "gradient",
        "calculus",
        "optimization",
        "derivatives"
      ],
      "relatedConcepts": [
        "gradient-descent",
        "backpropagation"
      ],
      "prerequisites": [
        "loss-functions"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "gradient-descent",
      "name": "Gradient Descent",
      "parentId": "gradient-based-optimization",
      "sectionId": "4.1.2",
      "level": 2,
      "fullExplanation": "Gradient Descent is an iterative optimization algorithm that updates parameters by moving in the direction of steepest descent (negative gradient). Update rule: θ_new = θ_old - α∇L(θ) where α is learning rate. It follows the loss surface downhill toward a minimum. Guaranteed to find global minimum for convex functions; finds local minimum for non-convex.",
      "simpleExplanation": "Walk downhill. Calculate which direction is steepest down, take a step that way. Repeat until you reach the bottom (minimum loss).",
      "example": {
        "description": "Current weights: w = [1.0, 2.0]\nGradient: ∇L = [0.5, -0.3]\nLearning rate: α = 0.1\n\nUpdate: w_new = [1.0, 2.0] - 0.1 × [0.5, -0.3]\n        w_new = [0.95, 2.03]\n\nRepeat thousands of times → converge to minimum.",
        "code": "import numpy as np\n\n# Simple gradient descent\ndef gradient_descent(gradient_fn, initial_w, lr=0.01, steps=1000):\n    w = initial_w.copy()\n    for _ in range(steps):\n        grad = gradient_fn(w)\n        w = w - lr * grad\n    return w\n\n# Example: minimize f(w) = w^2\ngradient_fn = lambda w: 2 * w  # derivative of w^2\nresult = gradient_descent(gradient_fn, np.array([5.0]), lr=0.1, steps=50)\nprint(f\"Minimum at: {result}\")  # Close to 0",
        "codeLanguage": "python"
      },
      "tags": [
        "gradient-descent",
        "optimization",
        "training"
      ],
      "relatedConcepts": [
        "gradient",
        "learning-rate",
        "sgd"
      ],
      "prerequisites": [
        "gradient"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "learning-rate",
      "name": "Learning Rate",
      "parentId": "gradient-based-optimization",
      "sectionId": "4.1.3",
      "level": 2,
      "fullExplanation": "Learning rate (α or η) is a hyperparameter controlling the step size during gradient descent. Too high: overshoots minimum, may diverge. Too low: converges very slowly, may get stuck. Finding the right learning rate is crucial—often the most important hyperparameter. Modern optimizers adapt learning rate per-parameter.",
      "simpleExplanation": "How big a step to take downhill. Too big: you jump past the bottom and bounce around. Too small: takes forever to get there. Just right: smooth descent to minimum.",
      "example": {
        "description": "Loss surface (imagine a valley):\n\nLearning rate too high (α=1.0):\nStep 1: Jump over valley\nStep 2: Jump back over → Diverging!\n\nLearning rate too low (α=0.0001):\nAfter 10000 steps, still far from minimum.\n\nLearning rate just right (α=0.01):\nSmooth convergence in 1000 steps."
      },
      "tags": [
        "learning-rate",
        "hyperparameter",
        "optimization"
      ],
      "relatedConcepts": [
        "gradient-descent",
        "adam",
        "lr-schedule"
      ],
      "prerequisites": [
        "gradient-descent"
      ],
      "difficulty": "beginner"
    },
    {
      "id": "sgd",
      "name": "Stochastic Gradient Descent (SGD)",
      "parentId": "gradient-based-optimization",
      "sectionId": "4.1.4",
      "level": 2,
      "fullExplanation": "SGD computes gradient using a single randomly selected training example instead of the entire dataset. This introduces noise in gradient estimates but enables much faster updates and can escape local minima. In practice, mini-batch SGD uses small batches (32-256 examples) balancing gradient accuracy and computational efficiency.",
      "simpleExplanation": "Instead of computing the perfect gradient from ALL data (slow), estimate it from one random example (fast, noisy). Like polling 100 people instead of entire country—less accurate but much faster. Repeat many times and it averages out.",
      "example": {
        "description": "Dataset: 1,000,000 examples\n\nBatch Gradient Descent:\n- Compute gradient using all 1M examples\n- 1 update per pass through data\n- Very slow\n\nSGD (batch size 32):\n- Compute gradient using 32 random examples\n- 31,250 updates per pass through data\n- Much faster, noisier, but works well in practice",
        "code": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nmodel = nn.Linear(10, 1)\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Training loop with mini-batches\nfor batch_x, batch_y in dataloader:  # batch_size=32\n    optimizer.zero_grad()\n    loss = criterion(model(batch_x), batch_y)\n    loss.backward()\n    optimizer.step()  # Update using batch gradient",
        "codeLanguage": "python"
      },
      "tags": [
        "sgd",
        "stochastic",
        "optimization",
        "mini-batch"
      ],
      "relatedConcepts": [
        "gradient-descent",
        "momentum",
        "adam"
      ],
      "prerequisites": [
        "gradient-descent"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "momentum",
      "name": "Momentum",
      "parentId": "gradient-based-optimization",
      "sectionId": "4.1.5",
      "level": 2,
      "fullExplanation": "Momentum accelerates gradient descent by accumulating past gradients, like a ball rolling downhill. Update: v = βv + ∇L(θ); θ = θ - αv. The velocity v accumulates past gradients, dampening oscillations in high-curvature directions while building speed in consistent directions. β (typically 0.9) controls momentum strength.",
      "simpleExplanation": "Remember which direction you've been going. Instead of just following current gradient, combine it with previous direction. Like a ball rolling—it builds speed going the same direction and smooths out jitter.",
      "example": {
        "description": "Without momentum (oscillating):\nStep 1: gradient says go right\nStep 2: gradient says go left\nStep 3: gradient says go right → Stuck oscillating\n\nWith momentum:\nAccumulated velocity remembers \"we've been going right\"\nStep 2's \"go left\" gets dampened by momentum\nResult: Smooth progress in the right average direction"
      },
      "tags": [
        "momentum",
        "optimization",
        "sgd",
        "velocity"
      ],
      "relatedConcepts": [
        "sgd",
        "adam",
        "gradient-descent"
      ],
      "prerequisites": [
        "sgd"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "adam",
      "name": "Adam Optimizer",
      "parentId": "gradient-based-optimization",
      "sectionId": "4.1.6",
      "level": 2,
      "fullExplanation": "Adam (Adaptive Moment Estimation) combines momentum with adaptive per-parameter learning rates. It maintains exponential moving averages of gradients (first moment) and squared gradients (second moment), using them to scale updates. Each parameter gets its own effective learning rate based on its gradient history. Default choice for many deep learning tasks.",
      "simpleExplanation": "The best of both worlds: momentum (remember direction) + adaptive learning rate (different speed for each weight). Parameters that have been getting consistent gradients move faster; inconsistent ones move carefully.",
      "example": {
        "description": "Parameter A: gradients consistently [2.0, 1.9, 2.1, 2.0]\n→ Adam: Confident! Take bigger steps.\n\nParameter B: gradients noisy [-0.5, 3.0, -2.0, 1.5]\n→ Adam: Uncertain. Take smaller, careful steps.\n\nEach parameter optimized at its own appropriate pace.",
        "code": "import torch.optim as optim\n\n# Adam is usually the default choice\noptimizer = optim.Adam(\n    model.parameters(),\n    lr=0.001,      # Initial learning rate\n    betas=(0.9, 0.999),  # Momentum parameters\n    eps=1e-8       # Numerical stability\n)\n\n# Training loop\nfor epoch in range(epochs):\n    optimizer.zero_grad()\n    loss = criterion(model(x), y)\n    loss.backward()\n    optimizer.step()",
        "codeLanguage": "python"
      },
      "tags": [
        "adam",
        "optimizer",
        "adaptive",
        "deep-learning"
      ],
      "relatedConcepts": [
        "sgd",
        "momentum",
        "learning-rate"
      ],
      "prerequisites": [
        "momentum"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "regularization-techniques",
      "name": "Regularization Techniques",
      "parentId": "regularization",
      "sectionId": "5.1",
      "level": 1,
      "fullExplanation": "Regularization encompasses techniques that constrain model complexity to prevent overfitting and improve generalization. It adds information or constraints beyond the training data, typically by penalizing large parameter values or model complexity. Regularization trades slight increase in training error for significant reduction in test error.",
      "simpleExplanation": "Rules to keep the model from getting too complicated. Without constraints, models memorize training data. Regularization says \"keep it simple\"—which usually means it'll work better on new data.",
      "example": {
        "description": "Without regularization:\n- Training accuracy: 99%\n- Test accuracy: 70%\n- Model is overfitting\n\nWith regularization:\n- Training accuracy: 92%\n- Test accuracy: 88%\n- Model generalizes better"
      },
      "tags": [
        "regularization",
        "overfitting",
        "generalization"
      ],
      "relatedConcepts": [
        "l2-regularization",
        "l1-regularization",
        "dropout"
      ],
      "prerequisites": [
        "overfitting"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "l2-regularization",
      "name": "L2 Regularization (Ridge)",
      "parentId": "regularization-techniques",
      "sectionId": "5.1.1",
      "level": 2,
      "fullExplanation": "L2 regularization adds the sum of squared weights to the loss function: L_total = L_original + λΣwᵢ². This penalizes large weights, pushing them toward zero without making them exactly zero. Equivalent to assuming weights follow Gaussian prior. Controlled by λ (regularization strength). Called weight decay because it continuously shrinks weights.",
      "simpleExplanation": "Penalty for big weights. The bigger your weights, the more you're penalized. Keeps all weights small rather than having a few huge ones. Like a tax—you can have big weights, but you pay a price.",
      "example": {
        "description": "Without L2: Model might learn weights [100, -50, 0.1, 200]\nWith L2 (λ=0.01): Weights become [2.5, -1.3, 0.08, 3.1]\n\nAll weights shrink. Model is simpler, less prone to overfitting.\nLoss = original_loss + 0.01 × (2.5² + 1.3² + 0.08² + 3.1²)",
        "code": "from sklearn.linear_model import Ridge\n\n# Ridge regression = Linear regression + L2\nmodel = Ridge(alpha=1.0)  # alpha is λ\nmodel.fit(X_train, y_train)\n\n# In PyTorch, use weight_decay parameter\noptimizer = torch.optim.Adam(\n    model.parameters(),\n    lr=0.001,\n    weight_decay=0.01  # L2 regularization strength\n)",
        "codeLanguage": "python"
      },
      "tags": [
        "l2",
        "ridge",
        "regularization",
        "weight-decay"
      ],
      "relatedConcepts": [
        "l1-regularization",
        "regularization-techniques"
      ],
      "prerequisites": [
        "regularization-techniques"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "l1-regularization",
      "name": "L1 Regularization (Lasso)",
      "parentId": "regularization-techniques",
      "sectionId": "5.1.2",
      "level": 2,
      "fullExplanation": "L1 regularization adds the sum of absolute weight values to the loss: L_total = L_original + λΣ|wᵢ|. Unlike L2, L1 drives weights to exactly zero, performing automatic feature selection. Results in sparse models where many parameters are exactly zero. Useful when you suspect many features are irrelevant.",
      "simpleExplanation": "Penalty that creates zeros. Some weights get driven all the way to zero (feature eliminated). L2 makes weights small; L1 makes some weights disappear entirely. Good for selecting which features actually matter.",
      "example": {
        "description": "100 features, many irrelevant:\n\nWithout L1: All 100 features have non-zero weights\nWith L1: Only 15 features have non-zero weights, others exactly 0\n\nAutomatic feature selection! Model only uses important features.",
        "code": "from sklearn.linear_model import Lasso\n\n# Lasso = Linear regression + L1\nmodel = Lasso(alpha=0.1)\nmodel.fit(X_train, y_train)\n\n# Check which features were selected (non-zero weights)\nselected = np.where(model.coef_ != 0)[0]\nprint(f\"Selected {len(selected)} out of {len(model.coef_)} features\")",
        "codeLanguage": "python"
      },
      "tags": [
        "l1",
        "lasso",
        "regularization",
        "sparsity",
        "feature-selection"
      ],
      "relatedConcepts": [
        "l2-regularization",
        "regularization-techniques"
      ],
      "prerequisites": [
        "regularization-techniques"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "dropout",
      "name": "Dropout",
      "parentId": "regularization-techniques",
      "sectionId": "5.1.3",
      "level": 2,
      "fullExplanation": "Dropout randomly sets a fraction of neuron activations to zero during training (typically 20-50%). This prevents co-adaptation where neurons rely too heavily on specific other neurons. At test time, all neurons are used but activations are scaled. Effectively trains an ensemble of sub-networks that share weights.",
      "simpleExplanation": "Randomly turn off some neurons during training. Forces the network to not rely on any single neuron too much—like making sure multiple students understand the material, not just one. At test time, use everyone together.",
      "example": {
        "description": "Training with dropout=0.5:\n- Forward pass 1: Neurons [1,3,4,7] active, [2,5,6,8] off\n- Forward pass 2: Neurons [2,3,5,8] active, [1,4,6,7] off\n- Each pass trains a different \"sub-network\"\n\nTest time:\n- All neurons active\n- Each weight multiplied by 0.5 (to compensate)\n- Effectively averages many sub-networks",
        "code": "import torch.nn as nn\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 256)\n        self.dropout = nn.Dropout(p=0.5)  # 50% dropout\n        self.fc2 = nn.Linear(256, 10)\n    \n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.dropout(x)  # Applied during training only\n        x = self.fc2(x)\n        return x",
        "codeLanguage": "python"
      },
      "tags": [
        "dropout",
        "regularization",
        "neural-networks",
        "ensemble"
      ],
      "relatedConcepts": [
        "regularization-techniques",
        "batch-norm"
      ],
      "prerequisites": [
        "regularization-techniques"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "early-stopping",
      "name": "Early Stopping",
      "parentId": "regularization-techniques",
      "sectionId": "5.1.4",
      "level": 2,
      "fullExplanation": "Early stopping monitors validation performance during training and stops when it begins degrading, even if training loss continues decreasing. This prevents overfitting by limiting effective model complexity—models fit simple patterns first, complex/noisy patterns later. The stopping point implicitly regularizes model complexity.",
      "simpleExplanation": "Stop training before the model starts memorizing. Watch validation accuracy—when it stops improving (or gets worse), stop training even if training accuracy is still going up. Training longer = more overfitting.",
      "example": {
        "description": "Training progression:\n- Epoch 10: Train loss=0.5, Val loss=0.6\n- Epoch 20: Train loss=0.3, Val loss=0.4\n- Epoch 30: Train loss=0.2, Val loss=0.35 ← Best!\n- Epoch 40: Train loss=0.1, Val loss=0.40 (getting worse)\n- Epoch 50: Train loss=0.05, Val loss=0.50 (overfitting!)\n\nEarly stopping: Use model from epoch 30.",
        "code": "from sklearn.model_selection import train_test_split\n\n# Keras example with early stopping\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nearly_stop = EarlyStopping(\n    monitor='val_loss',\n    patience=5,  # Wait 5 epochs for improvement\n    restore_best_weights=True\n)\n\nmodel.fit(\n    X_train, y_train,\n    validation_split=0.2,\n    epochs=100,\n    callbacks=[early_stop]\n)",
        "codeLanguage": "python"
      },
      "tags": [
        "early-stopping",
        "regularization",
        "validation",
        "overfitting"
      ],
      "relatedConcepts": [
        "regularization-techniques",
        "overfitting"
      ],
      "prerequisites": [
        "regularization-techniques"
      ],
      "difficulty": "beginner"
    },
    {
      "id": "batch-norm",
      "name": "Batch Normalization",
      "parentId": "regularization-techniques",
      "sectionId": "5.1.5",
      "level": 2,
      "fullExplanation": "Batch Normalization normalizes activations across the batch dimension: for each feature, subtract batch mean and divide by batch standard deviation, then scale and shift with learnable parameters γ and β. Reduces internal covariate shift, enables higher learning rates, acts as regularization, and accelerates training. Requires sufficiently large batches.",
      "simpleExplanation": "After each layer, normalize the outputs: make mean=0, variance=1 across the batch. Then let the model learn the best mean and variance. Keeps values in a nice range throughout training. Makes training faster and more stable.",
      "example": {
        "description": "Before batch norm, layer output: [100, -50, 200, 75]\n- Mean = 81.25, Std = 103.1\n\nAfter batch norm: [(100-81.25)/103.1, ...] = [0.18, -1.27, 1.15, -0.06]\n- Mean ≈ 0, Std ≈ 1\n\nThen scale/shift: γ × normalized + β (γ and β are learned)",
        "code": "import torch.nn as nn\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 256)\n        self.bn1 = nn.BatchNorm1d(256)  # Batch norm after fc1\n        self.fc2 = nn.Linear(256, 10)\n    \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.bn1(x)  # Normalize\n        x = torch.relu(x)\n        x = self.fc2(x)\n        return x",
        "codeLanguage": "python"
      },
      "tags": [
        "batch-normalization",
        "normalization",
        "regularization",
        "training"
      ],
      "relatedConcepts": [
        "dropout",
        "layer-norm"
      ],
      "prerequisites": [
        "regularization-techniques"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "classification-metrics",
      "name": "Classification Metrics",
      "parentId": "evaluation",
      "sectionId": "6.1",
      "level": 1,
      "fullExplanation": "Classification metrics evaluate how well a model categorizes data into discrete classes. Different metrics capture different aspects of performance: accuracy measures overall correctness, precision/recall focus on specific classes, F1 balances precision and recall, and AUC-ROC measures ranking ability. Metric choice depends on problem characteristics and business requirements.",
      "simpleExplanation": "Different ways to measure if your classifier is doing a good job. Overall accuracy isn't always enough—sometimes you care more about catching all the positives (recall) or being sure when you say positive (precision).",
      "example": {
        "description": "For a spam filter:\n- Accuracy: What % of all emails were classified correctly?\n- Precision: When we say 'spam', how often are we right?\n- Recall: What % of actual spam did we catch?\n- F1: Balance of precision and recall"
      },
      "tags": [
        "classification",
        "metrics",
        "evaluation"
      ],
      "relatedConcepts": [
        "confusion-matrix",
        "accuracy",
        "precision",
        "recall",
        "f1-score"
      ],
      "prerequisites": [
        "supervised-learning"
      ],
      "difficulty": "beginner"
    },
    {
      "id": "confusion-matrix",
      "name": "Confusion Matrix",
      "parentId": "classification-metrics",
      "sectionId": "6.1.1",
      "level": 2,
      "fullExplanation": "A confusion matrix is a table showing prediction outcomes for each actual class. For binary classification: True Positives (correctly predicted positive), True Negatives (correctly predicted negative), False Positives (incorrectly predicted positive), False Negatives (incorrectly predicted negative). Foundation for computing most classification metrics.",
      "simpleExplanation": "A table showing what the model predicted vs what was actually true. Shows exactly where the model is right and where it's wrong. Four boxes: correct positives, correct negatives, wrong positives (false alarms), wrong negatives (misses).",
      "example": {
        "description": "Cancer screening results:",
        "table": {
          "headers": [
            "",
            "Predicted: Cancer",
            "Predicted: No Cancer"
          ],
          "rows": [
            [
              "Actual: Cancer",
              "85 (TP)",
              "15 (FN)"
            ],
            [
              "Actual: No Cancer",
              "10 (FP)",
              "890 (TN)"
            ]
          ]
        }
      },
      "tags": [
        "confusion-matrix",
        "classification",
        "evaluation",
        "tp",
        "fp",
        "tn",
        "fn"
      ],
      "relatedConcepts": [
        "accuracy",
        "precision",
        "recall"
      ],
      "prerequisites": [
        "classification-metrics"
      ],
      "difficulty": "beginner"
    },
    {
      "id": "accuracy",
      "name": "Accuracy",
      "parentId": "classification-metrics",
      "sectionId": "6.1.2",
      "level": 2,
      "fullExplanation": "Accuracy is the proportion of correct predictions: (TP + TN) / (TP + TN + FP + FN). Simple and intuitive but misleading for imbalanced classes—a model predicting majority class always achieves high accuracy while being useless for minority class detection.",
      "simpleExplanation": "Percentage of predictions that are correct. Simple but can be deceiving. If 99% of emails are not spam, predicting \"not spam\" always gives 99% accuracy but catches zero spam.",
      "example": {
        "description": "From confusion matrix:\nAccuracy = (85 + 890) / (85 + 15 + 10 + 890) = 975/1000 = 97.5%\n\nSounds great! But...\nIf we predicted \"No Cancer\" for everyone:\nAccuracy = 900/1000 = 90% (still high, but misses ALL cancer!)"
      },
      "tags": [
        "accuracy",
        "classification",
        "metrics"
      ],
      "relatedConcepts": [
        "confusion-matrix",
        "precision",
        "recall"
      ],
      "prerequisites": [
        "confusion-matrix"
      ],
      "difficulty": "beginner"
    },
    {
      "id": "precision",
      "name": "Precision",
      "parentId": "classification-metrics",
      "sectionId": "6.1.3",
      "level": 2,
      "fullExplanation": "Precision measures the proportion of positive predictions that are actually positive: TP / (TP + FP). High precision means few false alarms. Important when false positive cost is high (spam filter—don't want to block legitimate emails, criminal conviction—don't want to convict innocent people).",
      "simpleExplanation": "Of all the times the model said \"positive,\" how often was it right? High precision = few false alarms. When model says \"yes,\" you can trust it.",
      "example": {
        "description": "Email spam filter:\n- Model flags 100 emails as spam\n- 90 are actually spam (TP)\n- 10 are legitimate emails (FP)\n\nPrecision = 90 / (90 + 10) = 90%\n\n\"When I say spam, I'm right 90% of the time.\""
      },
      "tags": [
        "precision",
        "classification",
        "metrics",
        "false-positive"
      ],
      "relatedConcepts": [
        "recall",
        "f1-score",
        "confusion-matrix"
      ],
      "prerequisites": [
        "confusion-matrix"
      ],
      "difficulty": "beginner"
    },
    {
      "id": "recall",
      "name": "Recall (Sensitivity)",
      "parentId": "classification-metrics",
      "sectionId": "6.1.4",
      "level": 2,
      "fullExplanation": "Recall (sensitivity, true positive rate) measures the proportion of actual positives correctly identified: TP / (TP + FN). High recall means few missed positives. Important when false negative cost is high (cancer detection—don't want to miss cancer, fraud detection—don't want to miss fraud).",
      "simpleExplanation": "Of all the actual positives, how many did the model find? High recall = few misses. Model catches most of the positives that exist.",
      "example": {
        "description": "Cancer detection:\n- 100 patients actually have cancer\n- Model identifies 85 of them (TP)\n- Model misses 15 (FN)\n\nRecall = 85 / (85 + 15) = 85%\n\n\"I catch 85% of all cancer cases.\""
      },
      "tags": [
        "recall",
        "sensitivity",
        "classification",
        "metrics",
        "false-negative"
      ],
      "relatedConcepts": [
        "precision",
        "f1-score",
        "confusion-matrix"
      ],
      "prerequisites": [
        "confusion-matrix"
      ],
      "difficulty": "beginner"
    },
    {
      "id": "f1-score",
      "name": "F1 Score",
      "parentId": "classification-metrics",
      "sectionId": "6.1.5",
      "level": 2,
      "fullExplanation": "F1 Score is the harmonic mean of precision and recall: 2 × (Precision × Recall) / (Precision + Recall). It balances both metrics—high F1 requires both high precision and high recall. Preferred over accuracy for imbalanced datasets. Range 0-1, higher is better.",
      "simpleExplanation": "Combined score balancing precision and recall. You can't game it by being extreme on one metric—both need to be good. If precision is 100% but recall is 10%, F1 is only 18%.",
      "example": {
        "description": "Model A: Precision=95%, Recall=60%\nF1 = 2 × (0.95 × 0.60) / (0.95 + 0.60) = 0.73\n\nModel B: Precision=80%, Recall=80%\nF1 = 2 × (0.80 × 0.80) / (0.80 + 0.80) = 0.80\n\nModel B wins despite lower precision—better balanced."
      },
      "tags": [
        "f1-score",
        "classification",
        "metrics",
        "harmonic-mean"
      ],
      "relatedConcepts": [
        "precision",
        "recall",
        "accuracy"
      ],
      "prerequisites": [
        "precision",
        "recall"
      ],
      "difficulty": "beginner"
    },
    {
      "id": "auc-roc",
      "name": "AUC-ROC",
      "parentId": "classification-metrics",
      "sectionId": "6.1.6",
      "level": 2,
      "fullExplanation": "AUC-ROC is the Area Under the Receiver Operating Characteristic curve. ROC plots True Positive Rate vs False Positive Rate at all classification thresholds. AUC summarizes performance across all thresholds: 1.0 is perfect, 0.5 is random guessing. Measures ranking ability—probability that random positive ranks higher than random negative.",
      "simpleExplanation": "How good is the model at ranking positives above negatives, regardless of threshold? AUC=0.5 means random, AUC=1.0 means perfect separation. Useful because it's threshold-independent—evaluates the model's overall discrimination ability.",
      "example": {
        "description": "Model outputs probabilities:\n- Actual frauds get scores: 0.8, 0.9, 0.75, 0.85\n- Actual non-frauds get scores: 0.2, 0.1, 0.3, 0.15\n\nAUC = 1.0 (all frauds rank above all non-frauds)\n\nIf scores overlapped:\n- Frauds: 0.6, 0.5, 0.7\n- Non-frauds: 0.55, 0.4, 0.3\n\nAUC ≈ 0.85 (some overlap but still good separation)",
        "code": "from sklearn.metrics import roc_auc_score, roc_curve\nimport matplotlib.pyplot as plt\n\n# Calculate AUC\ny_true = [1, 1, 0, 0, 1, 0]\ny_scores = [0.9, 0.8, 0.3, 0.2, 0.7, 0.4]\nauc = roc_auc_score(y_true, y_scores)\nprint(f\"AUC: {auc:.2f}\")\n\n# Plot ROC curve\nfpr, tpr, _ = roc_curve(y_true, y_scores)\nplt.plot(fpr, tpr, label=f'ROC (AUC={auc:.2f})')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()",
        "codeLanguage": "python"
      },
      "tags": [
        "auc",
        "roc",
        "classification",
        "metrics",
        "threshold-independent"
      ],
      "relatedConcepts": [
        "precision",
        "recall",
        "confusion-matrix"
      ],
      "prerequisites": [
        "classification-metrics"
      ],
      "difficulty": "intermediate"
    },
    {
      "id": "clustering",
      "name": "Clustering",
      "parentId": "unsupervised-learning",
      "sectionId": "12.1",
      "level": 1,
      "fullExplanation": "Clustering covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about clustering and related techniques.",
      "example": {
        "description": "This section contains concepts related to clustering."
      },
      "tags": [
        "unsupervised",
        "clustering"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "dimensionality-reduction",
      "name": "Dimensionality Reduction",
      "parentId": "unsupervised-learning",
      "sectionId": "12.2",
      "level": 1,
      "fullExplanation": "Dimensionality Reduction covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about dimensionality reduction and related techniques.",
      "example": {
        "description": "This section contains concepts related to dimensionality reduction."
      },
      "tags": [
        "unsupervised",
        "clustering",
        "dimensionality",
        "reduction"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "density-estimation",
      "name": "Density Estimation",
      "parentId": "unsupervised-learning",
      "sectionId": "12.3",
      "level": 1,
      "fullExplanation": "Density Estimation covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about density estimation and related techniques.",
      "example": {
        "description": "This section contains concepts related to density estimation."
      },
      "tags": [
        "unsupervised",
        "clustering",
        "density",
        "estimation"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "anomaly-detection",
      "name": "Anomaly Detection",
      "parentId": "unsupervised-learning",
      "sectionId": "12.4",
      "level": 1,
      "fullExplanation": "Anomaly Detection covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about anomaly detection and related techniques.",
      "example": {
        "description": "This section contains concepts related to anomaly detection."
      },
      "tags": [
        "unsupervised",
        "clustering",
        "anomaly",
        "detection"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "clustering",
      "name": "Clustering",
      "parentId": "clustering",
      "sectionId": "12.1.1",
      "level": 2,
      "fullExplanation": "Clustering groups unlabeled data points into clusters where points within a cluster are more similar to each other than to points in other clusters. No ground truth labels exist—the algorithm discovers structure. Clustering can be partition-based (K-means), hierarchical (agglomerative), density-based (DBSCAN), or model-based (GMM). Evaluation metrics include silhouette score, Davies-Bouldin index, and domain-specific validation.",
      "simpleExplanation": "Find natural groups in data without being told what groups exist. Customers might naturally cluster into \"budget shoppers,\" \"luxury buyers,\" etc. The algorithm discovers these groups from patterns in the data.",
      "example": {
        "description": "Customer data (no labels):\n```\nCustomer purchase patterns plotted in 2D:\n\n    $$$|           * *\n       |          * * *    <- Luxury cluster\n       |\n       |   x x x\n       |  x x x x          <- Mid-range cluster\n       |\n       |o o\n       |o o o              <- Budget cluster\n       +"
      },
      "tags": [
        "unsupervised",
        "clustering"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "k-means",
      "name": "K-Means Clustering",
      "parentId": "clustering",
      "sectionId": "12.1.2",
      "level": 2,
      "fullExplanation": "K-Means partitions data into K clusters by minimizing within-cluster variance. Algorithm: (1) Initialize K centroids randomly, (2) Assign each point to nearest centroid, (3) Recompute centroids as cluster means, (4) Repeat until convergence. Finds spherical clusters of similar size. Requires specifying K beforehand. Sensitive to initialization—use K-means++ or run multiple times.",
      "simpleExplanation": "Pick K cluster centers, assign each point to the nearest center, move centers to the middle of their points, repeat. Simple and fast. Works best when clusters are ball-shaped and similar size.",
      "example": {
        "description": "K=3 clustering:\n\n\n```python\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=3, init='k-means++')\nlabels = kmeans.fit_predict(X)\n```",
        "code": "Step 1: Random centroids: A, B, C\n\nStep 2: Assign points to nearest centroid\n        Points near A → Cluster 1\n        Points near B → Cluster 2\n        Points near C → Cluster 3\n\nStep 3: Move centroids to cluster means\n        A' = mean of Cluster 1\n        B' = mean of Cluster 2\n        C' = mean of Cluster 3\n\nStep 4: Reassign points with new centroids\n        Repeat until centroids stop moving",
        "codeLanguage": "python"
      },
      "tags": [
        "unsupervised",
        "clustering",
        "means"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "elbow-method",
      "name": "Elbow Method",
      "parentId": "clustering",
      "sectionId": "12.1.3",
      "level": 2,
      "fullExplanation": "The elbow method helps select K for K-means by plotting within-cluster sum of squares (WCSS) against K. As K increases, WCSS decreases (more clusters = tighter fit). Look for an \"elbow\" where the rate of decrease sharply changes—additional clusters beyond this point provide diminishing returns. Subjective but widely used. Supplement with silhouette analysis or domain knowledge.",
      "simpleExplanation": "Try different numbers of clusters, plot the \"tightness\" of clusters. Initially, adding clusters helps a lot. At some point, adding more doesn't help much—that's the elbow. Choose K at the elbow.",
      "example": {
        "description": "```\nWCSS vs K:\n\nWCSS |\n     |*\n     | *\n     |  *\n     |   *\n     |    * <-- Elbow at K=4\n     |     *  *  *  *\n     +"
      },
      "tags": [
        "unsupervised",
        "clustering",
        "elbow",
        "method"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "hierarchical-clustering",
      "name": "Hierarchical Clustering",
      "parentId": "clustering",
      "sectionId": "12.1.4",
      "level": 2,
      "fullExplanation": "Hierarchical clustering builds a tree (dendrogram) of clusters. Agglomerative (bottom-up): start with each point as its own cluster, repeatedly merge closest clusters. Divisive (top-down): start with one cluster, repeatedly split. Linkage methods define cluster distance: single (min), complete (max), average, Ward (minimizes variance). Cut dendrogram at desired level to get K clusters. No need to specify K upfront.",
      "simpleExplanation": "Build a tree of clusters from bottom up. Start with each point alone. Merge the two closest clusters. Repeat until one cluster remains. Cut the tree at any level to get different numbers of clusters.",
      "example": {
        "description": "",
        "code": "Agglomerative clustering of points A, B, C, D, E:\n\nStep 1: {A}, {B}, {C}, {D}, {E} (5 clusters)\nStep 2: Merge closest: {A,B}, {C}, {D}, {E} (4 clusters)\nStep 3: Merge: {A,B}, {C,D}, {E} (3 clusters)\nStep 4: Merge: {A,B,E}, {C,D} (2 clusters)\nStep 5: Merge: {A,B,C,D,E} (1 cluster)\n\nDendrogram:\n        ___|___\n       |       |\n    ___|___    |\n   |       |   |\n  _|_     _|_  |\n |   |   |   | |\n A   B   C   D E",
        "codeLanguage": "python"
      },
      "tags": [
        "unsupervised",
        "clustering",
        "hierarchical"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "dbscan",
      "name": "DBSCAN",
      "parentId": "clustering",
      "sectionId": "12.1.5",
      "level": 2,
      "fullExplanation": "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) finds clusters of arbitrary shape based on density. Parameters: ε (neighborhood radius) and minPts (minimum points to form dense region). Core points have ≥minPts neighbors within ε; border points are within ε of core points; noise points are neither. Doesn't require specifying K, handles noise naturally, finds non-spherical clusters.",
      "simpleExplanation": "Find dense regions separated by sparse regions. A point is \"core\" if it has enough neighbors nearby. Clusters are connected dense regions. Points in sparse areas are labeled as noise. Great for finding oddly-shaped clusters and handling outliers.",
      "example": {
        "description": "```python\nfrom sklearn.cluster import DBSCAN\ndbscan = DBSCAN(eps=0.5, min_samples=5)\nlabels = dbscan.fit_predict(X)  # -1 indicates noise\n```",
        "code": "DBSCAN with ε=1, minPts=3:\n\n  * * *\n   * *         <- Dense cluster (all core/border points)\n    *\n\n        * *\n         *     <- Dense cluster\n\n  *            <- Noise (isolated point)\n\nResults:\n- Cluster 1: Top group\n- Cluster 2: Middle group\n- Noise: Isolated point (not enough neighbors)",
        "codeLanguage": "python"
      },
      "tags": [
        "unsupervised",
        "clustering",
        "dbscan"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "gmm",
      "name": "Gaussian Mixture Model",
      "parentId": "clustering",
      "sectionId": "12.1.6",
      "level": 2,
      "fullExplanation": "GMM assumes data is generated from a mixture of K Gaussian distributions. Each Gaussian has its own mean μₖ and covariance Σₖ with mixing coefficient πₖ. Training via EM algorithm: E-step computes soft cluster assignments (probabilities), M-step updates parameters. Provides probabilistic cluster membership (soft clustering) and models elliptical clusters of different sizes/orientations.",
      "simpleExplanation": "Assume data comes from K bell curves (Gaussians) overlapping. Find the best K bell curves to explain the data. Unlike K-means, gives probabilities (soft assignments) and handles elliptical clusters.",
      "example": {
        "description": "```python\nfrom sklearn.mixture import GaussianMixture\ngmm = GaussianMixture(n_components=3)\ngmm.fit(X)\nprobabilities = gmm.predict_proba(X)  # Soft assignments\nlabels = gmm.predict(X)  # Hard assignments\n```",
        "code": "GMM with K=2:\n\nGaussian 1: mean=[0,0], cov=[[1,0],[0,1]]\nGaussian 2: mean=[3,3], cov=[[2,1],[1,2]]\n\nFor a new point x=[1,1]:\nP(from Gaussian 1) = 0.7\nP(from Gaussian 2) = 0.3\n→ Soft assignment: mostly Gaussian 1\n\nK-means would give hard assignment: Cluster 1 only.\nGMM gives probabilities: 70% Cluster 1, 30% Cluster 2.",
        "codeLanguage": "python"
      },
      "tags": [
        "unsupervised",
        "clustering",
        "gaussian",
        "mixture",
        "model"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "dimensionality-reduction",
      "name": "Dimensionality Reduction",
      "parentId": "dimensionality-reduction",
      "sectionId": "12.2.1",
      "level": 2,
      "fullExplanation": "Dimensionality reduction maps high-dimensional data to lower dimensions while preserving important structure. Motivations: visualization (2D/3D), noise reduction, computational efficiency, fighting curse of dimensionality. Linear methods (PCA, LDA) find linear subspaces; nonlinear methods (t-SNE, UMAP) can unfold complex manifolds. Trade-off between preserving global vs local structure.",
      "simpleExplanation": "Reduce the number of features while keeping important information. 1000 features → 50 features. Helps visualization, speeds up training, and removes noise. Different methods preserve different aspects of the data.",
      "example": {
        "description": "",
        "code": "Gene expression data: 20,000 genes × 100 patients\n\nToo many dimensions to visualize or analyze!\n\nAfter PCA → 50 dimensions:\n- Captures 95% of variance\n- Visualization possible (first 2-3 PCs)\n- Clustering works better\n- Model training 400× faster\n\nAfter t-SNE → 2 dimensions:\n- Beautiful visualization\n- Cancer patients cluster separately\n- Reveals subtypes invisible in raw data",
        "codeLanguage": "python"
      },
      "tags": [
        "unsupervised",
        "clustering",
        "dimensionality",
        "reduction"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "pca",
      "name": "Principal Component Analysis (PCA)",
      "parentId": "dimensionality-reduction",
      "sectionId": "12.2.2",
      "level": 2,
      "fullExplanation": "PCA finds orthogonal directions (principal components) of maximum variance. First PC captures most variance, second PC captures most remaining variance orthogonal to first, and so on. Computed via eigendecomposition of covariance matrix or SVD. Linear, fast, deterministic. Preserves global structure but may miss local nonlinear patterns. Use explained variance ratio to choose dimensionality.",
      "simpleExplanation": "Find the directions where data varies most. Project data onto these directions. First direction (PC1) is most important, then PC2, etc. Like finding the \"main axes\" of your data cloud.",
      "example": {
        "description": "```python\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=50)\nX_reduced = pca.fit_transform(X)\nprint(pca.explained_variance_ratio_.sum())  # e.g., 0.95 = 95% variance kept\n```",
        "code": "2D data with strong correlation:\n\n     *   *\n   *   *   *\n *   *   *   *      PC1 (most variance)\n   *   *   *        ─────────>\n     *   *            /\n                     /\n                    / PC2 (orthogonal)\n\nProject onto PC1 only: 2D → 1D\nKeep most of the information (high variance direction)\nLose little information (low variance direction)",
        "codeLanguage": "python"
      },
      "tags": [
        "unsupervised",
        "clustering",
        "principal",
        "component",
        "analysis"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "tsne",
      "name": "t-SNE",
      "parentId": "dimensionality-reduction",
      "sectionId": "12.2.3",
      "level": 2,
      "fullExplanation": "t-SNE (t-distributed Stochastic Neighbor Embedding) creates low-dimensional embeddings preserving local neighborhood structure. Converts point similarities to probabilities in high and low dimensions, minimizes KL divergence between them. Uses t-distribution in low-D to prevent crowding. Excellent for visualization (2D/3D). Non-deterministic, slow, not suitable for new data projection. Perplexity parameter controls local vs global emphasis.",
      "simpleExplanation": "Arrange points in 2D so nearby points stay nearby, far points stay far. Optimizes a \"similarity\" match between original and reduced space. Amazing for visualization, reveals clusters and patterns. Don't use for actual dimensionality reduction—only visualization.",
      "example": {
        "description": "",
        "code": "MNIST digits (784D) → t-SNE → 2D:\n\nOriginal: 784-dimensional (28×28 pixels)\nCannot visualize!\n\nAfter t-SNE:\n  0 0        1 1\n   0 0      1  1\n    0 0    1    1\n\n      2 2    3\n     2  2  3 3\n       2    3\n\nEach digit class forms a cluster in 2D!\nBeautiful visualization of 784D structure.",
        "codeLanguage": "python"
      },
      "tags": [
        "unsupervised",
        "clustering"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "umap",
      "name": "UMAP",
      "parentId": "dimensionality-reduction",
      "sectionId": "12.2.4",
      "level": 2,
      "fullExplanation": "UMAP (Uniform Manifold Approximation and Projection) learns manifold structure using fuzzy topology. Faster than t-SNE, preserves more global structure, and can embed new data (has transform method). Uses cross-entropy loss instead of KL divergence. Parameters: n_neighbors (local vs global), min_dist (cluster tightness), metric (distance function). Widely used for visualization and dimensionality reduction.",
      "simpleExplanation": "Like t-SNE but faster, preserves more global structure, and can apply to new data. The current state-of-the-art for visualization. Good for both visualization and actual dimensionality reduction.",
      "example": {
        "description": "UMAP vs t-SNE:\n- Speed: UMAP 10-100× faster\n- Global structure: UMAP preserves better\n- New data: UMAP can transform, t-SNE cannot\n- Quality: Both excellent for visualization",
        "code": "import umap\n\n# Fast dimensionality reduction\nreducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2)\nembedding = reducer.fit_transform(X)\n\n# Can transform new data (unlike t-SNE)\nnew_embedding = reducer.transform(X_new)",
        "codeLanguage": "python"
      },
      "tags": [
        "unsupervised",
        "clustering",
        "umap"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "autoencoder-dim-reduction",
      "name": "Autoencoders for Dimensionality Reduction",
      "parentId": "dimensionality-reduction",
      "sectionId": "12.2.5",
      "level": 2,
      "fullExplanation": "Autoencoders learn compressed representations by training to reconstruct inputs. Encoder compresses input to bottleneck (latent space); decoder reconstructs from latent space. Bottleneck dimension is the reduced dimensionality. Nonlinear compression (unlike PCA). Variational autoencoders add probabilistic structure. Can handle very high-dimensional data (images, text) and learn complex manifolds.",
      "simpleExplanation": "Train a neural network to compress and decompress data. The compression in the middle is your reduced representation. Learns whatever compression works best for reconstruction. Can capture nonlinear patterns that PCA misses.",
      "example": {
        "description": "",
        "code": "Image autoencoder:\n\nInput: 28×28 = 784 pixels\n       ↓\nEncoder: 784 → 512 → 256 → 128 → 32\n       ↓\nBottleneck: 32 dimensions (compressed representation!)\n       ↓\nDecoder: 32 → 128 → 256 → 512 → 784\n       ↓\nOutput: 28×28 = 784 pixels (reconstruction)\n\nLoss: ||input - output||² (reconstruction error)\n\nAfter training:\n- Encoder gives 32D representation\n- 32D captures essence of image\n- 96% compression with good reconstruction",
        "codeLanguage": "python"
      },
      "tags": [
        "unsupervised",
        "clustering",
        "autoencoders",
        "dimensionality",
        "reduction"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "density-estimation",
      "name": "Density Estimation",
      "parentId": "density-estimation",
      "sectionId": "12.3.1",
      "level": 2,
      "fullExplanation": "Density estimation models the probability distribution of data. Parametric methods assume a distribution form (Gaussian, GMM) and estimate parameters. Non-parametric methods (KDE, histograms) make minimal assumptions. Used for: understanding data distribution, anomaly detection (low-density = anomaly), sampling new data, and computing likelihoods. Foundation for many generative models.",
      "simpleExplanation": "Learn the shape of your data distribution. Where is data concentrated? Where is it sparse? Useful for understanding data, detecting outliers, and generating new samples.",
      "example": {
        "description": "```\nCustomer transaction amounts:\n\nHistogram (discrete):\nCount |****\n      |*********\n      |**************\n      |********\n      |***\n      +"
      },
      "tags": [
        "unsupervised",
        "clustering",
        "density",
        "estimation"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "kde",
      "name": "Kernel Density Estimation (KDE)",
      "parentId": "density-estimation",
      "sectionId": "12.3.2",
      "level": 2,
      "fullExplanation": "KDE estimates density by placing a kernel (typically Gaussian) at each data point and summing. p(x) = (1/nh)Σᵢ K((x-xᵢ)/h), where h is bandwidth (smoothing parameter). Small h = spiky, overfit; large h = smooth, underfit. Non-parametric—no assumption about distribution shape. Choice of bandwidth is crucial; use cross-validation or rules like Silverman's.",
      "simpleExplanation": "Put a small bump (Gaussian) at each data point, add them all up. More data points nearby = higher density. Bandwidth controls smoothness: small = bumpy, large = smooth.",
      "example": {
        "description": "",
        "code": "Data points: [2, 3, 3.5, 10]\n\nSmall bandwidth (h=0.5):\n      _   __\n     / \\ /  \\\n    /   v    \\                   _\n___/          \\_________________/ \\___\n   2  3 3.5                     10\n\nLarge bandwidth (h=2):\n       ____\n      /    \\\n     /      \\                 _\n    /        \\_______________/ \\\n___/                            \\___\n   2  3 3.5                     10\n\nMedium bandwidth best: captures two-peak structure clearly.",
        "codeLanguage": "python"
      },
      "tags": [
        "unsupervised",
        "clustering",
        "kernel",
        "density",
        "estimation"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "anomaly-detection",
      "name": "Anomaly Detection",
      "parentId": "anomaly-detection",
      "sectionId": "12.4.1",
      "level": 2,
      "fullExplanation": "Anomaly detection identifies rare observations that differ significantly from the majority. Approaches: statistical (z-score, IQR), density-based (low density = anomaly), distance-based (far from neighbors), reconstruction-based (high reconstruction error), and isolation-based (easy to isolate = anomaly). Applications: fraud detection, intrusion detection, equipment failure prediction, quality control.",
      "simpleExplanation": "Find the weird ones. Anomalies are data points that don't fit the normal pattern. Could be fraud, equipment failure, or data errors. Different methods define \"unusual\" differently.",
      "example": {
        "description": "",
        "code": "Credit card transactions:\n\nNormal: $50 grocery, $30 gas, $15 coffee, $80 restaurant\nAnomaly: $5000 electronics in foreign country at 3am\n\nDetection methods:\n1. Statistical: Spending way above mean (z-score > 3)\n2. Density: Very few transactions with this pattern\n3. Distance: Far from typical transaction clusters\n4. Isolation: Very easy to separate from other transactions",
        "codeLanguage": "python"
      },
      "tags": [
        "unsupervised",
        "clustering",
        "anomaly",
        "detection"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "isolation-forest",
      "name": "Isolation Forest",
      "parentId": "anomaly-detection",
      "sectionId": "12.4.2",
      "level": 2,
      "fullExplanation": "Isolation Forest detects anomalies by isolating points using random trees. Algorithm: randomly select feature and split value; anomalies require fewer splits to isolate (they're different from the crowd). Anomaly score based on average path length across many trees. Fast O(n log n), works well in high dimensions, no density estimation needed. Score interpretation: < 0.5 normal, > 0.5 anomaly.",
      "simpleExplanation": "Try to isolate each point with random splits. Normal points are buried in clusters—need many splits. Anomalies are different—can be isolated quickly. Points that isolate easily are anomalies.",
      "example": {
        "description": "```python\nfrom sklearn.ensemble import IsolationForest\niso_forest = IsolationForest(contamination=0.01)  # Expect 1% anomalies\npredictions = iso_forest.fit_predict(X)  # -1 = anomaly, 1 = normal\n```",
        "code": "Random tree isolation:\n\nPoint A (normal, in cluster):\nSplit 1: Still with 50% of data\nSplit 2: Still with 25% of data\nSplit 3: Still with 10% of data\nSplit 4: Still with 5% of data\nSplit 5: Finally isolated!\n→ Path length = 5 (normal)\n\nPoint B (anomaly, isolated):\nSplit 1: Isolated!\n→ Path length = 1 (very short = anomaly!)",
        "codeLanguage": "python"
      },
      "tags": [
        "unsupervised",
        "clustering",
        "isolation",
        "forest"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "one-class-svm",
      "name": "One-Class SVM",
      "parentId": "anomaly-detection",
      "sectionId": "12.4.3",
      "level": 2,
      "fullExplanation": "One-Class SVM learns a boundary around normal data. Training uses only normal examples; the model finds a hyperplane (in kernel space) that separates normal data from the origin with maximum margin. New points falling outside the boundary are anomalies. Uses kernel trick for nonlinear boundaries. Parameter ν controls the fraction of outliers expected. Effective but computationally expensive for large datasets.",
      "simpleExplanation": "Draw a boundary around normal data. Everything outside the boundary is an anomaly. Uses SVM techniques to find the best boundary. Train on normal examples only—no need for labeled anomalies.",
      "example": {
        "description": "```\nNormal data forms a cluster:\n\n    Boundary\n   /"
      },
      "tags": [
        "unsupervised",
        "clustering",
        "class"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "autoencoder-anomaly",
      "name": "Autoencoder Anomaly Detection",
      "parentId": "anomaly-detection",
      "sectionId": "12.4.4",
      "level": 2,
      "fullExplanation": "Autoencoders detect anomalies via reconstruction error. Train on normal data; the model learns to compress and reconstruct normal patterns. Anomalies, being different from training data, have high reconstruction error. Threshold on reconstruction error determines anomaly classification. Works well for complex data (images, sequences). Threshold selection is crucial—use validation data or percentile-based approaches.",
      "simpleExplanation": "Train autoencoder on normal data. It learns to reconstruct normal patterns well. Anomalies look different → can't reconstruct them well → high error = anomaly.",
      "example": {
        "description": "",
        "code": "Autoencoder for network traffic:\n\nTraining: Normal traffic patterns\nModel learns: \"Normal traffic looks like this\"\n\nTesting:\nNormal packet: input ≈ output, error = 0.01 ✓\nNormal packet: input ≈ output, error = 0.02 ✓\nAttack packet: input ≠ output, error = 0.85 ← Anomaly!\n\nThreshold = 0.1:\nError < 0.1 → Normal\nError > 0.1 → Anomaly",
        "codeLanguage": "python"
      },
      "tags": [
        "unsupervised",
        "clustering",
        "autoencoder",
        "anomaly",
        "detection"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "autoencoders",
      "name": "Autoencoders",
      "parentId": "generative-models",
      "sectionId": "13.1",
      "level": 1,
      "fullExplanation": "Autoencoders covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about autoencoders and related techniques.",
      "example": {
        "description": "This section contains concepts related to autoencoders."
      },
      "tags": [
        "generative",
        "generation",
        "autoencoders"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "gans-generative-adversarial-networks",
      "name": "GANs (Generative Adversarial Networks)",
      "parentId": "generative-models",
      "sectionId": "13.2",
      "level": 1,
      "fullExplanation": "GANs (Generative Adversarial Networks) covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about gans (generative adversarial networks) and related techniques.",
      "example": {
        "description": "This section contains concepts related to gans (generative adversarial networks)."
      },
      "tags": [
        "generative",
        "generation",
        "gans",
        "(generative",
        "adversarial"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "normalizing-flows",
      "name": "Normalizing Flows",
      "parentId": "generative-models",
      "sectionId": "13.3",
      "level": 1,
      "fullExplanation": "Normalizing Flows covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about normalizing flows and related techniques.",
      "example": {
        "description": "This section contains concepts related to normalizing flows."
      },
      "tags": [
        "generative",
        "generation",
        "normalizing",
        "flows"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "diffusion-models",
      "name": "Diffusion Models",
      "parentId": "generative-models",
      "sectionId": "13.4",
      "level": 1,
      "fullExplanation": "Diffusion Models covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about diffusion models and related techniques.",
      "example": {
        "description": "This section contains concepts related to diffusion models."
      },
      "tags": [
        "generative",
        "generation",
        "diffusion",
        "models"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "language-models",
      "name": "Language Models",
      "parentId": "generative-models",
      "sectionId": "13.5",
      "level": 1,
      "fullExplanation": "Language Models covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about language models and related techniques.",
      "example": {
        "description": "This section contains concepts related to language models."
      },
      "tags": [
        "generative",
        "generation",
        "language",
        "models"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "large-language-models",
      "name": "Large Language Models",
      "parentId": "generative-models",
      "sectionId": "13.6",
      "level": 1,
      "fullExplanation": "Large Language Models covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about large language models and related techniques.",
      "example": {
        "description": "This section contains concepts related to large language models."
      },
      "tags": [
        "generative",
        "generation",
        "large",
        "language",
        "models"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "autoencoder",
      "name": "Autoencoder",
      "parentId": "autoencoders",
      "sectionId": "13.1.1",
      "level": 2,
      "fullExplanation": "Autoencoders learn compressed representations by training to reconstruct inputs. Architecture: encoder f(x) → latent code z, decoder g(z) → reconstruction x̂. Loss: ||x - x̂||². The bottleneck forces compression, learning essential features. Not truly generative (can't sample new data) but foundation for VAEs. Applications: dimensionality reduction, denoising, pretraining, anomaly detection.",
      "simpleExplanation": "Compress data, then decompress it back. The compressed form in the middle captures the essence. Like learning to describe images with few words, then recreating images from those descriptions.",
      "example": {
        "description": "",
        "code": "Face autoencoder:\n\nInput: 64×64×3 face image\n       ↓\nEncoder: Conv layers → 128D vector\n       ↓\nLatent: 128 numbers describing the face\n        (pose, expression, lighting, identity)\n       ↓\nDecoder: Deconv layers → 64×64×3\n       ↓\nOutput: Reconstructed face\n\nThe 128D latent space captures face attributes.\nSimilar faces have similar latent vectors.",
        "codeLanguage": "python"
      },
      "tags": [
        "generative",
        "generation",
        "autoencoder"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "vae",
      "name": "Variational Autoencoder (VAE)",
      "parentId": "autoencoders",
      "sectionId": "13.1.2",
      "level": 2,
      "fullExplanation": "VAE adds probabilistic structure to autoencoders. Encoder outputs distribution parameters (μ, σ) instead of a point; z is sampled from N(μ, σ²). Loss combines reconstruction and KL divergence regularization: L = ||x-x̂||² + KL(q(z|x) || p(z)). KL term keeps latent space organized (close to N(0,1)), enabling sampling and interpolation. Reparameterization trick: z = μ + σ⊙ε (ε~N(0,1)) enables backprop.",
      "simpleExplanation": "Autoencoder where the compressed form is a probability distribution, not a fixed point. Adds randomness during training. Result: can sample from the latent space to generate new data. The first truly generative autoencoder.",
      "example": {
        "description": "",
        "code": "VAE generation:\n\nTraining:\nFace → Encoder → μ=[0.5, -0.3, ...], σ=[0.1, 0.2, ...]\nSample z ~ N(μ, σ²)\nz → Decoder → Reconstructed face\n\nGenerating new faces:\nSample z ~ N(0, 1)  (random point in latent space)\nz → Decoder → New face!\n\nInterpolation:\nz₁ = encode(face1), z₂ = encode(face2)\nz_interp = 0.5×z₁ + 0.5×z₂\ndecode(z_interp) → face between face1 and face2",
        "codeLanguage": "python"
      },
      "tags": [
        "generative",
        "generation",
        "variational",
        "autoencoder",
        "(vae)"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "denoising-autoencoder",
      "name": "Denoising Autoencoder",
      "parentId": "autoencoders",
      "sectionId": "13.1.3",
      "level": 2,
      "fullExplanation": "Denoising autoencoders add noise to inputs and train to reconstruct clean originals. Noise can be Gaussian, dropout (masking), or salt-and-pepper. Forces the model to learn robust features that capture essential structure, not pixel-level details. Prevents learning identity function. Better representations than standard autoencoders. Foundation for denoising diffusion models.",
      "simpleExplanation": "Add noise to input, try to reconstruct the clean version. Can't just memorize—must truly understand the data to remove noise. Learns more meaningful representations than regular autoencoders.",
      "example": {
        "description": "",
        "code": "Denoising autoencoder training:\n\nOriginal: Clean image of a cat\n       ↓ Add noise\nCorrupted: Fuzzy image with random dots\n       ↓\nEncoder → Latent\n       ↓\nDecoder\n       ↓\nOutput: Should be clean cat!\n\nLoss: Compare output to ORIGINAL clean image\n\nThe model learns:\n- What cats look like (to denoise)\n- Not just pixel copying (noise makes that fail)",
        "codeLanguage": "python"
      },
      "tags": [
        "generative",
        "generation",
        "denoising",
        "autoencoder"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "gan",
      "name": "Generative Adversarial Network",
      "parentId": "gans-generative-adversarial-networks",
      "sectionId": "13.2.1",
      "level": 2,
      "fullExplanation": "GANs train two networks adversarially: Generator G creates fake samples from noise; Discriminator D distinguishes real from fake. Training alternates: D learns to detect fakes, G learns to fool D. Game-theoretic equilibrium: G produces samples indistinguishable from real. min_G max_D E[log D(x)] + E[log(1-D(G(z)))]. Produces high-quality samples but training is notoriously unstable.",
      "simpleExplanation": "Two networks compete: one creates fakes, one detects fakes. Generator gets better at making fakes; Discriminator gets better at detecting them. Eventually, Generator creates such good fakes that Discriminator can't tell the difference.",
      "example": {
        "description": "",
        "code": "GAN for face generation:\n\nGenerator:\nRandom noise z ~ N(0,1) (100 numbers)\n       ↓\nNeural network\n       ↓\nFake face image\n\nDiscriminator:\nFace image (real or fake)\n       ↓\nNeural network\n       ↓\nP(real) between 0 and 1\n\nTraining loop:\n1. D sees real faces → learns \"this is real\"\n2. D sees G's fakes → learns \"this is fake\"\n3. G adjusts to fool D → makes better fakes\n4. Repeat until G makes convincing faces",
        "codeLanguage": "python"
      },
      "tags": [
        "generative",
        "generation",
        "adversarial",
        "network"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "generator",
      "name": "Generator Network",
      "parentId": "gans-generative-adversarial-networks",
      "sectionId": "13.2.2",
      "level": 2,
      "fullExplanation": "The generator maps random noise to data samples: G: z → x̂. Typically uses transposed convolutions (deconvolutions) for images, upsampling latent vectors to full resolution. Trained to maximize log(D(G(z)))—make Discriminator think fakes are real. Architecture choices: progressive growing, style injection (StyleGAN), conditional inputs. Generates increasingly realistic samples as training progresses.",
      "simpleExplanation": "Creates fake data from random numbers. Starts with random noise, applies neural network transformations to produce images, audio, or text. Learns to produce outputs that fool the Discriminator.",
      "example": {
        "description": "",
        "code": "Image Generator architecture:\n\nz ~ N(0,1) [100 dims]\n       ↓\nDense: 100 → 4×4×512\n       ↓\nDeConv: 4×4×512 → 8×8×256\n       ↓\nDeConv: 8×8×256 → 16×16×128\n       ↓\nDeConv: 16×16×128 → 32×32×64\n       ↓\nDeConv: 32×32×64 → 64×64×3\n       ↓\nOutput: 64×64 RGB image\n\nRandom 100 numbers → Full image!",
        "codeLanguage": "python"
      },
      "tags": [
        "generative",
        "generation",
        "generator",
        "network"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "discriminator",
      "name": "Discriminator Network",
      "parentId": "gans-generative-adversarial-networks",
      "sectionId": "13.2.3",
      "level": 2,
      "fullExplanation": "The discriminator classifies inputs as real or fake: D: x → [0,1]. For images, typically uses convolutions to downsample. Trained on both real data (label 1) and generator outputs (label 0). Acts as a learned loss function—provides gradient signal to Generator about what makes samples unrealistic. Should not be too strong or too weak relative to Generator.",
      "simpleExplanation": "A detective trying to spot fakes. Looks at samples and outputs probability of being real. Trained on both real data and Generator's fakes. Provides feedback to Generator about what's still fake-looking.",
      "example": {
        "description": "",
        "code": "Image Discriminator architecture:\n\nInput: 64×64×3 image\n       ↓\nConv: 64×64×3 → 32×32×64\n       ↓\nConv: 32×32×64 → 16×16×128\n       ↓\nConv: 16×16×128 → 8×8×256\n       ↓\nConv: 8×8×256 → 4×4×512\n       ↓\nFlatten + Dense → 1 (sigmoid)\n       ↓\nOutput: P(real)\n\nP(real) > 0.5 → Probably real\nP(real) < 0.5 → Probably fake",
        "codeLanguage": "python"
      },
      "tags": [
        "generative",
        "generation",
        "discriminator",
        "network"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "mode-collapse",
      "name": "Mode Collapse",
      "parentId": "gans-generative-adversarial-networks",
      "sectionId": "13.2.4",
      "level": 2,
      "fullExplanation": "Mode collapse occurs when the Generator produces only a limited variety of samples, ignoring data distribution diversity. Generator finds a few outputs that reliably fool Discriminator and sticks with them. Example: face GAN generating only young white women, ignoring other demographics. Causes: Discriminator forgets earlier modes, Generator exploits Discriminator weaknesses. Solutions: minibatch discrimination, unrolled GANs, diversity-promoting regularization.",
      "simpleExplanation": "Generator takes shortcuts—produces only a few types of samples that fool Discriminator. Like a counterfeiter who only makes $20 bills perfectly but can't make other denominations. Variety is lost.",
      "example": {
        "description": "",
        "code": "MNIST digit GAN with mode collapse:\n\nExpected: Generate all digits 0-9\n\nCollapsed: Only generates 1s and 7s\n          (found these fool D reliably)\n\nSymptoms:\n- Generated samples look similar\n- Some categories never appear\n- Discriminator accuracy on real varies by category\n\nSolutions:\n- Add minibatch statistics\n- Penalize similar generated samples\n- Use WGAN or other stable architectures",
        "codeLanguage": "python"
      },
      "tags": [
        "generative",
        "generation",
        "mode",
        "collapse"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "wgan",
      "name": "Wasserstein GAN (WGAN)",
      "parentId": "gans-generative-adversarial-networks",
      "sectionId": "13.2.5",
      "level": 2,
      "fullExplanation": "WGAN uses Wasserstein distance (Earth Mover's distance) instead of JS divergence, providing smoother gradients and more stable training. Critic (not discriminator) outputs unbounded scores; difference between real and fake scores is minimized/maximized. Requires Lipschitz constraint on Critic—achieved via weight clipping (original) or gradient penalty (WGAN-GP). More stable training, meaningful loss values, reduced mode collapse.",
      "simpleExplanation": "A more stable GAN using a different way to measure how different fake and real distributions are. The Critic gives scores instead of probabilities. Training is smoother—loss actually correlates with sample quality.",
      "example": {
        "description": "",
        "code": "WGAN-GP training:\n\nCritic loss:\nL_C = E[C(fake)] - E[C(real)] + λ·gradient_penalty\n\nGenerator loss:\nL_G = -E[C(fake)]\n\nGradient penalty:\nFor interpolated samples x̂ = εx_real + (1-ε)x_fake\nGP = (||∇C(x̂)||₂ - 1)²\n\nBenefits:\n- Loss correlates with sample quality\n- More stable training\n- Less mode collapse\n- Can train critic more (5:1 ratio)",
        "codeLanguage": "python"
      },
      "tags": [
        "generative",
        "generation",
        "wasserstein",
        "(wgan)"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "stylegan",
      "name": "StyleGAN",
      "parentId": "gans-generative-adversarial-networks",
      "sectionId": "13.2.6",
      "level": 2,
      "fullExplanation": "StyleGAN revolutionized face generation with style-based architecture. Latent z maps to intermediate space W via mapping network. W controls generator via adaptive instance normalization (AdaIN) at each layer. Coarse layers control pose/face shape, fine layers control textures/colors. Progressive growing during training. Enables unprecedented control over generated attributes and mixing of styles.",
      "simpleExplanation": "Advanced GAN for high-quality face generation. Separates \"style\" (attributes like hair color, age) from \"structure.\" Can mix styles from different latents—take face shape from one, hair from another. State-of-the-art face synthesis.",
      "example": {
        "description": "",
        "code": "StyleGAN architecture:\n\nz ~ N(0,1) → Mapping Network → w (style vector)\n                                   ↓\nConstant 4×4 input → Style injection → 4×4\n                  → Style injection → 8×8\n                  → Style injection → 16×16\n                  ...\n                  → Style injection → 1024×1024\n\nStyle mixing:\nTake w₁ for layers 1-4 (coarse: face shape)\nTake w₂ for layers 5-8 (medium: features)\nTake w₃ for layers 9-12 (fine: texture)\n→ Mix attributes from different sources!",
        "codeLanguage": "python"
      },
      "tags": [
        "generative",
        "generation",
        "stylegan"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "normalizing-flow",
      "name": "Normalizing Flow",
      "parentId": "normalizing-flows",
      "sectionId": "13.3.1",
      "level": 2,
      "fullExplanation": "Normalizing flows learn invertible transformations between simple distributions (Gaussian) and complex data distributions. Chain of bijective functions: x = f(z) = fₙ ∘ fₙ₋₁ ∘ ... ∘ f₁(z). Log-likelihood tractable via change of variables: log p(x) = log p(z) - Σ log|det(∂fᵢ/∂z)|. Unlike VAEs and GANs, provides exact likelihood. Requires invertible architectures with efficient Jacobian computation.",
      "simpleExplanation": "Transform simple distribution to complex one through a chain of reversible steps. Can compute exact probability of any data point. Can generate new samples by sampling from simple distribution and transforming.",
      "example": {
        "description": "",
        "code": "Normalizing flow:\n\nSimple distribution (Gaussian):\nz ~ N(0, 1)\n\nChain of transformations:\nz → f₁ → h₁ → f₂ → h₂ → f₃ → x\n\nEach fᵢ is:\n- Invertible (can go backwards)\n- Has tractable Jacobian determinant\n\nGeneration: z → f₁ → f₂ → f₃ → x\nDensity: log p(x) = log p(z) - sum of log|det(Jacobians)|\nInversion: x → f₃⁻¹ → f₂⁻¹ → f₁⁻¹ → z",
        "codeLanguage": "python"
      },
      "tags": [
        "generative",
        "generation",
        "normalizing",
        "flow"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "diffusion-model",
      "name": "Diffusion Model",
      "parentId": "diffusion-models",
      "sectionId": "13.4.1",
      "level": 2,
      "fullExplanation": "Diffusion models gradually add noise to data (forward process) and learn to reverse this (reverse process). Forward: x₀ → x₁ → ... → xₜ (pure noise) via Gaussian noise addition. Reverse: learn neural network to denoise xₜ → xₜ₋₁ → ... → x₀. Training: predict noise added at each step. Generation: start from noise, iteratively denoise. State-of-the-art image synthesis (DALL-E, Stable Diffusion, Midjourney).",
      "simpleExplanation": "Slowly destroy data by adding noise, then learn to reverse the destruction. To generate: start with pure noise, gradually remove it to reveal a realistic sample. Like watching film grain accumulate, then learning to clean it up.",
      "example": {
        "description": "",
        "code": "Diffusion process:\n\nForward (training, fixed):\nClean image → Add noise → Add noise → ... → Pure noise\n    x₀     →    x₁     →    x₂    → ... →    xₜ\n\nReverse (learned):\nPure noise → Remove noise → Remove noise → ... → Clean image\n    xₜ     →    xₜ₋₁     →    xₜ₋₂    → ... →    x₀\n\nTraining:\nGiven xₜ and t, predict the noise ε that was added\nLoss = ||ε - ε_predicted||²\n\nGeneration:\n1. Sample pure noise xₜ ~ N(0,1)\n2. For t = T down to 0:\n   - Predict noise using neural network\n   - Remove predicted noise: xₜ₋₁ = denoise(xₜ)\n3. Return x₀",
        "codeLanguage": "python"
      },
      "tags": [
        "generative",
        "generation",
        "diffusion",
        "model"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "ddpm",
      "name": "DDPM (Denoising Diffusion Probabilistic Model)",
      "parentId": "diffusion-models",
      "sectionId": "13.4.2",
      "level": 2,
      "fullExplanation": "DDPM formalized diffusion models with rigorous probabilistic framework. Forward process: q(xₜ|xₜ₋₁) = N(√(1-βₜ)xₜ₋₁, βₜI). Reverse process: pθ(xₜ₋₁|xₜ) learned to approximate q(xₜ₋₁|xₜ,x₀). Training maximizes variational lower bound. Simple implementation: U-Net predicts noise, linear noise schedule. Produced high-quality samples rivaling GANs with more stable training.",
      "simpleExplanation": "The foundational paper that made diffusion models work well. Defined exact math for noising/denoising. Uses a U-Net to predict noise at each step. Simpler and more stable to train than GANs.",
      "example": {
        "description": "",
        "code": "DDPM specifics:\n\nNoise schedule:\nβ₁ = 0.0001, β₂ = 0.0002, ..., β₁₀₀₀ = 0.02\n\nForward: xₜ = √(αₜ)x₀ + √(1-αₜ)ε\nwhere αₜ = ∏(1-βᵢ)\n\nNeural network εθ predicts noise:\nInput: (xₜ, t)\nOutput: Predicted noise ε̂\n\nLoss: ||ε - ε̂||² (simple MSE!)\n\nGeneration (1000 steps):\nStart with pure noise\nDenoise step by step using εθ",
        "codeLanguage": "python"
      },
      "tags": [
        "generative",
        "generation",
        "ddpm",
        "(denoising",
        "diffusion"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "stable-diffusion",
      "name": "Stable Diffusion",
      "parentId": "diffusion-models",
      "sectionId": "13.4.3",
      "level": 2,
      "fullExplanation": "Stable Diffusion operates in learned latent space rather than pixel space, dramatically reducing computation. VAE encodes images to latent (4× smaller), diffusion happens in latent space, VAE decodes back. Cross-attention conditions on text embeddings (from CLIP). Open-source with various fine-tuned models. Enables high-resolution generation (512×512+) on consumer hardware. Supports text-to-image, image-to-image, inpainting.",
      "simpleExplanation": "Diffusion in a compressed space instead of pixel space—much faster! Use a text description to guide what image is generated. Open source and runs on regular GPUs. Powers many image generation tools.",
      "example": {
        "description": "",
        "code": "Stable Diffusion pipeline:\n\nText-to-Image:\n\"A cat wearing a hat, digital art\"\n       ↓\nCLIP text encoder → text embedding\n       ↓\nStart with latent noise (64×64×4)\n       ↓\nIterative denoising with text conditioning\n(U-Net with cross-attention to text)\n       ↓\nDenoised latent (64×64×4)\n       ↓\nVAE decoder\n       ↓\nFinal image (512×512×3)\n\n50 denoising steps in latent space\n= 512×512 image in seconds on GPU",
        "codeLanguage": "python"
      },
      "tags": [
        "generative",
        "generation",
        "stable",
        "diffusion"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "language-model",
      "name": "Language Model",
      "parentId": "language-models",
      "sectionId": "13.5.1",
      "level": 2,
      "fullExplanation": "Language models estimate probability distributions over sequences of tokens: P(w₁, w₂, ..., wₙ) = ∏P(wᵢ|w₁,...,wᵢ₋₁). Autoregressive models predict next token given previous tokens. Used for text generation, scoring fluency, feature extraction. Traditional: n-gram models. Neural: RNN LMs, Transformer LMs. Large-scale transformers (GPT) achieve remarkable generalization across tasks.",
      "simpleExplanation": "Predict the next word given previous words. \"The cat sat on the ___\" → \"mat\" (high probability). Trained on massive text, learns grammar, facts, and reasoning patterns. Foundation of ChatGPT and similar AI.",
      "example": {
        "description": "",
        "code": "Language model prediction:\n\nInput: \"The quick brown fox\"\nModel outputs probability for each word in vocabulary:\n\nP(\"jumps\") = 0.15  ← Most likely\nP(\"runs\") = 0.08\nP(\"is\") = 0.05\nP(\"the\") = 0.001\n...\n\nSample or take argmax:\n\"The quick brown fox jumps\"\n\nContinue:\n\"The quick brown fox jumps over\" → \"the\"\n\"The quick brown fox jumps over the\" → \"lazy\"\n...",
        "codeLanguage": "python"
      },
      "tags": [
        "generative",
        "generation",
        "language",
        "model"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "perplexity",
      "name": "Perplexity",
      "parentId": "language-models",
      "sectionId": "13.5.2",
      "level": 2,
      "fullExplanation": "Perplexity measures how well a language model predicts a held-out test set. PPL = exp(-(1/N)Σ log P(wᵢ|context)). Interpretation: effective number of equally likely words the model is uncertain between. Lower is better—perplexity of 10 means ~10 equally likely choices per word on average. Used to compare language models. Related to cross-entropy: PPL = 2^H.",
      "simpleExplanation": "How surprised is the model by the text? Lower perplexity = less surprised = better predictions. If perplexity is 100, it's like the model is confused between 100 equally likely words. Good models have low perplexity.",
      "example": {
        "description": "",
        "code": "Test sentence: \"The cat sat on the mat\"\n\nGood model:\nP(cat|The) = 0.1\nP(sat|The cat) = 0.2\nP(on|The cat sat) = 0.3\nP(the|...) = 0.4\nP(mat|...) = 0.2\n\nPPL = exp(-1/6 × (log(0.1)+log(0.2)+...)) ≈ 15\n\nBad model:\nEach probability ≈ 0.0001\nPPL ≈ 10,000\n\nInterpretation: Good model like choosing from 15 words\n              Bad model like choosing from 10,000 words",
        "codeLanguage": "python"
      },
      "tags": [
        "generative",
        "generation",
        "perplexity"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "llm",
      "name": "Large Language Model (LLM)",
      "parentId": "large-language-models",
      "sectionId": "13.6.1",
      "level": 2,
      "fullExplanation": "LLMs are language models with billions of parameters trained on massive text corpora. Scale enables emergent capabilities: in-context learning, reasoning, code generation, instruction following. Key innovations: transformer architecture, scaled training (compute optimal laws), RLHF for alignment. Examples: GPT-4, Claude, LLaMA, PaLM. Foundation models adaptable to many tasks via prompting or fine-tuning.",
      "simpleExplanation": "Very large neural networks trained on internet-scale text. At sufficient size, they learn to follow instructions, reason about problems, and perform tasks they weren't explicitly trained for. ChatGPT, Claude, and similar AI assistants are LLMs.",
      "example": {
        "description": "",
        "code": "LLM scale evolution:\n\nGPT-2 (2019): 1.5B parameters\nGPT-3 (2020): 175B parameters\nGPT-4 (2023): ~1T parameters (estimated)\n\nEmergent capabilities at scale:\n- Few-shot learning\n- Chain-of-thought reasoning\n- Code generation\n- Following complex instructions\n- Multilingual transfer\n\nTraining:\n- Trillions of tokens from internet\n- Months of training on thousands of GPUs\n- RLHF for instruction-following",
        "codeLanguage": "python"
      },
      "tags": [
        "generative",
        "generation",
        "large",
        "language",
        "model"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "in-context-learning",
      "name": "In-Context Learning",
      "parentId": "large-language-models",
      "sectionId": "13.6.2",
      "level": 2,
      "fullExplanation": "In-context learning enables LLMs to perform tasks based on examples in the prompt, without parameter updates. Zero-shot: task description only. Few-shot: include examples of input-output pairs. The model learns the pattern from examples and applies to new inputs. Emergent capability at scale—small models cannot do this. Limited by context window and example quality.",
      "simpleExplanation": "Give the model examples in the prompt, and it figures out the pattern. No training needed—just show it what you want. \"Q: 2+2? A: 4. Q: 3+5? A: 8. Q: 7+3? A:\" → Model outputs \"10\".",
      "example": {
        "description": "",
        "code": "Few-shot translation:\n\nPrompt:\nEnglish: Hello\nFrench: Bonjour\nEnglish: Goodbye\nFrench: Au revoir\nEnglish: Thank you\nFrench: Merci\nEnglish: How are you?\nFrench:\n\nModel output: Comment allez-vous?\n\nZero-shot:\n\"Translate to French: How are you?\"\n→ Comment allez-vous?\n\nThe model learned translation from examples!",
        "codeLanguage": "python"
      },
      "tags": [
        "generative",
        "generation",
        "context",
        "learning"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "rlhf",
      "name": "RLHF (Reinforcement Learning from Human Feedback)",
      "parentId": "large-language-models",
      "sectionId": "13.6.3",
      "level": 2,
      "fullExplanation": "RLHF aligns LLMs with human preferences through three stages: (1) Supervised fine-tuning on high-quality examples, (2) Train reward model on human preference comparisons (which response is better?), (3) Use RL (PPO) to optimize policy against reward model. Makes models more helpful, harmless, and honest. Key for chat assistants. Challenge: reward hacking, distribution shift.",
      "simpleExplanation": "Train the model to match human preferences. Humans rank different outputs. Train a \"reward model\" to predict rankings. Use RL to make the language model produce higher-reward outputs. How ChatGPT became helpful and conversational.",
      "example": {
        "description": "",
        "code": "RLHF pipeline:\n\nStage 1: Supervised Fine-Tuning\nHuman writes: \"What is 2+2?\" → \"2+2 equals 4\"\nModel learns to follow instruction format\n\nStage 2: Reward Model\nPrompt: \"Tell me a joke\"\nResponse A: \"Why did the chicken cross the road?\"\nResponse B: \"I don't tell jokes.\"\nHuman preference: A > B\nTrain reward model to predict: R(A) > R(B)\n\nStage 3: RL Fine-Tuning\nModel generates response\nReward model scores it\nPPO updates model to increase reward\nRepeat many times\n\nResult: Model produces responses humans prefer!",
        "codeLanguage": "python"
      },
      "tags": [
        "generative",
        "generation",
        "rlhf",
        "(reinforcement",
        "learning"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "prompt-engineering",
      "name": "Prompt Engineering",
      "parentId": "large-language-models",
      "sectionId": "13.6.4",
      "level": 2,
      "fullExplanation": "Prompt engineering crafts inputs to elicit desired LLM outputs. Techniques: clear instructions, role specification (\"You are an expert...\"), few-shot examples, chain-of-thought (\"Let's think step by step\"), structured output formats, constraints. Quality of output heavily depends on prompt. An art and science—systematic exploration often needed. Increasingly important skill as LLMs proliferate.",
      "simpleExplanation": "Craft the right question to get the right answer. Small changes in phrasing dramatically affect outputs. Adding \"Let's think step by step\" improves reasoning. Examples in the prompt guide format and style.",
      "example": {
        "description": "python\n[code here]\n```\"\n\nChain-of-thought prompt:\n\"Solve this math problem step by step, showing your work:\nIf a train travels at 60mph for 2.5 hours, how far does\nit go?\"\n\n→ Model: \"Let's solve this step by step:\n1. Distance = Speed × Time\n2. Speed = 60 mph\n3. Time = 2.5 hours\n4. Distance = 60 × 2.5 = 150 miles\"\n```",
        "code": "Bad prompt:\n\"Fix this code\"\n→ Vague, unclear what's wrong\n\nGood prompt:\n\"You are an expert Python developer. The following code\nthrows an IndexError on line 5. Identify the bug and\nprovide a corrected version with explanation.",
        "codeLanguage": "python"
      },
      "tags": [
        "generative",
        "generation",
        "prompt",
        "engineering"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "llm-finetuning",
      "name": "Fine-Tuning LLMs",
      "parentId": "large-language-models",
      "sectionId": "13.6.5",
      "level": 2,
      "fullExplanation": "Fine-tuning adapts pretrained LLMs to specific tasks or domains. Full fine-tuning updates all parameters—expensive but most flexible. Parameter-efficient methods (LoRA, adapters, prefix tuning) update small fractions of parameters—cheaper, avoids catastrophic forgetting. Instruction tuning teaches following diverse instructions. Domain adaptation improves performance on specialized corpora (medical, legal).",
      "simpleExplanation": "Adapt the general LLM to your specific needs. Full fine-tuning: expensive but thorough. LoRA: add small trainable adapters—cheap and effective. Like teaching a generalist to specialize in your domain.",
      "example": {
        "description": "",
        "code": "Fine-tuning approaches:\n\nFull Fine-Tuning:\n- Update all 7B parameters\n- Needs 8× A100 GPUs\n- Risk of forgetting general knowledge\n\nLoRA (Low-Rank Adaptation):\n- Freeze base model\n- Add small rank-16 matrices: +0.1% parameters\n- Train only adapters\n- Works on single GPU!\n\nAdapter architecture:\nOriginal layer: h = Wx\nWith LoRA: h = Wx + BAx\nwhere B (d×r), A (r×d), r << d\n\nExample: 7B model\nFull: 7B trainable parameters\nLoRA (r=16): ~4M trainable parameters (0.06%)",
        "codeLanguage": "python"
      },
      "tags": [
        "generative",
        "generation",
        "fine",
        "tuning",
        "llms"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "bagging-methods",
      "name": "Bagging Methods",
      "parentId": "ensemble-methods",
      "sectionId": "9.2",
      "level": 1,
      "fullExplanation": "Bagging Methods covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about bagging methods and related techniques.",
      "example": {
        "description": "This section contains concepts related to bagging methods."
      },
      "tags": [
        "ensemble",
        "boosting",
        "bagging",
        "methods"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "boosting-methods",
      "name": "Boosting Methods",
      "parentId": "ensemble-methods",
      "sectionId": "9.3",
      "level": 1,
      "fullExplanation": "Boosting Methods covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about boosting methods and related techniques.",
      "example": {
        "description": "This section contains concepts related to boosting methods."
      },
      "tags": [
        "ensemble",
        "boosting",
        "bagging",
        "methods"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "stacking-blending",
      "name": "Stacking & Blending",
      "parentId": "ensemble-methods",
      "sectionId": "9.4",
      "level": 1,
      "fullExplanation": "Stacking & Blending covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about stacking & blending and related techniques.",
      "example": {
        "description": "This section contains concepts related to stacking & blending."
      },
      "tags": [
        "ensemble",
        "boosting",
        "bagging",
        "stacking",
        "blending"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "ensemble-learning",
      "name": "Ensemble Learning",
      "parentId": "core-concepts",
      "sectionId": "9.1.1",
      "level": 2,
      "fullExplanation": "Ensemble learning combines multiple models (base learners) to produce a better predictor than any single model. Key insight: diverse models make different errors, and combining them cancels out individual mistakes. Ensembles reduce variance (bagging), bias (boosting), or both. Success requires base models that are accurate (better than random) and diverse (make different errors).",
      "simpleExplanation": "Combine many models to make a better one. Like asking 100 experts instead of just one—they'll make different mistakes, but the majority is usually right. The \"wisdom of crowds\" for machine learning.",
      "example": {
        "description": "Weather prediction:\n- Model 1 (temperature-based): \"Rain\" (70% confidence)\n- Model 2 (humidity-based): \"Rain\" (65% confidence)\n- Model 3 (pressure-based): \"No rain\" (55% confidence)\n\nEnsemble vote: Rain (2-1)\nEnsemble probability: avg([0.7, 0.65, 0.45]) = 0.6 = \"Rain\"\n\nIndividual accuracy: ~75%\nEnsemble accuracy: ~85% (errors cancel out)"
      },
      "tags": [
        "ensemble",
        "boosting",
        "bagging",
        "learning"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "wisdom-of-crowds",
      "name": "Wisdom of Crowds",
      "parentId": "core-concepts",
      "sectionId": "9.1.2",
      "level": 2,
      "fullExplanation": "The wisdom of crowds principle states that aggregate judgments from diverse, independent individuals often outperform expert opinions. In ML: combining diverse models reduces variance without increasing bias. Mathematical basis: if individual errors are uncorrelated, averaging N models reduces variance by factor of 1/N. Key requirements: diversity (different approaches) and independence (not correlated errors).",
      "simpleExplanation": "Many guesses averaged together beat one expert guess. If 100 people guess how many jellybeans are in a jar, the average is usually very close to the truth, even though individuals are way off. Same principle for ML models.",
      "example": {
        "description": "Estimating a value (true = 100):\n- Expert 1: 120 (error: +20)\n- Expert 2: 85 (error: -15)\n- Expert 3: 105 (error: +5)\n- Expert 4: 90 (error: -10)\n\nAverage: (120+85+105+90)/4 = 100 ← Perfect!\n\nIndividual errors canceled out. This is why ensembles work."
      },
      "tags": [
        "ensemble",
        "boosting",
        "bagging",
        "wisdom",
        "crowds"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "base-learner",
      "name": "Base Learner",
      "parentId": "core-concepts",
      "sectionId": "9.1.3",
      "level": 2,
      "fullExplanation": "A base learner (weak learner) is an individual model in an ensemble. It should be better than random but doesn't need to be highly accurate. Common choices: decision trees (especially stumps for boosting), linear models, neural networks. Trade-off: complex base learners = fewer needed but risk overfitting; simple base learners = need more models but better regularization.",
      "simpleExplanation": "The individual models that get combined. They don't have to be great on their own—just slightly better than guessing. Decision trees are popular base learners because they're fast and diverse (small changes in data create different trees).",
      "example": {
        "description": "Boosting typically uses \"weak learners\":\n- Decision stump (depth=1): ~55% accuracy\n- Single rule: \"If age > 30, predict yes\" ~52% accuracy\n\nThese weak learners are combined:\n- 100 decision stumps combined: ~95% accuracy\n\nEach weak learner adds a little information. Together = strong model."
      },
      "tags": [
        "ensemble",
        "boosting",
        "bagging",
        "base",
        "learner"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "model-diversity",
      "name": "Model Diversity",
      "parentId": "core-concepts",
      "sectionId": "9.1.4",
      "level": 2,
      "fullExplanation": "Diversity ensures ensemble members make different errors. Sources of diversity: (1) Different training data (bagging samples with replacement), (2) Different features (random subspace), (3) Different algorithms (heterogeneous ensemble), (4) Different hyperparameters, (5) Different random initializations. Diversity is as important as individual accuracy—100 identical models = no improvement.",
      "simpleExplanation": "Ensemble members should disagree on different examples. If all models make the same mistakes, combining them doesn't help. Create diversity through different data, different features, or different algorithms.",
      "example": {
        "description": "Two scenarios with 3 models:\n\nLow diversity (all similar):\n- Model 1 wrong on examples: {1, 2, 3}\n- Model 2 wrong on examples: {1, 2, 4}\n- Model 3 wrong on examples: {1, 2, 5}\n- Ensemble wrong on: {1, 2} (still wrong where they agree)\n\nHigh diversity:\n- Model 1 wrong on examples: {1, 5, 9}\n- Model 2 wrong on examples: {2, 6, 10}\n- Model 3 wrong on examples: {3, 7, 11}\n- Ensemble wrong on: {} (no overlap in errors!)"
      },
      "tags": [
        "ensemble",
        "boosting",
        "bagging",
        "model",
        "diversity"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "bagging",
      "name": "Bagging",
      "parentId": "bagging-methods",
      "sectionId": "9.2.1",
      "level": 2,
      "fullExplanation": "Bagging (Bootstrap Aggregating) trains each base model on a bootstrap sample (random sample with replacement) of the training data. For classification, final prediction uses majority voting; for regression, averaging. Bagging reduces variance without substantially increasing bias. Most effective with unstable models (high variance) like decision trees. Each bootstrap sample contains ~63.2% unique examples (rest are duplicates).",
      "simpleExplanation": "Create different training sets by randomly sampling with replacement. Train a model on each sample, then average their predictions. Each model sees a slightly different version of the data, creating diversity.",
      "example": {
        "description": "Original data: [A, B, C, D, E] (5 examples)\n\nBootstrap sample 1: [A, A, C, D, E] (A appears twice, B missing)\nBootstrap sample 2: [B, B, C, D, E] (B appears twice, A missing)\nBootstrap sample 3: [A, C, C, E, E] (C, E duplicated)\n\nTrain 3 models on these different samples.\nEach model learns slightly different patterns.\nAverage predictions → reduced variance."
      },
      "tags": [
        "ensemble",
        "boosting",
        "bagging"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "bootstrap-sampling",
      "name": "Bootstrap Sampling",
      "parentId": "bagging-methods",
      "sectionId": "9.2.2",
      "level": 2,
      "fullExplanation": "Bootstrap sampling creates a new dataset of size N by randomly drawing N samples with replacement from the original data. Each sample has equal probability 1/N of being selected at each draw. On average, each bootstrap sample contains 63.2% unique examples (1 - 1/e). The ~36.8% of examples not selected form the \"out-of-bag\" (OOB) set, useful for validation.",
      "simpleExplanation": "Pick random examples, allowing repeats. With N examples, draw N times—some examples appear multiple times, some don't appear at all. This randomness creates different training sets for each model.",
      "example": {
        "description": "Original: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\nBootstrap sample (draw 10 with replacement):\n[3, 7, 7, 1, 5, 3, 9, 10, 2, 7]\n\nUnique examples included: {1, 2, 3, 5, 7, 9, 10} = 7 out of 10 (70%)\nOut-of-bag (OOB): {4, 6, 8}\n\nOn average, ~63% unique, ~37% OOB."
      },
      "tags": [
        "ensemble",
        "boosting",
        "bagging",
        "bootstrap",
        "sampling"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "oob-error",
      "name": "Out-of-Bag Error",
      "parentId": "bagging-methods",
      "sectionId": "9.2.3",
      "level": 2,
      "fullExplanation": "Out-of-bag (OOB) error provides validation without a separate holdout set. For each training example, only ~1/3 of bagged models didn't see it during training. The OOB prediction for that example uses only these models. OOB error approximates test error closely, eliminating the need for cross-validation in bagging methods. Free validation during training.",
      "simpleExplanation": "Each example is \"out of bag\" for some models (wasn't in their training sample). Use those models to predict for that example—it's like a test set they never saw. Do this for all examples to estimate generalization error.",
      "example": {
        "description": "Example X appears in training for models [1, 3, 5] but not [2, 4, 6].\nFor OOB error, predict X using only models [2, 4, 6].\n\nOOB predictions:\n- Model 2: Cat\n- Model 4: Cat\n- Model 6: Dog\n\nOOB prediction for X: Cat (majority vote)\nIf X is actually Cat → correct\nIf X is actually Dog → counts as OOB error"
      },
      "tags": [
        "ensemble",
        "boosting",
        "bagging",
        "error"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "random-forest",
      "name": "Random Forest",
      "parentId": "bagging-methods",
      "sectionId": "9.2.4",
      "level": 2,
      "fullExplanation": "Random Forest extends bagging by adding random feature selection at each split. Each tree uses a bootstrap sample AND only considers a random subset of features (typically √p for classification, p/3 for regression) when splitting. This additional randomness creates even more diverse trees. Random Forest is robust, accurate, handles high dimensions, provides feature importance, and rarely overfits. Default ensemble choice for tabular data.",
      "simpleExplanation": "Bagging + random feature selection. Each tree sees different data AND can only use a random subset of features at each split. Double randomness = more diversity = better ensemble. Often the first algorithm to try for structured data.",
      "example": {
        "description": "Building a random forest with 100 trees:\n\nTree 1:\n- Bootstrap sample 1\n- At root split: can only consider features [1, 4, 7, 9] (random 4 of 20)\n- Picks feature 4\n\nTree 2:\n- Bootstrap sample 2\n- At root split: can only consider features [2, 5, 8, 11] (different random 4)\n- Picks feature 8\n\nDifferent data + different feature subsets = very different trees.",
        "code": "from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100, max_features='sqrt')\nrf.fit(X_train, y_train)",
        "codeLanguage": "python"
      },
      "tags": [
        "ensemble",
        "boosting",
        "bagging",
        "random",
        "forest"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "random-subspace",
      "name": "Random Subspace Method",
      "parentId": "bagging-methods",
      "sectionId": "9.2.5",
      "level": 2,
      "fullExplanation": "Random subspace method trains each model on a random subset of features rather than random subset of samples. Each model sees all training examples but only some features. Creates diversity through feature space partitioning. Especially useful when features are plentiful but samples are limited. Can be combined with bagging for additional diversity (as in Random Forest).",
      "simpleExplanation": "Each model only sees some features. Instead of hiding examples, hide columns. Model 1 might see [age, income], Model 2 sees [education, zipcode]. Each model specializes in different aspects.",
      "example": {
        "description": "Dataset with 10 features: [f1, f2, f3, f4, f5, f6, f7, f8, f9, f10]\n\nModel 1 uses: [f1, f3, f5, f7]\nModel 2 uses: [f2, f4, f6, f8]\nModel 3 uses: [f1, f2, f9, f10]\n\nEach model learns patterns in different feature subspaces.\nCombine for robust predictions across all features."
      },
      "tags": [
        "ensemble",
        "boosting",
        "bagging",
        "random",
        "subspace",
        "method"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "extra-trees",
      "name": "Extra Trees",
      "parentId": "bagging-methods",
      "sectionId": "9.2.6",
      "level": 2,
      "fullExplanation": "Extra Trees (Extremely Randomized Trees) adds more randomness than Random Forest: splits are chosen randomly rather than optimized. For each considered feature, a random split point is selected instead of the best one. This extreme randomness increases diversity and speed (no need to find optimal splits) at the cost of slightly higher bias. Often matches or beats Random Forest performance.",
      "simpleExplanation": "Random Forest picks the best split from random features. Extra Trees picks a random split point too—even more randomness. Faster training, more diverse trees, surprisingly effective. Trade precision for speed and diversity.",
      "example": {
        "description": "At a split, considering feature \"age\" (range 20-60):\n\nRandom Forest:\n- Try all possible splits: 21, 22, 23, ..., 59\n- Find best: split at 35 (lowest Gini)\n- Use split at 35\n\nExtra Trees:\n- Pick random threshold: 42 (just random between 20-60)\n- Use split at 42 (no optimization)\n\nFaster + more diverse. The \"suboptimal\" splits average out in the ensemble.",
        "code": "from sklearn.ensemble import ExtraTreesClassifier\net = ExtraTreesClassifier(n_estimators=100)\net.fit(X_train, y_train)",
        "codeLanguage": "python"
      },
      "tags": [
        "ensemble",
        "boosting",
        "bagging",
        "extra",
        "trees"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "boosting",
      "name": "Boosting",
      "parentId": "boosting-methods",
      "sectionId": "9.3.1",
      "level": 2,
      "fullExplanation": "Boosting builds models sequentially, each focusing on examples the previous models got wrong. Early models capture easy patterns; later models handle difficult cases. Predictions are weighted combinations of all models. Boosting reduces bias (and variance) by progressively correcting errors. More powerful than bagging but more prone to overfitting. Key algorithms: AdaBoost, Gradient Boosting, XGBoost.",
      "simpleExplanation": "Train models one at a time, each fixing the previous one's mistakes. First model does its best. Second model focuses on what the first got wrong. Third model focuses on remaining errors. Keep going until errors are minimized.",
      "example": {
        "description": "Sequential error correction:\n\nModel 1: Gets 100 examples right, 20 wrong\nModel 2: Focuses on those 20, gets 15 right, 5 wrong\nModel 3: Focuses on those 5, gets 4 right, 1 wrong\n\nCombined: 100 + 15 + 4 = 119 right, 1 wrong\nMuch better than any single model!"
      },
      "tags": [
        "ensemble",
        "boosting",
        "bagging"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "adaboost",
      "name": "AdaBoost",
      "parentId": "boosting-methods",
      "sectionId": "9.3.2",
      "level": 2,
      "fullExplanation": "AdaBoost (Adaptive Boosting) adjusts example weights based on classification accuracy. Initially, all examples have equal weight. After each weak learner, weights of misclassified examples increase; correctly classified examples decrease. Next learner focuses on hard examples. Final prediction: weighted vote where better-performing models have higher influence. Sensitive to noisy data and outliers.",
      "simpleExplanation": "Give each example a weight. Misclassified examples get heavier (more important). Next model pays more attention to heavy examples. After many rounds, hard examples have been repeatedly targeted. Models that perform better get more voting power.",
      "example": {
        "description": "Round 1: All 100 examples weight = 1.0\n- Model 1 trained, 80% accuracy\n- 20 wrong examples: weight increases to 1.5\n- 80 right examples: weight decreases to 0.9\n\nRound 2: Focus on high-weight examples\n- Model 2 trained on weighted data\n- Gets 15 of those 20 hard examples right\n- Update weights again...\n\nFinal: Combine Model 1 (weight 0.6) + Model 2 (weight 0.4) + ...",
        "code": "from sklearn.ensemble import AdaBoostClassifier\nada = AdaBoostClassifier(n_estimators=50, learning_rate=1.0)\nada.fit(X_train, y_train)",
        "codeLanguage": "python"
      },
      "tags": [
        "ensemble",
        "boosting",
        "bagging",
        "adaboost"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "gradient-boosting",
      "name": "Gradient Boosting",
      "parentId": "boosting-methods",
      "sectionId": "9.3.3",
      "level": 2,
      "fullExplanation": "Gradient Boosting fits new models to the residual errors (gradients) of the current ensemble. Each iteration: compute pseudo-residuals (negative gradients of loss), fit a model to predict residuals, add to ensemble with learning rate shrinkage. Works with any differentiable loss function. More flexible than AdaBoost. Learning rate controls each tree's contribution—smaller rates need more trees but often generalize better.",
      "simpleExplanation": "Each new model predicts the remaining error. Model 1 predicts the target. Model 2 predicts what Model 1 got wrong (the residual). Model 3 predicts what's still wrong. Add all predictions together for the final answer.",
      "example": {
        "description": "Predicting house price = $300K:\n\nModel 1 predicts: $280K (residual: +$20K)\nModel 2 predicts residual: +$15K (remaining: +$5K)\nModel 3 predicts remaining: +$4K (remaining: +$1K)\n\nFinal prediction: $280K + $15K + $4K = $299K (close!)\n\nEach model fixes the previous error.",
        "code": "from sklearn.ensemble import GradientBoostingRegressor\ngb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\ngb.fit(X_train, y_train)",
        "codeLanguage": "python"
      },
      "tags": [
        "ensemble",
        "boosting",
        "bagging",
        "gradient"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "xgboost",
      "name": "XGBoost",
      "parentId": "boosting-methods",
      "sectionId": "9.3.4",
      "level": 2,
      "fullExplanation": "XGBoost (eXtreme Gradient Boosting) is an optimized gradient boosting implementation with regularization, parallel processing, and advanced features. Includes L1/L2 regularization on weights, tree pruning, handling of missing values, and column subsampling. Uses second-order gradient information for better splits. Highly efficient, scalable, and consistently wins ML competitions. Industry standard for tabular data.",
      "simpleExplanation": "Gradient boosting on steroids. Faster, handles missing data, prevents overfitting with regularization, works at scale. The go-to algorithm for structured data. Won countless Kaggle competitions.",
      "example": {
        "description": "XGBoost advantages:\n- Speed: Parallel tree building, cache optimization\n- Regularization: Built-in L1/L2 to prevent overfitting\n- Missing values: Learns optimal direction for missing data\n- Flexibility: Custom objectives and evaluation metrics",
        "code": "import xgboost as xgb\nmodel = xgb.XGBClassifier(\n    n_estimators=100,\n    learning_rate=0.1,\n    max_depth=6,\n    reg_lambda=1.0,  # L2 regularization\n    reg_alpha=0.0    # L1 regularization\n)\nmodel.fit(X_train, y_train)",
        "codeLanguage": "python"
      },
      "tags": [
        "ensemble",
        "boosting",
        "bagging",
        "xgboost"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "lightgbm",
      "name": "LightGBM",
      "parentId": "boosting-methods",
      "sectionId": "9.3.5",
      "level": 2,
      "fullExplanation": "LightGBM uses histogram-based learning and leaf-wise tree growth for efficiency. Instead of scanning all data for splits, it bins continuous features into discrete histograms. Leaf-wise growth (best-first) creates more accurate trees with fewer iterations than level-wise growth. GOSS (Gradient-based One-Side Sampling) and EFB (Exclusive Feature Bundling) further improve speed. Excellent for large datasets.",
      "simpleExplanation": "Even faster gradient boosting for big data. Groups feature values into bins (histograms) instead of checking every value. Grows trees leaf-by-leaf (picking best leaf to split) rather than level-by-level. Often faster than XGBoost with similar accuracy.",
      "example": {
        "description": "LightGBM speed tricks:\n- Histogram binning: 256 bins instead of millions of values\n- Leaf-wise growth: Splits the most promising leaf, not all leaves\n- GOSS: Keeps examples with large gradients, samples rest\n- EFB: Bundles mutually exclusive features together",
        "code": "import lightgbm as lgb\nmodel = lgb.LGBMClassifier(\n    n_estimators=100,\n    learning_rate=0.1,\n    num_leaves=31,\n    boosting_type='gbdt'\n)\nmodel.fit(X_train, y_train)",
        "codeLanguage": "python"
      },
      "tags": [
        "ensemble",
        "boosting",
        "bagging",
        "lightgbm"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "catboost",
      "name": "CatBoost",
      "parentId": "boosting-methods",
      "sectionId": "9.3.6",
      "level": 2,
      "fullExplanation": "CatBoost specializes in categorical feature handling without manual encoding. Uses ordered target encoding to avoid target leakage: for each example, encodes categories using only examples that appear before it in a random permutation. Implements ordered boosting to reduce overfitting. Symmetric trees (same feature/threshold at each level) enable fast inference. Excellent for datasets with many categorical features.",
      "simpleExplanation": "Gradient boosting that handles categories natively. No need to one-hot encode or manually encode categorical features—CatBoost does it smartly while avoiding data leakage. Great when you have many categorical columns.",
      "example": {
        "description": "Dataset with categories: [City, Color, Brand]\n\nTraditional approach:\n1. Encode City (100 categories → 100 columns)\n2. Encode Color (10 categories → 10 columns)\n3. Encode Brand (500 categories → 500 columns)\n4. Train XGBoost\n\nCatBoost approach:\n1. Pass categorical columns directly\n2. CatBoost handles encoding internally\n3. No feature explosion, no leakage",
        "code": "from catboost import CatBoostClassifier\nmodel = CatBoostClassifier(\n    iterations=100,\n    learning_rate=0.1,\n    cat_features=['City', 'Color', 'Brand']\n)\nmodel.fit(X_train, y_train)",
        "codeLanguage": "python"
      },
      "tags": [
        "ensemble",
        "boosting",
        "bagging",
        "catboost"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "boosting-learning-rate",
      "name": "Learning Rate in Boosting",
      "parentId": "boosting-methods",
      "sectionId": "9.3.7",
      "level": 2,
      "fullExplanation": "Learning rate (shrinkage) scales each tree's contribution to the ensemble. Prediction: F_m(x) = F_{m-1}(x) + η × h_m(x), where η is the learning rate. Smaller η means each tree contributes less, requiring more trees but often improving generalization. Trade-off: smaller learning rate + more trees = slower training but often better accuracy. Typical values: 0.01-0.3.",
      "simpleExplanation": "How much each new tree contributes. Learning rate 1.0 = full contribution. Learning rate 0.1 = only 10% contribution. Smaller rates are more conservative—need more trees but often generalize better. Like taking small steps instead of big jumps.",
      "example": {
        "description": "Predicting with learning rate η=0.1:\n\nIteration 1: Tree predicts +20, add 0.1×20 = +2\nIteration 2: Tree predicts +15, add 0.1×15 = +1.5\nIteration 3: Tree predicts +10, add 0.1×10 = +1\n\nTotal after 3 trees: 2 + 1.5 + 1 = 4.5\n\nvs. Learning rate η=1.0:\nTotal after 3 trees: 20 + 15 + 10 = 45 (overshoots!)\n\nLower learning rate = slower convergence but more stable."
      },
      "tags": [
        "ensemble",
        "boosting",
        "bagging",
        "learning",
        "rate"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "stacking",
      "name": "Stacking",
      "parentId": "stacking-blending",
      "sectionId": "9.4.1",
      "level": 2,
      "fullExplanation": "Stacking (Stacked Generalization) uses a meta-learner to combine base model predictions. Level 0: Train diverse base models. Level 1: Use base model predictions as features to train a meta-learner. The meta-learner learns optimal weighting and interactions between base predictions. Cross-validation is essential to avoid overfitting: base predictions on the training set must be out-of-fold predictions.",
      "simpleExplanation": "Train a model to combine other models. Instead of averaging, learn the best way to combine. Maybe Random Forest is good for some examples, while SVM is better for others. The meta-learner figures out when to trust each base model.",
      "example": {
        "description": "Base models (Level 0):\n- Random Forest\n- XGBoost\n- Neural Network\n- Logistic Regression\n\nFor each training example, get out-of-fold predictions:\n[RF_pred, XGB_pred, NN_pred, LR_pred] → new features\n\nMeta-learner (Level 1):\nTrain a Logistic Regression on [RF_pred, XGB_pred, NN_pred, LR_pred]\nMeta-learner learns: \"Trust XGBoost when RF and NN disagree...\"",
        "code": "from sklearn.ensemble import StackingClassifier\nestimators = [\n    ('rf', RandomForestClassifier()),\n    ('xgb', XGBClassifier()),\n    ('svc', SVC(probability=True))\n]\nstack = StackingClassifier(\n    estimators=estimators,\n    final_estimator=LogisticRegression()\n)\nstack.fit(X_train, y_train)",
        "codeLanguage": "python"
      },
      "tags": [
        "ensemble",
        "boosting",
        "bagging",
        "stacking"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "blending",
      "name": "Blending",
      "parentId": "stacking-blending",
      "sectionId": "9.4.2",
      "level": 2,
      "fullExplanation": "Blending is a simpler variant of stacking that uses a holdout set instead of cross-validation. Split training data into two parts: train base models on part 1, generate predictions on part 2 (blend set), train meta-learner on blend set predictions. Faster than stacking (no cross-validation) but uses less data for base model training. Risk: base models never see blend set during training.",
      "simpleExplanation": "Like stacking but simpler. Hold out some training data. Train base models on the rest. Make predictions on the holdout. Train a combiner on those predictions. Faster than full cross-validation stacking.",
      "example": {
        "description": "Training data: 10,000 examples\n\nSplit:\n- Train set (8,000): Train base models\n- Blend set (2,000): Generate base model predictions\n\nBase model predictions on blend set:\n[RF: 0.8, XGB: 0.7, SVC: 0.9] for each of 2,000 examples\n\nMeta-learner:\nTrain on 2,000 examples with 3 features (base predictions)\nLearns: Final = 0.3×RF + 0.5×XGB + 0.2×SVC"
      },
      "tags": [
        "ensemble",
        "boosting",
        "bagging",
        "blending"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "voting-ensemble",
      "name": "Voting Ensemble",
      "parentId": "stacking-blending",
      "sectionId": "9.4.3",
      "level": 2,
      "fullExplanation": "Voting ensemble combines predictions through voting (classification) or averaging (regression). Hard voting: each model gets one vote, majority wins. Soft voting: average predicted probabilities, highest average probability wins. Soft voting often performs better as it leverages confidence information. Simple to implement, no training of meta-learner required. Works best with diverse, accurate base models.",
      "simpleExplanation": "Democracy for models. Hard voting: each model votes for a class, majority wins. Soft voting: average the probability predictions. Simple, no training needed, often surprisingly effective.",
      "example": {
        "description": "Three models classify an image:\n\nHard voting:\n- Random Forest: Cat\n- SVM: Dog\n- Neural Net: Cat\n- Result: Cat (2-1)\n\nSoft voting:\n- Random Forest: [Cat: 0.8, Dog: 0.2]\n- SVM: [Cat: 0.3, Dog: 0.7]\n- Neural Net: [Cat: 0.9, Dog: 0.1]\n- Average: [Cat: 0.67, Dog: 0.33]\n- Result: Cat (higher probability)",
        "code": "from sklearn.ensemble import VotingClassifier\nvoting = VotingClassifier(\n    estimators=[('rf', rf), ('svm', svm), ('nn', nn)],\n    voting='soft'  # or 'hard'\n)\nvoting.fit(X_train, y_train)",
        "codeLanguage": "python"
      },
      "tags": [
        "ensemble",
        "boosting",
        "bagging",
        "voting"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "weighted-averaging",
      "name": "Weighted Averaging",
      "parentId": "stacking-blending",
      "sectionId": "9.4.4",
      "level": 2,
      "fullExplanation": "Weighted averaging assigns different weights to base models based on their quality. Unlike simple averaging, better models have more influence. Weights can be determined by: validation performance, optimization (grid search), or learned (regression with non-negativity constraints). For regression: ŷ = Σwᵢŷᵢ where Σwᵢ = 1. For classification: average weighted probabilities.",
      "simpleExplanation": "Better models get more say. If Random Forest has 90% accuracy and SVM has 70%, Random Forest should count more. Assign weights proportional to performance or optimize them.",
      "example": {
        "description": "Three regression models with validation errors:\n- Model A: RMSE = 10 (best)\n- Model B: RMSE = 15\n- Model C: RMSE = 20 (worst)\n\nWeights from inverse error:\n- w_A = 1/10 = 0.100\n- w_B = 1/15 = 0.067\n- w_C = 1/20 = 0.050\n\nNormalized: w_A=0.46, w_B=0.31, w_C=0.23\n\nWeighted prediction: 0.46×pred_A + 0.31×pred_B + 0.23×pred_C\nBest model contributes most."
      },
      "tags": [
        "ensemble",
        "boosting",
        "bagging",
        "weighted",
        "averaging"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "building-blocks",
      "name": "Building Blocks",
      "parentId": "neural-networks",
      "sectionId": "10.1",
      "level": 1,
      "fullExplanation": "Building Blocks covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about building blocks and related techniques.",
      "example": {
        "description": "This section contains concepts related to building blocks."
      },
      "tags": [
        "neural-networks",
        "deep-learning",
        "building",
        "blocks"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "activation-functions",
      "name": "Activation Functions",
      "parentId": "neural-networks",
      "sectionId": "10.2",
      "level": 1,
      "fullExplanation": "Activation Functions covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about activation functions and related techniques.",
      "example": {
        "description": "This section contains concepts related to activation functions."
      },
      "tags": [
        "neural-networks",
        "deep-learning",
        "activation",
        "functions"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "backpropagation",
      "name": "Backpropagation",
      "parentId": "neural-networks",
      "sectionId": "10.3",
      "level": 1,
      "fullExplanation": "Backpropagation covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about backpropagation and related techniques.",
      "example": {
        "description": "This section contains concepts related to backpropagation."
      },
      "tags": [
        "neural-networks",
        "deep-learning",
        "backpropagation"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "weight-initialization",
      "name": "Weight Initialization",
      "parentId": "neural-networks",
      "sectionId": "10.4",
      "level": 1,
      "fullExplanation": "Weight Initialization covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about weight initialization and related techniques.",
      "example": {
        "description": "This section contains concepts related to weight initialization."
      },
      "tags": [
        "neural-networks",
        "deep-learning",
        "weight",
        "initialization"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "cnns-convolutional-neural-networks",
      "name": "CNNs (Convolutional Neural Networks)",
      "parentId": "deep-learning",
      "sectionId": "11.1",
      "level": 1,
      "fullExplanation": "CNNs (Convolutional Neural Networks) covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about cnns (convolutional neural networks) and related techniques.",
      "example": {
        "description": "This section contains concepts related to cnns (convolutional neural networks)."
      },
      "tags": [
        "deep-learning",
        "architectures",
        "cnns",
        "(convolutional",
        "neural"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "rnns-recurrent-neural-networks",
      "name": "RNNs (Recurrent Neural Networks)",
      "parentId": "deep-learning",
      "sectionId": "11.2",
      "level": 1,
      "fullExplanation": "RNNs (Recurrent Neural Networks) covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about rnns (recurrent neural networks) and related techniques.",
      "example": {
        "description": "This section contains concepts related to rnns (recurrent neural networks)."
      },
      "tags": [
        "deep-learning",
        "architectures",
        "rnns",
        "(recurrent",
        "neural"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "attention-mechanisms",
      "name": "Attention Mechanisms",
      "parentId": "deep-learning",
      "sectionId": "11.3",
      "level": 1,
      "fullExplanation": "Attention Mechanisms covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about attention mechanisms and related techniques.",
      "example": {
        "description": "This section contains concepts related to attention mechanisms."
      },
      "tags": [
        "deep-learning",
        "architectures",
        "attention",
        "mechanisms"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "transformers",
      "name": "Transformers",
      "parentId": "deep-learning",
      "sectionId": "11.4",
      "level": 1,
      "fullExplanation": "Transformers covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about transformers and related techniques.",
      "example": {
        "description": "This section contains concepts related to transformers."
      },
      "tags": [
        "deep-learning",
        "architectures",
        "transformers"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "efficient-transformers",
      "name": "Efficient Transformers",
      "parentId": "deep-learning",
      "sectionId": "11.5",
      "level": 1,
      "fullExplanation": "Efficient Transformers covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about efficient transformers and related techniques.",
      "example": {
        "description": "This section contains concepts related to efficient transformers."
      },
      "tags": [
        "deep-learning",
        "architectures",
        "efficient",
        "transformers"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "state-space-models",
      "name": "State Space Models",
      "parentId": "deep-learning",
      "sectionId": "11.6",
      "level": 1,
      "fullExplanation": "State Space Models covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about state space models and related techniques.",
      "example": {
        "description": "This section contains concepts related to state space models."
      },
      "tags": [
        "deep-learning",
        "architectures",
        "state",
        "space",
        "models"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "artificial-neuron",
      "name": "Artificial Neuron",
      "parentId": "building-blocks",
      "sectionId": "10.1.1",
      "level": 2,
      "fullExplanation": "An artificial neuron (perceptron) computes a weighted sum of inputs plus a bias, then applies an activation function: output = f(Σwᵢxᵢ + b). It loosely mimics biological neurons: inputs are dendrites, weights are synapse strengths, activation is the firing threshold, output is the axon. Multiple neurons form layers, and multiple layers form neural networks capable of learning complex functions.",
      "simpleExplanation": "A tiny decision-maker. Takes several inputs, multiplies each by a weight (importance), adds them up, then decides whether to \"fire\" based on the total. Many neurons working together can recognize patterns, classify images, and more.",
      "example": {
        "description": "Neuron deciding if someone will buy a product:\n- Input 1: Age = 25, Weight = 0.5\n- Input 2: Income = 50000, Weight = 0.0001\n- Input 3: Past purchases = 10, Weight = 0.3\n- Bias = -5\n\nWeighted sum = 0.5×25 + 0.0001×50000 + 0.3×10 + (-5)\n            = 12.5 + 5 + 3 - 5 = 15.5\n\nApply sigmoid: σ(15.5) = 0.999 → Will buy"
      },
      "tags": [
        "neural-networks",
        "deep-learning",
        "artificial",
        "neuron"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "perceptron",
      "name": "Perceptron",
      "parentId": "building-blocks",
      "sectionId": "10.1.2",
      "level": 2,
      "fullExplanation": "The perceptron is the simplest neural network: a single neuron with a step activation function. It classifies linearly separable data by finding a hyperplane. Learning rule: w ← w + η(y - ŷ)x for misclassified examples. Converges in finite steps for linearly separable data. Cannot learn XOR or other nonlinear functions—this limitation motivated multilayer networks.",
      "simpleExplanation": "The original neural network: one neuron with a yes/no output. Can only draw a straight line to separate classes. Works great for simple problems, fails completely on complex ones like XOR. The starting point that led to deep learning.",
      "example": {
        "description": "AND gate with perceptron:\n- Inputs: (0,0), (0,1), (1,0), (1,1)\n- Outputs: 0, 0, 0, 1\n\nLearned weights: w1=0.5, w2=0.5, bias=-0.7\n- (0,0): 0.5×0 + 0.5×0 - 0.7 = -0.7 < 0 → Output 0 ✓\n- (0,1): 0.5×0 + 0.5×1 - 0.7 = -0.2 < 0 → Output 0 ✓\n- (1,0): 0.5×1 + 0.5×0 - 0.7 = -0.2 < 0 → Output 0 ✓\n- (1,1): 0.5×1 + 0.5×1 - 0.7 = +0.3 > 0 → Output 1 ✓"
      },
      "tags": [
        "neural-networks",
        "deep-learning",
        "perceptron"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "mlp",
      "name": "Multilayer Perceptron (MLP)",
      "parentId": "building-blocks",
      "sectionId": "10.1.3",
      "level": 2,
      "fullExplanation": "A Multilayer Perceptron stacks multiple layers of neurons: input layer, one or more hidden layers, and output layer. Each layer is fully connected to the next. Hidden layers with nonlinear activations enable learning complex, nonlinear functions. Universal approximation theorem: MLPs with one hidden layer can approximate any continuous function given enough neurons. Trained via backpropagation.",
      "simpleExplanation": "Stack multiple layers of neurons together. Input layer receives data, hidden layers learn patterns, output layer makes predictions. Adding hidden layers allows learning complex relationships that a single neuron cannot. The foundation of deep learning.",
      "example": {
        "description": "MLP for digit recognition (0-9):\n\n\nTotal parameters: 784×256 + 256×128 + 128×10 = ~235K weights",
        "code": "Input Layer:  784 neurons (28×28 pixel image)\n       ↓\nHidden Layer 1: 256 neurons (learns edges, curves)\n       ↓\nHidden Layer 2: 128 neurons (learns parts of digits)\n       ↓\nOutput Layer: 10 neurons (one per digit 0-9)",
        "codeLanguage": "python"
      },
      "tags": [
        "neural-networks",
        "deep-learning",
        "multilayer",
        "perceptron",
        "(mlp)"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "feedforward-network",
      "name": "Feedforward Network",
      "parentId": "building-blocks",
      "sectionId": "10.1.4",
      "level": 2,
      "fullExplanation": "Feedforward networks process information in one direction: from input through hidden layers to output with no cycles. Each layer's output feeds only into subsequent layers. This acyclic structure enables efficient computation and training via backpropagation. Contrasts with recurrent networks that have feedback connections. MLPs, CNNs (for a single image), and transformers are feedforward.",
      "simpleExplanation": "Information flows forward only, never backward or in loops. Input goes in, passes through layers, output comes out. No memory of previous inputs. Simple, efficient, and the basis for most neural networks.",
      "example": {
        "description": "Feedforward pass:\n\n\nEach arrow goes forward. No connections going backward.",
        "code": "Input [x1, x2, x3]\n       ↓\nHidden Layer 1: h1 = relu(W1·x + b1)\n       ↓\nHidden Layer 2: h2 = relu(W2·h1 + b2)\n       ↓\nOutput: y = softmax(W3·h2 + b3)",
        "codeLanguage": "python"
      },
      "tags": [
        "neural-networks",
        "deep-learning",
        "feedforward",
        "network"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "dense-layer",
      "name": "Dense Layer (Fully Connected)",
      "parentId": "building-blocks",
      "sectionId": "10.1.5",
      "level": 2,
      "fullExplanation": "A dense (fully connected) layer connects every neuron in the previous layer to every neuron in the current layer. For input dimension n and output dimension m, there are n×m weights plus m biases. Dense layers can learn arbitrary relationships between inputs and outputs. High parameter count makes them prone to overfitting and computationally expensive. Often used as final layers after convolutional or recurrent layers.",
      "simpleExplanation": "Every input connects to every output. If the previous layer has 100 neurons and this layer has 50, there are 100×50 = 5000 connections. Flexible but expensive. Used when spatial structure isn't important.",
      "example": {
        "description": "Dense layer computation:\n\n\n```python\nfrom tensorflow.keras.layers import Dense\nmodel.add(Dense(64, activation='relu', input_dim=100))\n# 100×64 + 64 = 6464 parameters\n```",
        "code": "Input: [x1, x2, x3] (3 neurons)\nOutput: [y1, y2] (2 neurons)\n\ny1 = σ(w11×x1 + w12×x2 + w13×x3 + b1)\ny2 = σ(w21×x1 + w22×x2 + w23×x3 + b2)\n\nParameters: 3×2 weights + 2 biases = 8",
        "codeLanguage": "python"
      },
      "tags": [
        "neural-networks",
        "deep-learning",
        "dense",
        "layer",
        "(fully"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "activation-function",
      "name": "Activation Function",
      "parentId": "activation-functions",
      "sectionId": "10.2.1",
      "level": 2,
      "fullExplanation": "Activation functions introduce nonlinearity into neural networks. Without them, stacking linear layers would still be linear (composition of linear functions is linear). Activations enable networks to learn complex, nonlinear patterns. Properties to consider: nonlinearity, differentiability (for backprop), range (bounded vs unbounded), zero-centered outputs, and computational efficiency. Choice affects training dynamics and final performance.",
      "simpleExplanation": "The \"decision-maker\" inside each neuron. Without activation functions, a 100-layer network would behave like a single layer. Activations add the nonlinearity that allows deep learning to work.",
      "example": {
        "description": "Without activation (linear):\nLayer 1: y1 = W1·x\nLayer 2: y2 = W2·y1 = W2·W1·x = W'·x\n→ Still just a linear transformation!\n\nWith activation (ReLU):\nLayer 1: y1 = ReLU(W1·x)\nLayer 2: y2 = ReLU(W2·y1)\n→ Nonlinear! Can learn complex patterns."
      },
      "tags": [
        "neural-networks",
        "deep-learning",
        "activation",
        "function"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "relu",
      "name": "ReLU (Rectified Linear Unit)",
      "parentId": "activation-functions",
      "sectionId": "10.2.2",
      "level": 2,
      "fullExplanation": "ReLU: f(x) = max(0, x). Outputs x for positive values, 0 for negative. Advantages: computationally efficient, mitigates vanishing gradient (gradient is 1 for positive inputs), induces sparsity. Drawback: \"dying ReLU\" problem—neurons with large negative weights may never activate again (gradient is 0 for negative inputs). Default choice for hidden layers in most architectures.",
      "simpleExplanation": "If positive, output as-is. If negative, output zero. Super simple, super fast. Works great in practice. The default choice for most neural networks.",
      "example": {
        "description": "```\nReLU function:\nInput:  -5   -2    0    2    5\nOutput:  0    0    0    2    5\n\nGraph:\n    |      /\n  0 +"
      },
      "tags": [
        "neural-networks",
        "deep-learning",
        "relu",
        "(rectified",
        "linear"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "leaky-relu",
      "name": "Leaky ReLU",
      "parentId": "activation-functions",
      "sectionId": "10.2.3",
      "level": 2,
      "fullExplanation": "Leaky ReLU: f(x) = x if x > 0, else αx (typically α = 0.01). Unlike ReLU, it has a small gradient for negative inputs, preventing dying neurons. The small negative slope keeps neurons \"alive\" and allows learning even when inputs are negative. Parametric ReLU (PReLU) learns α from data. Often performs similarly to ReLU but more robust.",
      "simpleExplanation": "ReLU but with a small slope for negative values instead of flat zero. Negative inputs still produce a small output (and gradient), preventing dead neurons.",
      "example": {
        "description": "```\nLeaky ReLU (α=0.01):\nInput:  -100   -10    0    10   100\nOutput:  -1   -0.1    0    10   100\n\nGraph:\n      |       /\n    0 +"
      },
      "tags": [
        "neural-networks",
        "deep-learning",
        "leaky",
        "relu"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "tanh",
      "name": "Tanh",
      "parentId": "activation-functions",
      "sectionId": "10.2.4",
      "level": 2,
      "fullExplanation": "Tanh: f(x) = (eˣ - e⁻ˣ)/(eˣ + e⁻ˣ). Outputs range (-1, 1), zero-centered unlike sigmoid. Larger gradients than sigmoid (steeper), potentially faster learning. Suffers from vanishing gradients for large |x|. Often used in RNNs and when zero-centered outputs are beneficial. Related to sigmoid: tanh(x) = 2σ(2x) - 1.",
      "simpleExplanation": "Squashes values to range -1 to 1. Zero-centered (unlike sigmoid's 0 to 1). Often used in recurrent networks. Gradient vanishes for very large or small inputs.",
      "example": {
        "description": "```\nTanh function:\nInput:  -3    -1     0     1     3\nOutput: -0.99 -0.76  0   0.76  0.99\n\nGraph:\n   1 |        ****\n     |     ***\n   0 +"
      },
      "tags": [
        "neural-networks",
        "deep-learning",
        "tanh"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "gelu",
      "name": "GELU",
      "parentId": "activation-functions",
      "sectionId": "10.2.5",
      "level": 2,
      "fullExplanation": "GELU (Gaussian Error Linear Unit): f(x) = x × Φ(x), where Φ is the Gaussian CDF. Smoothly interpolates between 0 and x based on input value probability. Unlike ReLU's hard threshold, GELU provides smooth, probabilistic gating. Default activation in transformers (BERT, GPT). Slightly more expensive than ReLU but often performs better.",
      "simpleExplanation": "A smooth version of ReLU that weights inputs by how positive they are (probabilistically). The activation used in transformers like BERT and GPT. Smoother than ReLU, often works better for language models.",
      "example": {
        "description": "",
        "code": "GELU approximate:\nInput:  -3    -1     0     1     3\nOutput: -0.04 -0.16  0   0.84  2.96\n\nGELU ≈ 0.5x(1 + tanh(√(2/π)(x + 0.044715x³)))\n\nComparison with ReLU:\n- ReLU(-1) = 0\n- GELU(-1) ≈ -0.16 (small negative output)\n- Smoother transition around 0",
        "codeLanguage": "python"
      },
      "tags": [
        "neural-networks",
        "deep-learning",
        "gelu"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "swish",
      "name": "Swish",
      "parentId": "activation-functions",
      "sectionId": "10.2.6",
      "level": 2,
      "fullExplanation": "Swish: f(x) = x × σ(βx), where σ is sigmoid and β is learnable (often fixed at 1). Self-gated: uses the input itself to control how much passes through. Non-monotonic: slightly negative for some negative inputs before approaching 0. Outperforms ReLU on deeper networks. Used in EfficientNet and other modern architectures. SiLU (Sigmoid Linear Unit) is Swish with β=1.",
      "simpleExplanation": "The input multiplied by its own sigmoid. Self-gated: large positive inputs pass through, negative inputs are suppressed but not hard zeroed. Smooth, works well in deep networks.",
      "example": {
        "description": "",
        "code": "Swish (β=1):\nInput:  -4    -2     0     2     4\nOutput: -0.07 -0.24  0   1.76  3.93\n\nSwish(x) = x × sigmoid(x)\n- Swish(2) = 2 × 0.88 = 1.76\n- Swish(-2) = -2 × 0.12 = -0.24 (small negative)\n\nSmooth, non-monotonic near 0.",
        "codeLanguage": "python"
      },
      "tags": [
        "neural-networks",
        "deep-learning",
        "swish"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "backpropagation",
      "name": "Backpropagation",
      "parentId": "backpropagation",
      "sectionId": "10.3.1",
      "level": 2,
      "fullExplanation": "Backpropagation efficiently computes gradients of the loss with respect to all weights using the chain rule. Forward pass: compute activations layer by layer. Backward pass: compute gradients from output to input, propagating error derivatives. For weight wᵢⱼ: ∂L/∂wᵢⱼ = ∂L/∂aⱼ × ∂aⱼ/∂zⱼ × ∂zⱼ/∂wᵢⱼ. Enables training deep networks with gradient descent.",
      "simpleExplanation": "How neural networks learn. After making a prediction, figure out how much each weight contributed to the error. Start from the output, work backward through each layer, adjusting weights to reduce error. The chain rule from calculus makes this efficient.",
      "example": {
        "description": "Simple network: x → [w1] → h → [w2] → y\nTarget: t, Prediction: y, Loss: L = (y-t)²\n\nForward pass:\nh = relu(w1 × x)\ny = w2 × h\n\nBackward pass:\n∂L/∂y = 2(y-t)\n∂L/∂w2 = ∂L/∂y × h\n∂L/∂h = ∂L/∂y × w2\n∂L/∂w1 = ∂L/∂h × relu'(w1×x) × x\n\nUpdate: w1 -= lr × ∂L/∂w1, w2 -= lr × ∂L/∂w2"
      },
      "tags": [
        "neural-networks",
        "deep-learning",
        "backpropagation"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "computational-graph",
      "name": "Computational Graph",
      "parentId": "backpropagation",
      "sectionId": "10.3.2",
      "level": 2,
      "fullExplanation": "A computational graph represents operations as nodes and data flow as edges. Forward mode: traverse graph input-to-output computing values. Reverse mode (backprop): traverse output-to-input computing gradients. Intermediate values are cached for gradient computation. Frameworks like PyTorch and TensorFlow build these graphs automatically, enabling automatic differentiation for any differentiable operation.",
      "simpleExplanation": "A flowchart of math operations. Draw how inputs become outputs through operations. Forward: follow arrows to compute result. Backward: follow arrows in reverse to compute gradients. Frameworks build this automatically.",
      "example": {
        "description": "",
        "code": "Expression: L = (wx + b - y)²\n\nComputational graph:\nx ─────┐\n       ├─[×]──┐\nw ─────┘      ├─[+]───┐\n              │       ├─[-]───[²]──→ L\nb ────────────┘       │\n                      │\ny ────────────────────┘\n\nForward: x=2, w=3, b=1, y=5\nwx=6, wx+b=7, 7-5=2, 2²=4\n\nBackward: dL/dw = 2(wx+b-y)×x = 2×2×2 = 8",
        "codeLanguage": "python"
      },
      "tags": [
        "neural-networks",
        "deep-learning",
        "computational",
        "graph"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "vanishing-gradient",
      "name": "Vanishing Gradient",
      "parentId": "backpropagation",
      "sectionId": "10.3.3",
      "level": 2,
      "fullExplanation": "Vanishing gradients occur when gradients become exponentially small as they propagate backward through many layers. With sigmoid/tanh (gradients < 1), multiplying many small gradients → near-zero. Early layers receive tiny gradients, learning extremely slowly or not at all. Solutions: ReLU activation (gradient = 1), batch normalization, residual connections, proper initialization.",
      "simpleExplanation": "Gradients shrink as they travel backward through layers. By the time they reach early layers, they're so small the network can't learn. Deep networks used to be impossible to train until solutions like ReLU and skip connections were developed.",
      "example": {
        "description": "100-layer network with sigmoid:\n- Sigmoid gradient max ≈ 0.25\n- Gradient at layer 1 ≈ 0.25¹⁰⁰ ≈ 10⁻⁶⁰\n- Effectively zero—no learning!\n\nWith ReLU:\n- ReLU gradient = 1 (for positive inputs)\n- Gradient at layer 1 ≈ 1¹⁰⁰ = 1\n- Full gradient signal—learning happens!"
      },
      "tags": [
        "neural-networks",
        "deep-learning",
        "vanishing",
        "gradient"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "exploding-gradient",
      "name": "Exploding Gradient",
      "parentId": "backpropagation",
      "sectionId": "10.3.4",
      "level": 2,
      "fullExplanation": "Exploding gradients occur when gradients become exponentially large, causing unstable training (loss becomes NaN, weights overflow). Common in RNNs processing long sequences and deep networks with improper initialization. Solutions: gradient clipping (cap gradient magnitude), proper weight initialization (Xavier/He), batch normalization, LSTM/GRU for sequences.",
      "simpleExplanation": "Gradients grow so large they break training. Weights update by huge amounts, oscillating wildly or overflowing to infinity. Common in recurrent networks. Fixed by clipping gradients to a maximum value.",
      "example": {
        "description": "Gradient explosion:\nStep 1: weight = 1.0, gradient = 2.0, lr = 0.1\n        weight = 1.0 - 0.1×2.0 = 0.8\n\nStep 2: gradient = 20.0 (exploding!)\n        weight = 0.8 - 0.1×20.0 = -1.2\n\nStep 3: gradient = 200.0 (worse!)\n        weight = -1.2 - 0.1×200.0 = -21.2\n\nGradient clipping (max=5):\nStep 2: gradient = clip(20.0, 5) = 5.0\n        weight = 0.8 - 0.1×5.0 = 0.3 ✓ Stable"
      },
      "tags": [
        "neural-networks",
        "deep-learning",
        "exploding",
        "gradient"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "weight-initialization",
      "name": "Weight Initialization",
      "parentId": "weight-initialization",
      "sectionId": "10.4.1",
      "level": 2,
      "fullExplanation": "Weight initialization sets starting values for network parameters before training. Poor initialization causes vanishing/exploding gradients, slow convergence, or getting stuck in bad optima. Goals: maintain signal variance through layers, prevent saturation of activations, enable gradient flow. Modern initializations (Xavier, He) scale weights based on layer dimensions. Biases typically initialized to zero.",
      "simpleExplanation": "How to set initial weights before training. Too small: signal dies out. Too large: activations saturate or explode. Just right: signal flows through all layers. The starting point matters a lot.",
      "example": {
        "description": "Bad initialization:\n- Weights initialized to 0: All neurons compute same thing, never differentiate\n- Weights initialized to 100: Activations saturate, gradients vanish\n\nGood initialization (He):\n- Weights ∼ N(0, 2/n_in)\n- Maintains variance across layers\n- Enables training of deep networks"
      },
      "tags": [
        "neural-networks",
        "deep-learning",
        "weight",
        "initialization"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "xavier-initialization",
      "name": "Xavier Initialization",
      "parentId": "weight-initialization",
      "sectionId": "10.4.2",
      "level": 2,
      "fullExplanation": "Xavier (Glorot) initialization scales weights by layer dimensions: W ∼ Uniform(-√(6/(n_in+n_out)), √(6/(n_in+n_out))) or W ∼ Normal(0, √(2/(n_in+n_out))). Designed for sigmoid/tanh activations to maintain variance of activations and gradients across layers. Key insight: both forward (activations) and backward (gradients) signals should preserve variance.",
      "simpleExplanation": "Initialize weights based on how many neurons connect in and out. Smaller weights for layers with many connections. Keeps signal strength consistent as it flows through the network. Designed for sigmoid and tanh activations.",
      "example": {
        "description": "Layer with 784 inputs, 256 outputs:\n\nXavier uniform:\nlimit = √(6 / (784 + 256)) = √(6/1040) = 0.076\nweights ∼ Uniform(-0.076, 0.076)\n\nXavier normal:\nstd = √(2 / (784 + 256)) = √(2/1040) = 0.044\nweights ∼ Normal(0, 0.044)",
        "code": "import torch.nn as nn\nnn.init.xavier_uniform_(layer.weight)\nnn.init.xavier_normal_(layer.weight)",
        "codeLanguage": "python"
      },
      "tags": [
        "neural-networks",
        "deep-learning",
        "xavier",
        "initialization"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "he-initialization",
      "name": "He Initialization",
      "parentId": "weight-initialization",
      "sectionId": "10.4.3",
      "level": 2,
      "fullExplanation": "He (Kaiming) initialization is designed for ReLU activations: W ∼ Normal(0, √(2/n_in)). Accounts for ReLU zeroing half the values (factor of 2 vs Xavier). Maintains variance when using ReLU/Leaky ReLU. Essential for training very deep networks with ReLU. The default choice for modern networks using ReLU-family activations.",
      "simpleExplanation": "Like Xavier but adjusted for ReLU. Since ReLU zeros out half the values, He init uses larger weights (√2 factor) to compensate. The go-to for ReLU networks.",
      "example": {
        "description": "Layer with 784 inputs, ReLU activation:\n\nHe normal:\nstd = √(2 / 784) = 0.0505\nweights ∼ Normal(0, 0.0505)\n\nHe uniform:\nlimit = √(6 / 784) = 0.0875\nweights ∼ Uniform(-0.0875, 0.0875)",
        "code": "import torch.nn as nn\nnn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')\nnn.init.kaiming_normal_(layer.weight, nonlinearity='relu')",
        "codeLanguage": "python"
      },
      "tags": [
        "neural-networks",
        "deep-learning",
        "initialization"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "cnn",
      "name": "Convolutional Neural Network",
      "parentId": "cnns-convolutional-neural-networks",
      "sectionId": "11.1.1",
      "level": 2,
      "fullExplanation": "CNNs are specialized networks for grid-like data (images, time series). They use convolutional layers that apply learned filters to local regions, capturing spatial hierarchies. Key properties: local connectivity (each neuron sees only a region), weight sharing (same filter applied everywhere), and translation equivariance (patterns detected anywhere). Typically: Conv → Activation → Pool, repeated, then Dense layers for classification.",
      "simpleExplanation": "Neural networks designed for images. Instead of connecting every pixel to every neuron (expensive!), use small filters that slide across the image detecting patterns. First layers find edges, deeper layers find shapes, deepest layers find objects.",
      "example": {
        "description": "",
        "code": "Input: 224×224×3 image\n\nConv1: 64 filters, 3×3 → 224×224×64 (edges, colors)\nPool1: 2×2 → 112×112×64\nConv2: 128 filters, 3×3 → 112×112×128 (textures)\nPool2: 2×2 → 56×56×128\nConv3: 256 filters, 3×3 → 56×56×256 (parts)\nPool3: 2×2 → 28×28×256\n...\nFlatten: 7×7×512 → 25088\nDense: 25088 → 1000 (ImageNet classes)",
        "codeLanguage": "python"
      },
      "tags": [
        "deep-learning",
        "architectures",
        "convolutional",
        "neural",
        "network"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "convolution",
      "name": "Convolution Operation",
      "parentId": "cnns-convolutional-neural-networks",
      "sectionId": "11.1.2",
      "level": 2,
      "fullExplanation": "Convolution slides a kernel (filter) across input, computing dot products at each position to produce an output feature map. For 2D: output[i,j] = Σₘ Σₙ input[i+m, j+n] × kernel[m,n]. Multiple kernels produce multiple channels. Learns edge detectors, texture patterns, and higher-level features. Kernel size, stride, and padding control output dimensions.",
      "simpleExplanation": "A small filter slides across the image, multiplying and summing at each position. Different filters detect different patterns: one might find vertical edges, another horizontal edges. The network learns which filters are useful.",
      "example": {
        "description": "3×3 edge detection kernel:",
        "code": "[-1, 0, 1]\n[-1, 0, 1]\n[-1, 0, 1]\n\nApply to image patch:\n[10, 10, 50]\n[10, 10, 50]   → Sum of element-wise product\n[10, 10, 50]     = -10-10-10 + 0 + 50+50+50 = 120\n\nHigh value = vertical edge detected!",
        "codeLanguage": "python"
      },
      "tags": [
        "deep-learning",
        "architectures",
        "convolution",
        "operation"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "pooling",
      "name": "Pooling Layer",
      "parentId": "cnns-convolutional-neural-networks",
      "sectionId": "11.1.3",
      "level": 2,
      "fullExplanation": "Pooling reduces spatial dimensions by aggregating values in local regions. Max pooling takes the maximum value; average pooling takes the mean. Reduces parameters and computation, provides translation invariance (small shifts don't change output), and enlarges receptive field. Typically 2×2 with stride 2, reducing each dimension by half. Global pooling reduces to single value per channel.",
      "simpleExplanation": "Shrink the image by summarizing small regions. Max pooling: keep the largest value in each 2×2 block (preserves strongest features). Average pooling: keep the average (smoother). Makes the network faster and more robust to small shifts.",
      "example": {
        "description": "Max pooling 2×2:",
        "code": "Input 4×4:                Output 2×2:\n[1, 3, 2, 4]\n[5, 6, 7, 8]  →  [6,  8]\n[2, 2, 1, 1]     [4,  3]\n[3, 4, 2, 3]\n\nEach 2×2 region → max value\nTop-left: max(1,3,5,6) = 6",
        "codeLanguage": "python"
      },
      "tags": [
        "deep-learning",
        "architectures",
        "pooling",
        "layer"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "stride-padding",
      "name": "Stride and Padding",
      "parentId": "cnns-convolutional-neural-networks",
      "sectionId": "11.1.4",
      "level": 2,
      "fullExplanation": "Stride is the step size of the convolution filter. Stride 1: move one pixel at a time. Stride 2: skip every other position, halving output size. Padding adds border pixels (usually zeros) to control output dimensions. \"Same\" padding: output size equals input size. \"Valid\" padding: no padding, output shrinks. Together they determine spatial dimensions of feature maps.",
      "simpleExplanation": "Stride: how far the filter moves each step. Bigger stride = smaller output.\nPadding: add border around the image. Keeps size constant and allows filters to see edge pixels.",
      "example": {
        "description": "Input: 6×6, Kernel: 3×3\n\nStride 1, no padding:\nOutput = (6-3)/1 + 1 = 4×4\n\nStride 2, no padding:\nOutput = (6-3)/2 + 1 = 2×2\n\nStride 1, padding 1:\nOutput = (6+2-3)/1 + 1 = 6×6 (same size!)"
      },
      "tags": [
        "deep-learning",
        "architectures",
        "stride",
        "padding"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "receptive-field",
      "name": "Receptive Field",
      "parentId": "cnns-convolutional-neural-networks",
      "sectionId": "11.1.5",
      "level": 2,
      "fullExplanation": "The receptive field is the input region that affects a particular output neuron. Early layers have small receptive fields (see local patterns); deeper layers have larger receptive fields (see global patterns). Grows through convolutions and pooling. Larger receptive field = more context. Critical for understanding what scale of patterns a network can detect. Effective receptive field often smaller than theoretical.",
      "simpleExplanation": "How much of the original image does one neuron \"see\"? First layer neurons see just a small patch. Deep layer neurons effectively see most of the image because they combine info from many earlier neurons.",
      "example": {
        "description": "",
        "code": "Layer 1 (3×3 conv): Receptive field = 3×3\nLayer 2 (3×3 conv): RF = 5×5 (each position sees 3×3 of layer 1, which sees 3×3)\nLayer 3 (3×3 conv): RF = 7×7\n+ 2×2 pooling: RF doubles\n\nAfter several layers:\n- A single neuron might \"see\" 128×128 of original image\n- Can detect large objects, context, relationships",
        "codeLanguage": "python"
      },
      "tags": [
        "deep-learning",
        "architectures",
        "receptive",
        "field"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "feature-map",
      "name": "Feature Map",
      "parentId": "cnns-convolutional-neural-networks",
      "sectionId": "11.1.6",
      "level": 2,
      "fullExplanation": "A feature map is the output of applying one filter to the input, representing where a particular pattern is detected. Each convolutional layer produces multiple feature maps (one per filter), stacked as channels. Early maps show edges and colors; later maps show textures, parts, and objects. Feature maps are visualizable—useful for understanding what the network learns.",
      "simpleExplanation": "The result of one filter scanning an image. High values where the pattern is found, low values elsewhere. Like a heat map of \"where is this pattern?\" Many filters = many feature maps = many different patterns detected.",
      "example": {
        "description": "",
        "code": "Filter: Horizontal edge detector\nApplied to image of a building:\n\nFeature map:\n[0.1, 0.2, 0.1]\n[0.9, 0.9, 0.9]  ← Roof edge detected (high values)\n[0.1, 0.2, 0.1]\n[0.8, 0.8, 0.8]  ← Window ledge detected\n[0.1, 0.1, 0.1]",
        "codeLanguage": "python"
      },
      "tags": [
        "deep-learning",
        "architectures",
        "feature"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "rnn",
      "name": "Recurrent Neural Network",
      "parentId": "rnns-recurrent-neural-networks",
      "sectionId": "11.2.1",
      "level": 2,
      "fullExplanation": "RNNs process sequential data by maintaining a hidden state that carries information across time steps. At each step: hₜ = f(hₜ₋₁, xₜ). This creates cycles in the computational graph, enabling memory of past inputs. Backpropagation through time (BPTT) unfolds the network across time for gradient computation. Struggles with long-term dependencies due to vanishing gradients.",
      "simpleExplanation": "A network that remembers. Processes sequences one element at a time, keeping a \"memory\" (hidden state) that gets updated at each step. Good for text, audio, time series—anything where order matters.",
      "example": {
        "description": "Processing \"hello\":",
        "code": "h0 = 0 (initial state)\nh1 = tanh(W_h×h0 + W_x×embed(\"h\")) = [0.5, -0.2, ...]\nh2 = tanh(W_h×h1 + W_x×embed(\"e\")) = [0.3, 0.4, ...]\nh3 = tanh(W_h×h2 + W_x×embed(\"l\")) = [0.1, 0.6, ...]\nh4 = tanh(W_h×h3 + W_x×embed(\"l\")) = [0.2, 0.5, ...]\nh5 = tanh(W_h×h4 + W_x×embed(\"o\")) = [0.4, 0.3, ...]\n\nh5 contains accumulated information about \"hello\"",
        "codeLanguage": "python"
      },
      "tags": [
        "deep-learning",
        "architectures",
        "recurrent",
        "neural",
        "network"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "hidden-state",
      "name": "Hidden State",
      "parentId": "rnns-recurrent-neural-networks",
      "sectionId": "11.2.2",
      "level": 2,
      "fullExplanation": "The hidden state is an RNN's memory vector, updated at each time step and carrying information from past inputs. Dimension is a hyperparameter (typically 64-1024). Contains a compressed representation of the sequence seen so far. Can be used for classification (use final state), sequence generation (sample from state at each step), or encoding (state as sequence representation).",
      "simpleExplanation": "The RNN's memory. A vector that summarizes everything it's seen so far. Gets updated with each new input. Can be used to predict the next word, classify the sequence, or encode it for another task.",
      "example": {
        "description": "Sentiment classification:",
        "code": "\"This movie was amazing!\"\n\nh0 = [0, 0, 0]\nh1 = update(h0, \"This\") = [0.1, 0.2, 0.1]\nh2 = update(h1, \"movie\") = [0.2, 0.3, 0.0]\nh3 = update(h2, \"was\") = [0.2, 0.2, 0.1]\nh4 = update(h3, \"amazing\") = [0.8, -0.1, 0.5]\nh5 = update(h4, \"!\") = [0.9, -0.2, 0.6]\n\nFinal hidden state h5 → Dense → sigmoid → 0.95 (positive sentiment)",
        "codeLanguage": "python"
      },
      "tags": [
        "deep-learning",
        "architectures",
        "hidden",
        "state"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "lstm",
      "name": "LSTM (Long Short-Term Memory)",
      "parentId": "rnns-recurrent-neural-networks",
      "sectionId": "11.2.3",
      "level": 2,
      "fullExplanation": "LSTM addresses vanishing gradients with a gated architecture. Cell state (Cₜ) carries long-term memory; hidden state (hₜ) carries short-term. Three gates control information flow: forget gate (what to remove from cell state), input gate (what to add to cell state), output gate (what to output). Gates use sigmoid (0-1) to control how much passes through. Enables learning dependencies over 100s of time steps.",
      "simpleExplanation": "RNN with a better memory. Has gates that decide: what to forget, what to remember, what to output. The special \"cell state\" can carry important information unchanged across many time steps. Solves the vanishing gradient problem.",
      "example": {
        "description": "",
        "code": "LSTM at time t:\n\nForget gate: fₜ = σ(Wf·[hₜ₋₁, xₜ] + bf)\n\"Should I forget any of the old memory?\"\n\nInput gate: iₜ = σ(Wi·[hₜ₋₁, xₜ] + bi)\nCandidate: C̃ₜ = tanh(Wc·[hₜ₋₁, xₜ] + bc)\n\"What new info should I store?\"\n\nCell update: Cₜ = fₜ ⊙ Cₜ₋₁ + iₜ ⊙ C̃ₜ\n\"Forget some old + add some new\"\n\nOutput gate: oₜ = σ(Wo·[hₜ₋₁, xₜ] + bo)\nHidden: hₜ = oₜ ⊙ tanh(Cₜ)\n\"What part of memory to output?\"",
        "codeLanguage": "python"
      },
      "tags": [
        "deep-learning",
        "architectures",
        "lstm",
        "(long",
        "short"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "gru",
      "name": "GRU (Gated Recurrent Unit)",
      "parentId": "rnns-recurrent-neural-networks",
      "sectionId": "11.2.4",
      "level": 2,
      "fullExplanation": "GRU simplifies LSTM with two gates instead of three, combining forget and input into an update gate. Reset gate (rₜ) controls how much past to ignore; update gate (zₜ) controls how much past state to keep vs new candidate. Fewer parameters than LSTM, faster training, similar performance on many tasks. Often preferred for smaller datasets or simpler sequences.",
      "simpleExplanation": "A simpler LSTM. Two gates instead of three, no separate cell state. Reset gate: how much to ignore the past. Update gate: balance between old state and new state. Faster, lighter, often works just as well.",
      "example": {
        "description": "",
        "code": "GRU at time t:\n\nReset gate: rₜ = σ(Wr·[hₜ₋₁, xₜ])\n\"How much to reset/ignore past?\"\n\nUpdate gate: zₜ = σ(Wz·[hₜ₋₁, xₜ])\n\"How much to update with new info?\"\n\nCandidate: h̃ₜ = tanh(W·[rₜ ⊙ hₜ₋₁, xₜ])\n\"New hidden state candidate\"\n\nHidden: hₜ = (1-zₜ) ⊙ hₜ₋₁ + zₜ ⊙ h̃ₜ\n\"Blend old and new based on update gate\"",
        "codeLanguage": "python"
      },
      "tags": [
        "deep-learning",
        "architectures",
        "(gated",
        "recurrent",
        "unit)"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "bidirectional-rnn",
      "name": "Bidirectional RNN",
      "parentId": "rnns-recurrent-neural-networks",
      "sectionId": "11.2.5",
      "level": 2,
      "fullExplanation": "Bidirectional RNNs process sequences in both directions with two separate RNNs. Forward RNN reads left-to-right; backward RNN reads right-to-left. Outputs are concatenated, giving each position context from both past and future. Essential for tasks where future context matters (NER, translation). Doubles parameters and computation but often significantly improves accuracy.",
      "simpleExplanation": "Run the sequence forward AND backward. Each position knows what came before AND what comes after. For understanding \"bank\" in \"I deposited money at the bank,\" seeing future words helps disambiguate.",
      "example": {
        "description": "",
        "code": "Sentence: \"The cat sat on the mat\"\n\nForward RNN:\n→ The → cat → sat → on → the → mat →\nh1f    h2f   h3f   h4f  h5f   h6f\n\nBackward RNN:\n← The ← cat ← sat ← on ← the ← mat ←\nh1b    h2b   h3b   h4b  h5b   h6b\n\nCombined for \"sat\":\n[h3f || h3b] contains context from both directions\n\"sat\" knows: subject is \"cat\" (before), action is on \"mat\" (after)",
        "codeLanguage": "python"
      },
      "tags": [
        "deep-learning",
        "architectures",
        "bidirectional"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "attention",
      "name": "Attention Mechanism",
      "parentId": "attention-mechanisms",
      "sectionId": "11.3.1",
      "level": 2,
      "fullExplanation": "Attention computes a weighted combination of values based on query-key similarity. Given queries Q, keys K, and values V: Attention(Q,K,V) = softmax(QKᵀ/√d)V. Each query attends to all keys, producing attention weights that determine how much each value contributes to the output. Enables direct connections between any positions, bypassing sequential processing limitations of RNNs.",
      "simpleExplanation": "Let the model focus on relevant parts. Like highlighting important words when reading. For each position, calculate how relevant every other position is, then take a weighted average. Enables direct long-range connections.",
      "example": {
        "description": "Translating \"The cat sat on the mat\":\nWhen generating \"le\" (French for \"the\"):\n\nAttention weights:\n- \"The\": 0.8 (highly relevant!)\n- \"cat\": 0.1\n- \"sat\": 0.05\n- \"on\": 0.02\n- \"the\": 0.02\n- \"mat\": 0.01\n\nOutput = 0.8×embed(\"The\") + 0.1×embed(\"cat\") + ...\nFocus on \"The\" to generate \"le\""
      },
      "tags": [
        "deep-learning",
        "architectures",
        "attention",
        "mechanism"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "self-attention",
      "name": "Self-Attention",
      "parentId": "attention-mechanisms",
      "sectionId": "11.3.2",
      "level": 2,
      "fullExplanation": "Self-attention applies attention within a single sequence—each position attends to all positions in the same sequence. Q, K, V all come from the same input (projected with different learned matrices). Captures relationships between any positions regardless of distance. Foundation of transformers. Computational cost O(n²) in sequence length is a key limitation for very long sequences.",
      "simpleExplanation": "Each word looks at every other word in the same sentence to understand context. \"Bank\" in \"river bank\" attends to \"river\" to understand its meaning. Every position can directly connect to every other position.",
      "example": {
        "description": "",
        "code": "Sentence: \"The animal didn't cross the road because it was too tired\"\n\nWhat does \"it\" refer to?\n\nSelf-attention for \"it\":\n- \"The\": 0.05\n- \"animal\": 0.7 ← High attention! \"it\" refers to \"animal\"\n- \"didn't\": 0.02\n- \"cross\": 0.03\n- \"road\": 0.15\n- \"tired\": 0.05\n\n\"it\" attends strongly to \"animal\" → understands the reference",
        "codeLanguage": "python"
      },
      "tags": [
        "deep-learning",
        "architectures",
        "self",
        "attention"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "multi-head-attention",
      "name": "Multi-Head Attention",
      "parentId": "attention-mechanisms",
      "sectionId": "11.3.3",
      "level": 2,
      "fullExplanation": "Multi-head attention runs multiple attention operations in parallel, each with different learned projections. Each \"head\" can focus on different aspects (e.g., one on syntax, another on semantics). Outputs are concatenated and projected. MultiHead(Q,K,V) = Concat(head₁,...,headₕ)Wᵒ where headᵢ = Attention(QWᵢQ, KWᵢK, VWᵢV). Typically 8-16 heads.",
      "simpleExplanation": "Multiple attention patterns at once. One head might focus on nearby words, another on verbs, another on pronouns. Different perspectives combined give richer understanding than single attention.",
      "example": {
        "description": "",
        "code": "8-head attention on \"The cat sat on the mat\":\n\nHead 1: Focuses on subject-verb relationships\n        \"sat\" → \"cat\" (subject)\n\nHead 2: Focuses on positional relationships\n        \"on\" → \"mat\" (location)\n\nHead 3: Focuses on article-noun relationships\n        \"the\" → \"cat\", \"the\" → \"mat\"\n\nHead 4-8: Other patterns...\n\nConcatenate all heads → Rich representation",
        "codeLanguage": "python"
      },
      "tags": [
        "deep-learning",
        "architectures",
        "multi",
        "head",
        "attention"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "transformer",
      "name": "Transformer Architecture",
      "parentId": "transformers",
      "sectionId": "11.4.1",
      "level": 2,
      "fullExplanation": "Transformers use self-attention to process sequences in parallel, unlike sequential RNNs. Architecture: embedding + positional encoding → N encoder/decoder blocks → output. Each block has multi-head attention and feed-forward layers with residual connections and layer normalization. Enables massive parallelization, better long-range dependencies, and scales to billions of parameters. Foundation for GPT, BERT, and modern LLMs.",
      "simpleExplanation": "A powerful architecture using attention instead of recurrence. Processes all positions simultaneously (fast!), directly connects any positions (good for long-range patterns), and scales to huge sizes. The basis of ChatGPT, BERT, and most modern AI.",
      "example": {
        "description": "",
        "code": "Transformer Encoder Block:\nInput\n  ↓\n[Multi-Head Self-Attention] ← All positions attend to all\n  ↓ + Residual Connection\n[Layer Norm]\n  ↓\n[Feed-Forward Network] ← Process each position independently\n  ↓ + Residual Connection\n[Layer Norm]\n  ↓\nOutput\n\nStack 6-96 blocks for complete model.",
        "codeLanguage": "python"
      },
      "tags": [
        "deep-learning",
        "architectures",
        "transformer",
        "architecture"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "positional-encoding",
      "name": "Positional Encoding",
      "parentId": "transformers",
      "sectionId": "11.4.2",
      "level": 2,
      "fullExplanation": "Since transformers have no inherent position sense (attention is permutation equivariant), positional encodings add position information. Original approach: sinusoidal functions at different frequencies. PE(pos,2i) = sin(pos/10000^(2i/d)), PE(pos,2i+1) = cos(pos/10000^(2i/d)). Learned position embeddings are an alternative. Enables the model to use position information despite parallel processing.",
      "simpleExplanation": "Tell the model where each word is. Without positions, \"dog bites man\" = \"man bites dog\" to attention. Positional encoding adds a unique pattern to each position. Model learns to use these patterns to understand word order.",
      "example": {
        "description": "",
        "code": "\"The cat sat\"\n\nWithout positional encoding:\nembed(\"The\"), embed(\"cat\"), embed(\"sat\") ← No position info!\n\nWith positional encoding:\nembed(\"The\") + pos_enc(0)\nembed(\"cat\") + pos_enc(1)\nembed(\"sat\") + pos_enc(2)\n\nPosition 0: [sin(0), cos(0), sin(0/ω), cos(0/ω), ...]\nPosition 1: [sin(1), cos(1), sin(1/ω), cos(1/ω), ...]\n\nEach position has unique signature.",
        "codeLanguage": "python"
      },
      "tags": [
        "deep-learning",
        "architectures",
        "positional",
        "encoding"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "encoder-decoder",
      "name": "Encoder-Decoder Architecture",
      "parentId": "transformers",
      "sectionId": "11.4.3",
      "level": 2,
      "fullExplanation": "Encoder-decoder separates understanding input (encoder) from generating output (decoder). Encoder processes source sequence, creating representations. Decoder generates target sequence, attending to encoder outputs via cross-attention. Encoder is bidirectional (sees full input); decoder is unidirectional (only sees past outputs during generation). Used for sequence-to-sequence tasks: translation, summarization, question answering.",
      "simpleExplanation": "Two parts: encoder reads and understands the input, decoder generates the output. Encoder sees the whole input at once. Decoder generates one token at a time, looking back at the input (via attention) and its own previous outputs.",
      "example": {
        "description": "",
        "code": "Translation: \"Hello world\" → \"Bonjour monde\"\n\nEncoder:\n\"Hello world\" → [rich representations of each word]\n                → Memory for decoder to attend to\n\nDecoder (generating):\nStep 1: [START] + attend to encoder → \"Bonjour\"\nStep 2: \"Bonjour\" + attend to encoder → \"monde\"\nStep 3: \"Bonjour monde\" + attend to encoder → [END]\n\nCross-attention: \"Bonjour\" attends to \"Hello\"\n                 \"monde\" attends to \"world\"",
        "codeLanguage": "python"
      },
      "tags": [
        "deep-learning",
        "architectures",
        "encoder",
        "decoder",
        "architecture"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "decoder-only",
      "name": "Decoder-Only Architecture",
      "parentId": "transformers",
      "sectionId": "11.4.4",
      "level": 2,
      "fullExplanation": "Decoder-only transformers use only the decoder stack, trained for autoregressive language modeling. Each position attends only to previous positions (causal masking). Given context, predicts next token. Simpler architecture, scales well, and achieves strong performance on many tasks when large. GPT series, LLaMA, and most modern LLMs use this architecture. Prompt engineering replaces explicit encoder input.",
      "simpleExplanation": "Just the decoder part, no encoder. Predicts the next word based on all previous words. The architecture behind ChatGPT. Can handle many tasks by framing them as text generation.",
      "example": {
        "description": "",
        "code": "GPT-style generation:\n\nInput: \"The capital of France is\"\n\nModel attends causally:\nPosition 5 (\"is\") can attend to: \"The\", \"capital\", \"of\", \"France\", \"is\"\nCannot see future (masked)\n\nOutput distribution at position 5:\nP(next = \"Paris\") = 0.95 ← Most likely\nP(next = \"Lyon\") = 0.02\n...\n\nGenerate: \"Paris\"",
        "codeLanguage": "python"
      },
      "tags": [
        "deep-learning",
        "architectures",
        "decoder",
        "only",
        "architecture"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "sparse-attention",
      "name": "Sparse Attention",
      "parentId": "efficient-transformers",
      "sectionId": "11.5.1",
      "level": 2,
      "fullExplanation": "Sparse attention reduces O(n²) complexity by attending only to a subset of positions. Patterns include: local attention (nearby positions), strided attention (every k positions), random attention, or learned sparsity. Longformer combines local windows with global tokens. BigBird uses random + window + global. Enables processing longer sequences (thousands to millions of tokens) with manageable compute.",
      "simpleExplanation": "Don't attend to everything—pick important positions. Nearby words, global summary tokens, and some random connections. Much faster than full attention, can handle much longer documents.",
      "example": {
        "description": "",
        "code": "Full attention (1000 tokens):\n1000 × 1000 = 1,000,000 attention computations\n\nSparse attention:\n- Local window (128 tokens): 1000 × 128 = 128,000\n- Global tokens (10): 1000 × 10 = 10,000\n- Random (16 per position): 1000 × 16 = 16,000\nTotal: 154,000 ← 85% reduction!\n\nPattern for position 500:\nAttends to: [436-564] (local) + [0,1,2...] (global) + random positions",
        "codeLanguage": "python"
      },
      "tags": [
        "deep-learning",
        "architectures",
        "sparse",
        "attention"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "linear-attention",
      "name": "Linear Attention",
      "parentId": "efficient-transformers",
      "sectionId": "11.5.2",
      "level": 2,
      "fullExplanation": "Linear attention approximates softmax attention with O(n) complexity. Key insight: rewrite attention as kernel function, decompose so computation is linear in sequence length. Methods include Linformer (low-rank projection), Performer (random features for softmax approximation), and Linear Transformer. Trade-off: slight accuracy loss for dramatic speed gains on long sequences.",
      "simpleExplanation": "Make attention linear instead of quadratic. Use math tricks to avoid computing all n² pairs. Much faster for long sequences, with some quality trade-off.",
      "example": {
        "description": "",
        "code": "Standard attention:\nsoftmax(QKᵀ)V = O(n²d)\n\nLinear attention (Performer):\nReplace softmax with φ(Q)φ(K)ᵀ where φ is random feature map\nφ(Q)(φ(K)ᵀV) = O(nd²)\n\nFor sequence length n=10000, d=64:\nStandard: 100,000,000 × 64 = 6.4B ops\nLinear: 10,000 × 64 × 64 = 41M ops ← 150× faster!",
        "codeLanguage": "python"
      },
      "tags": [
        "deep-learning",
        "architectures",
        "linear",
        "attention"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "ssm",
      "name": "State Space Model (SSM)",
      "parentId": "state-space-models",
      "sectionId": "11.6.1",
      "level": 2,
      "fullExplanation": "State Space Models are a family of sequence models based on continuous-time systems: dx/dt = Ax + Bu, y = Cx + Du. Discretized for discrete sequences. Key advantage: can be computed as RNN (sequential) or convolution (parallel), getting benefits of both. Linear time complexity O(n), constant memory. Mamba and S4 are prominent examples achieving transformer-level performance with better efficiency.",
      "simpleExplanation": "An alternative to transformers for sequences. Based on control theory mathematics. Can process sequences as efficiently as CNNs while handling long-range dependencies like RNNs. Linear time complexity—much faster than transformers for long sequences.",
      "example": {
        "description": "",
        "code": "SSM computation modes:\n\n1. Recurrent mode (inference):\n   h₁ = Āh₀ + B̄x₁\n   h₂ = Āh₁ + B̄x₂\n   ... sequential, O(1) memory per step\n\n2. Convolutional mode (training):\n   K = [CB̄, CĀB̄, CĀ²B̄, ...]  ← Precompute kernel\n   y = K * x  ← Single convolution, parallel\n\nSame computation, different modes for different use cases.",
        "codeLanguage": "python"
      },
      "tags": [
        "deep-learning",
        "architectures",
        "state",
        "space",
        "model"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "mamba",
      "name": "Mamba",
      "parentId": "state-space-models",
      "sectionId": "11.6.2",
      "level": 2,
      "fullExplanation": "Mamba introduces selective state spaces with input-dependent parameters. Unlike fixed SSM matrices, Mamba's A, B, C parameters depend on the input, enabling content-aware reasoning similar to attention. Achieves transformer-quality results with O(n) time and memory. Hardware-optimized implementation enables 5× faster training and inference than transformers of similar size. Strong results on language modeling and long-context tasks.",
      "simpleExplanation": "A selective state space model where the \"memory\" adapts based on what it's reading. Can decide what to remember and forget based on content, like attention but with linear complexity. Much faster than transformers for long sequences.",
      "example": {
        "description": "",
        "code": "Standard SSM (fixed parameters):\nA, B, C are constant → Same processing for all inputs\n\nMamba (selective, input-dependent):\nA(x), B(x), C(x) depend on input x\n\"Interesting content → remember more\"\n\"Boring content → forget faster\"\n\nResult: Content-aware long-range modeling\nSpeed: Linear in sequence length\nQuality: Matches transformers on language modeling",
        "codeLanguage": "python"
      },
      "tags": [
        "deep-learning",
        "architectures",
        "mamba"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "reinforcement-learning",
      "name": "Reinforcement Learning",
      "parentId": "specialized-learning",
      "sectionId": "14.2",
      "level": 1,
      "fullExplanation": "Reinforcement Learning covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about reinforcement learning and related techniques.",
      "example": {
        "description": "This section contains concepts related to reinforcement learning."
      },
      "tags": [
        "transfer-learning",
        "reinforcement",
        "learning"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "sequence-labeling",
      "name": "Sequence Labeling",
      "parentId": "structured-prediction",
      "sectionId": "15.1",
      "level": 1,
      "fullExplanation": "Sequence Labeling covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about sequence labeling and related techniques.",
      "example": {
        "description": "This section contains concepts related to sequence labeling."
      },
      "tags": [
        "sequence",
        "structured",
        "labeling"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "sequence-to-sequence",
      "name": "Sequence-to-Sequence",
      "parentId": "structured-prediction",
      "sectionId": "15.2",
      "level": 1,
      "fullExplanation": "Sequence-to-Sequence covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about sequence-to-sequence and related techniques.",
      "example": {
        "description": "This section contains concepts related to sequence-to-sequence."
      },
      "tags": [
        "sequence",
        "structured"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "structured-output",
      "name": "Structured Output",
      "parentId": "structured-prediction",
      "sectionId": "15.3",
      "level": 1,
      "fullExplanation": "Structured Output covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about structured output and related techniques.",
      "example": {
        "description": "This section contains concepts related to structured output."
      },
      "tags": [
        "sequence",
        "structured",
        "output"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "domain-adaptation",
      "name": "Domain Adaptation",
      "parentId": "transfer-learning",
      "sectionId": "14.1.2",
      "level": 2,
      "fullExplanation": "Domain adaptation handles distribution shift between source and target domains. Source: labeled training data. Target: different distribution (domain shift), often unlabeled. Techniques: feature alignment (minimize domain discrepancy), adversarial training (domain-invariant features), self-training on target. Examples: synthetic-to-real, lab-to-production, cross-dataset transfer.",
      "simpleExplanation": "Make models work when training and deployment data look different. Trained on photos, deployed on sketches. Trained in lab conditions, deployed in the real world. Adapt the model to handle these differences.",
      "example": {
        "description": "",
        "code": "Synthetic to Real adaptation:\n\nSource domain: Simulated car images (cheap to generate)\nTarget domain: Real car photos (expensive to label)\n\nProblem:\nModel trained on simulation\nPerforms poorly on real images (domain gap)\n\nSolution - Domain Adversarial:\n1. Feature extractor (shared)\n2. Task classifier: \"Is this a car?\"\n3. Domain classifier: \"Is this simulated or real?\"\n\nTrain to:\n- Maximize task accuracy\n- Confuse domain classifier (domain-invariant features)\n\nResult: Features work for both domains!",
        "codeLanguage": "python"
      },
      "tags": [
        "transfer-learning",
        "reinforcement",
        "domain",
        "adaptation"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "zero-shot-learning",
      "name": "Zero-Shot Learning",
      "parentId": "transfer-learning",
      "sectionId": "14.1.3",
      "level": 2,
      "fullExplanation": "Zero-shot learning classifies instances of classes never seen during training. Relies on auxiliary information connecting seen and unseen classes: attributes (\"has stripes,\" \"is large\"), text descriptions, or semantic embeddings. Model learns to map images to semantic space where similar classes cluster. At test time, matches to unseen class descriptions. CLIP enables zero-shot via image-text alignment.",
      "simpleExplanation": "Recognize things you've never seen examples of. Trained on cats and dogs. Can classify \"zebra\" by knowing it's \"horse-like with stripes.\" Uses descriptions or attributes to bridge to new classes.",
      "example": {
        "description": "",
        "code": "Zero-shot animal classification:\n\nTraining classes: cat, dog, elephant\nUnseen class: zebra\n\nAttribute-based:\nZebra = [has_stripes: 1, is_large: 0.5, has_hooves: 1, ...]\n\nModel learns:\nImage → attribute predictions\n[stripes: 0.9, large: 0.4, hooves: 0.95]\n\nMatch to class attributes:\nClosest to zebra attributes → Classify as zebra!\n\nCLIP-style:\nImage embedding ≈ Text embedding(\"a photo of a zebra\")\n→ Zero-shot without explicit attributes",
        "codeLanguage": "python"
      },
      "tags": [
        "transfer-learning",
        "reinforcement",
        "zero",
        "shot",
        "learning"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "few-shot-learning",
      "name": "Few-Shot Learning",
      "parentId": "transfer-learning",
      "sectionId": "14.1.4",
      "level": 2,
      "fullExplanation": "Few-shot learning trains models to learn new classes from very few examples (1-5 per class). Meta-learning approach: train on many tasks, each with few examples, to learn how to learn. Methods: Prototypical Networks (class prototypes from examples), MAML (learn good initialization), Matching Networks (attention over support set). Critical for personalization, rare categories, rapid adaptation.",
      "simpleExplanation": "Learn from just a few examples. Humans can recognize a new animal from one picture. Few-shot learning teaches models this ability. Show 5 examples of a new class, and the model can classify it.",
      "example": {
        "description": "",
        "code": "5-shot, 5-way classification:\n\nSupport set:\nClass A: [img1, img2, img3, img4, img5]\nClass B: [img1, img2, img3, img4, img5]\n...\nClass E: [img1, img2, img3, img4, img5]\n\nQuery: New image → Which class?\n\nPrototypical Network:\n1. Embed all support images\n2. Compute class prototype (mean embedding)\n   Prototype_A = mean(embed(A_images))\n3. Embed query image\n4. Find nearest prototype\n   → Classify!\n\nNo retraining needed for new classes.",
        "codeLanguage": "python"
      },
      "tags": [
        "transfer-learning",
        "reinforcement",
        "shot",
        "learning"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "reinforcement-learning",
      "name": "Reinforcement Learning",
      "parentId": "reinforcement-learning",
      "sectionId": "14.2.1",
      "level": 2,
      "fullExplanation": "RL learns through interaction with an environment to maximize cumulative reward. Agent observes state, takes action, receives reward, transitions to new state. Policy π(a|s) maps states to actions. Value function V(s) estimates future reward from state. Key challenges: exploration vs exploitation, credit assignment, sample efficiency. Algorithms: Q-learning, Policy Gradient, Actor-Critic.",
      "simpleExplanation": "Learn by trial and error. Take actions, get rewards (or punishments), learn what works. Like training a dog with treats. No labeled examples—just try things and learn from outcomes.",
      "example": {
        "description": "",
        "code": "RL for game playing:\n\nState: Current game screen\nActions: Up, Down, Left, Right, Jump\nReward: +1 for points, -1 for losing life\n\nLearning loop:\n1. See game state\n2. Choose action (e.g., Jump)\n3. Game updates\n4. Receive reward (+10 for collecting coin)\n5. See new state\n6. Update policy: \"Jump was good in that situation\"\n7. Repeat millions of times\n\nResult: Agent masters the game!",
        "codeLanguage": "python"
      },
      "tags": [
        "transfer-learning",
        "reinforcement",
        "learning"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "mdp",
      "name": "Markov Decision Process (MDP)",
      "parentId": "reinforcement-learning",
      "sectionId": "14.2.2",
      "level": 2,
      "fullExplanation": "MDP formalizes RL problems with tuple (S, A, P, R, γ). S: state space. A: action space. P(s'|s,a): transition probability. R(s,a): reward function. γ: discount factor for future rewards. Markov property: future depends only on current state, not history. Optimal policy maximizes expected discounted return: E[Σγᵗrₜ]. Bellman equations describe optimal values recursively.",
      "simpleExplanation": "The math framework for RL. States you can be in, actions you can take, probabilities of what happens next, and rewards you get. The goal: find the best strategy (policy) to maximize total rewards.",
      "example": {
        "description": "",
        "code": "MDP for robot navigation:\n\nStates: Grid positions (x, y)\nActions: North, South, East, West\nTransitions: Move in direction (80% success, 10% left, 10% right)\nRewards: +100 at goal, -1 per step, -50 for falling off\nDiscount: γ = 0.99\n\nBellman equation:\nV*(s) = max_a [R(s,a) + γ Σ P(s'|s,a) V*(s')]\n\n\"Value of state = best action's immediate reward +\n discounted expected value of next states\"",
        "codeLanguage": "python"
      },
      "tags": [
        "transfer-learning",
        "reinforcement",
        "markov",
        "decision",
        "process"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "q-learning",
      "name": "Q-Learning",
      "parentId": "reinforcement-learning",
      "sectionId": "14.2.3",
      "level": 2,
      "fullExplanation": "Q-learning learns action-value function Q(s,a) = expected return from taking action a in state s, then following optimal policy. Update rule: Q(s,a) ← Q(s,a) + α[r + γ max_a' Q(s',a') - Q(s,a)]. Off-policy: learns optimal Q even from exploratory behavior. Tabular version requires discrete states; Deep Q-Network (DQN) uses neural network for continuous/large state spaces.",
      "simpleExplanation": "Learn the value of each action in each situation. \"In this state, going left gives +10 future reward, going right gives +5.\" Pick actions with highest Q-values. Learn from any experience, not just optimal behavior.",
      "example": {
        "description": "",
        "code": "Q-table for simple grid:\n\n           Up    Down   Left   Right\nState A:  10.5   5.2    3.1    8.4\nState B:   7.3   9.1    2.0    6.5\nState C:   ...\n\nLearning update:\nIn state A, took \"Up\", got reward 5, landed in state B\n\nOld Q(A, Up) = 10.5\nNew estimate = 5 + 0.9 × max(Q(B)) = 5 + 0.9 × 9.1 = 13.19\nUpdated Q(A, Up) = 10.5 + 0.1 × (13.19 - 10.5) = 10.77",
        "codeLanguage": "python"
      },
      "tags": [
        "transfer-learning",
        "reinforcement",
        "learning"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "policy-gradient",
      "name": "Policy Gradient",
      "parentId": "reinforcement-learning",
      "sectionId": "14.2.4",
      "level": 2,
      "fullExplanation": "Policy gradient directly optimizes the policy πθ(a|s) by gradient ascent on expected reward. REINFORCE: ∇J(θ) = E[∇log πθ(a|s) × R]. Increase probability of actions that led to high rewards. Works with continuous actions (unlike Q-learning). High variance—addressed by baseline subtraction (A = R - V(s)), actor-critic methods. Foundation for PPO, A3C, and modern RL.",
      "simpleExplanation": "Directly improve the policy (decision-making rules). If an action led to good outcomes, make it more likely. If it led to bad outcomes, make it less likely. Adjusts probabilities based on results.",
      "example": {
        "description": "",
        "code": "Policy gradient for robot arm:\n\nPolicy: Neural network\nInput: Joint angles, target position\nOutput: Probability distribution over motor commands\n\nEpisode 1:\nAction: Move left\nReward: -10 (moved away from target)\nUpdate: Decrease P(move left | this situation)\n\nEpisode 2:\nAction: Move right\nReward: +50 (reached target!)\nUpdate: Increase P(move right | this situation)\n\nAfter many episodes:\nPolicy learns to move efficiently to targets.",
        "codeLanguage": "python"
      },
      "tags": [
        "transfer-learning",
        "reinforcement",
        "policy",
        "gradient"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "actor-critic",
      "name": "Actor-Critic",
      "parentId": "reinforcement-learning",
      "sectionId": "14.2.5",
      "level": 2,
      "fullExplanation": "Actor-Critic combines policy gradient (actor) with value function (critic). Actor: policy πθ(a|s) selects actions. Critic: value function Vφ(s) estimates expected returns, provides baseline for variance reduction. Advantage: A(s,a) = Q(s,a) - V(s) = r + γV(s') - V(s). Actor updated with advantage-weighted policy gradient; critic updated to minimize TD error. More stable than pure policy gradient.",
      "simpleExplanation": "Two networks working together. Actor decides what to do. Critic evaluates how good the situation is. Critic's feedback helps Actor learn faster and more stably. Like having both a player and a coach.",
      "example": {
        "description": "",
        "code": "Actor-Critic for game:\n\nActor network:\nState → Action probabilities\n\"In this state, 70% attack, 30% defend\"\n\nCritic network:\nState → Value estimate\n\"This state is worth +25 expected future reward\"\n\nTraining step:\n1. Actor chooses: Attack\n2. Game gives: +10 reward, new state\n3. Critic evaluates: New state worth +20\n4. TD error = 10 + γ×20 - 25 = +3 (better than expected!)\n5. Update actor: \"Attack was good here, do more\"\n6. Update critic: \"Adjust value estimate\"",
        "codeLanguage": "python"
      },
      "tags": [
        "transfer-learning",
        "reinforcement",
        "actor",
        "critic"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "contrastive-learning",
      "name": "Contrastive Learning",
      "parentId": "self-supervised-learning",
      "sectionId": "14.3.2",
      "level": 2,
      "fullExplanation": "Contrastive learning learns representations by pulling similar samples together and pushing dissimilar samples apart in embedding space. Positives: augmentations of the same image. Negatives: other images in batch. Loss (InfoNCE): maximize similarity of positive pairs relative to negatives. SimCLR, MoCo, CLIP use this. Requires large batch sizes or memory banks for sufficient negatives.",
      "simpleExplanation": "Learn by comparison. \"These two augmented versions of the same image should have similar embeddings. This other image should be different.\" Pushes and pulls in embedding space until similar things cluster together.",
      "example": {
        "description": "",
        "code": "SimCLR contrastive learning:\n\nBatch of N images\nEach image augmented twice: 2N views\n\nFor image i:\nPositive pair: (augment1_i, augment2_i)\nNegative pairs: All other 2(N-1) views\n\nLoss for image i:\n-log(exp(sim(z_i, z_i+)/τ) / Σ exp(sim(z_i, z_k)/τ))\n\n\"Maximize similarity with positive, minimize with negatives\"\n\nResult: Images of cats cluster together,\n        images of dogs cluster separately.",
        "codeLanguage": "python"
      },
      "tags": [
        "transfer-learning",
        "reinforcement",
        "contrastive",
        "learning"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "masked-language-modeling",
      "name": "Masked Language Modeling",
      "parentId": "self-supervised-learning",
      "sectionId": "14.3.3",
      "level": 2,
      "fullExplanation": "MLM masks random tokens in input and trains model to predict them. BERT masks 15% of tokens: 80% replaced with [MASK], 10% random token, 10% unchanged. Forces model to understand context from both directions. Pretraining objective for bidirectional transformers. Creates powerful text encoders for downstream tasks. Differs from autoregressive LM which only sees past context.",
      "simpleExplanation": "Hide some words, guess them from context. \"The ___ chased the mouse\" → \"cat\". Both left and right context help. After seeing millions of sentences, the model deeply understands language.",
      "example": {
        "description": "",
        "code": "Masked Language Modeling:\n\nOriginal: \"The quick brown fox jumps over the lazy dog\"\nMasked:   \"The quick brown [MASK] jumps over the lazy dog\"\nTask:     Predict [MASK] = \"fox\"\n\nMulti-mask:\nInput:  \"The [MASK] brown fox [MASK] over the [MASK] dog\"\nTarget: \"The quick brown fox jumps over the lazy dog\"\n\nModel learns:\n- Grammar: \"jumps\" (verb after subject)\n- Semantics: \"fox\" (animal that jumps)\n- Common phrases: \"lazy dog\"",
        "codeLanguage": "python"
      },
      "tags": [
        "transfer-learning",
        "reinforcement",
        "masked",
        "language",
        "modeling"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "sequence-labeling",
      "name": "Sequence Labeling",
      "parentId": "sequence-labeling",
      "sectionId": "15.1.1",
      "level": 2,
      "fullExplanation": "Sequence labeling assigns a label to each element in a sequence. Input: sequence x = (x₁, ..., xₙ). Output: labels y = (y₁, ..., yₙ). Tasks: POS tagging (word → part of speech), NER (word → entity type), chunking. Models must capture dependencies between labels. Approaches: HMM, CRF, BiLSTM-CRF, Transformer + CRF.",
      "simpleExplanation": "Label each item in a sequence. For each word: is it a noun, verb, adjective? Is it a person name, location, or organization? Each position gets its own label.",
      "example": {
        "description": "",
        "code": "Named Entity Recognition:\n\nInput:  \"John works at Google in California\"\nLabels: \"B-PER O O B-ORG O B-LOC\"\n\nB-PER = Beginning of Person name\nB-ORG = Beginning of Organization\nB-LOC = Beginning of Location\nO = Outside (not an entity)\n\nPart-of-Speech tagging:\nInput:  \"The cat sat on the mat\"\nLabels: \"DET NOUN VERB PREP DET NOUN\"",
        "codeLanguage": "python"
      },
      "tags": [
        "sequence",
        "structured",
        "labeling"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "crf",
      "name": "Conditional Random Field (CRF)",
      "parentId": "sequence-labeling",
      "sectionId": "15.1.2",
      "level": 2,
      "fullExplanation": "CRF models P(y|x) directly, considering the entire output sequence. Linear-chain CRF: P(y|x) ∝ exp(Σ score(yᵢ₋₁, yᵢ, x, i)). Scores include transition features (label-to-label) and emission features (input-to-label). Viterbi algorithm finds optimal sequence; forward-backward computes marginals. Unlike HMM, CRF is discriminative and handles overlapping features. Often combined with neural networks (BiLSTM-CRF).",
      "simpleExplanation": "Model the whole label sequence, not just individual labels. \"B-PER should be followed by I-PER, not I-ORG.\" Learns label transition patterns. Predicts the globally best label sequence.",
      "example": {
        "description": "",
        "code": "CRF for NER:\n\nEmission scores (from neural network):\n           O    B-PER  I-PER  B-LOC\n\"John\"   -2.0   5.0    0.1   -1.0\n\"works\"   4.0  -1.0   -2.0   -1.0\n\"at\"      5.0  -2.0   -2.0   -1.0\n\nTransition scores:\n         O    B-PER  I-PER  B-LOC\nO        1.0   0.5   -5.0    0.5\nB-PER    0.5  -1.0    3.0   -1.0  ← B-PER → I-PER likely\nI-PER    0.8  -1.0    2.0   -1.0\nB-LOC    1.0   0.5   -5.0   -1.0\n\nViterbi finds best path considering both!",
        "codeLanguage": "python"
      },
      "tags": [
        "sequence",
        "structured",
        "conditional",
        "random",
        "field"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "ner",
      "name": "Named Entity Recognition (NER)",
      "parentId": "sequence-labeling",
      "sectionId": "15.1.3",
      "level": 2,
      "fullExplanation": "NER identifies and classifies named entities in text into predefined categories: Person, Organization, Location, Date, etc. BIO tagging: B-type (beginning), I-type (inside), O (outside). Challenges: ambiguity (\"Apple\" = company or fruit), nested entities, rare entities. Modern approaches: fine-tuned BERT + CRF layer. Key for information extraction, question answering, knowledge graphs.",
      "simpleExplanation": "Find names of people, places, companies, dates in text. \"Apple announced new products\" → Apple is B-ORG (organization). Crucial for extracting structured information from unstructured text.",
      "example": {
        "description": "",
        "code": "NER example:\n\nText: \"Elon Musk founded SpaceX in California in 2002\"\n\nEntities found:\n- Elon Musk: PERSON\n- SpaceX: ORGANIZATION\n- California: LOCATION\n- 2002: DATE\n\nBIO format:\nElon    → B-PER\nMusk    → I-PER\nfounded → O\nSpaceX  → B-ORG\nin      → O\nCalifornia → B-LOC\nin      → O\n2002    → B-DATE",
        "codeLanguage": "python"
      },
      "tags": [
        "sequence",
        "structured",
        "named",
        "entity",
        "recognition"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "seq2seq",
      "name": "Sequence-to-Sequence (Seq2Seq)",
      "parentId": "sequence-to-sequence",
      "sectionId": "15.2.1",
      "level": 2,
      "fullExplanation": "Seq2Seq maps variable-length input sequences to variable-length output sequences. Encoder-decoder architecture: encoder compresses input to fixed representation (or sequence of representations with attention), decoder generates output token by token. Applications: machine translation, summarization, dialogue, code generation. Original: LSTM encoder-decoder. Modern: Transformer-based.",
      "simpleExplanation": "Transform one sequence into another. Input sentence in English → output sentence in French. Input document → output summary. The input and output can be different lengths.",
      "example": {
        "description": "",
        "code": "Machine translation:\n\nInput:  \"I love machine learning\"\nOutput: \"J'aime l'apprentissage automatique\"\n\nEncoder processes: \"I love machine learning\"\n→ Context representation\n\nDecoder generates:\nStep 1: [START] → \"J'\"\nStep 2: \"J'\" → \"aime\"\nStep 3: \"J'aime\" → \"l'\"\nStep 4: \"J'aime l'\" → \"apprentissage\"\nStep 5: → \"automatique\"\nStep 6: → [END]\n\nWith attention: decoder focuses on relevant source words\n\"aime\" attends to \"love\"",
        "codeLanguage": "python"
      },
      "tags": [
        "sequence",
        "structured",
        "(seq2seq)"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "beam-search",
      "name": "Beam Search",
      "parentId": "sequence-to-sequence",
      "sectionId": "15.2.3",
      "level": 2,
      "fullExplanation": "Beam search is a decoding algorithm that maintains the k most promising partial sequences (beam width k). At each step, expand each beam by all possible next tokens, score, keep top k. Balances exploration (multiple hypotheses) and exploitation (focus on likely). Better than greedy (k=1) which may miss globally optimal sequences. Larger k = better quality but slower. Length normalization handles length bias.",
      "simpleExplanation": "Keep track of several promising paths, not just the best one. At each step, extend all paths, keep the best few. Like exploring a maze by following multiple routes simultaneously. Usually finds better solutions than greedy search.",
      "example": {
        "description": "",
        "code": "Beam search with k=2:\n\nStep 1:\n\"The\" → P=0.4\n\"A\"   → P=0.3\nKeep: [\"The\": 0.4, \"A\": 0.3]\n\nStep 2:\n\"The cat\" → 0.4 × 0.3 = 0.12\n\"The dog\" → 0.4 × 0.5 = 0.20 ✓\n\"A cat\"   → 0.3 × 0.4 = 0.12\n\"A dog\"   → 0.3 × 0.3 = 0.09\nKeep: [\"The dog\": 0.20, \"The cat\": 0.12]\n\nStep 3:\nContinue expanding best 2...\n\nvs. Greedy:\nStep 1: \"The\" (best)\nStep 2: \"The dog\" (best)\nSame result here, but beam often finds better sequences!",
        "codeLanguage": "python"
      },
      "tags": [
        "sequence",
        "structured",
        "beam",
        "search"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "structured-output",
      "name": "Structured Output Prediction",
      "parentId": "structured-output",
      "sectionId": "15.3.1",
      "level": 2,
      "fullExplanation": "Structured output prediction produces complex, interdependent outputs rather than single labels. Examples: parse trees, graphs, alignments, segmentations. Output space is exponentially large; specialized algorithms needed for inference. Methods: dynamic programming (parsing), graph algorithms, iterative refinement, neural set prediction. Loss functions must handle structure (tree kernels, edit distance).",
      "simpleExplanation": "Predict complex outputs like trees, graphs, or structured documents. Not just one label but many interconnected pieces. The structure between output pieces matters—can't just predict each piece independently.",
      "example": {
        "description": "",
        "code": "Structured outputs:\n\n1. Dependency Parse Tree:\nInput: \"The cat sat on the mat\"\nOutput:    sat\n          / | \\\n       cat  on  .\n       /     |\n     The    mat\n              |\n             the\n\n2. Scene Graph:\nInput: Image of dog on couch\nOutput:\n  dog --sitting_on--> couch\n   |\n  --has_color--> brown\n\n3. Document Layout:\nInput: PDF page\nOutput: {title: rect1, paragraph: [rect2,rect3], figure: rect4}",
        "codeLanguage": "python"
      },
      "tags": [
        "sequence",
        "structured",
        "output",
        "prediction"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "gnn",
      "name": "Graph Neural Network (GNN)",
      "parentId": "structured-output",
      "sectionId": "15.3.2",
      "level": 2,
      "fullExplanation": "GNNs learn on graph-structured data by aggregating information from neighboring nodes. Message passing: each node collects messages from neighbors, updates its representation. After K layers, each node has K-hop neighborhood information. Types: GCN (spectral), GraphSAGE (sampling), GAT (attention-based). Applications: social networks, molecules, knowledge graphs, recommendation, program analysis.",
      "simpleExplanation": "Neural networks that work on graphs. Each node looks at its neighbors to update its representation. After several rounds, each node knows about its extended neighborhood. Perfect for social networks, molecules, and any connected data.",
      "example": {
        "description": "",
        "code": "GNN for molecular property prediction:\n\nMolecule as graph:\nAtoms = nodes (with features: element type, charge)\nBonds = edges (with features: bond type)\n\nMessage passing:\nRound 1: Each atom aggregates from direct neighbors\nRound 2: Each atom aggregates from 2-hop neighborhood\nRound 3: Each atom knows 3-hop context\n\n        O\n        ‖\n    H-C-C-OH\n        |\n        H\n\nAfter 3 rounds:\nCarbon atom knows about: attached H, attached O, the OH group\n\nFinal:\nAggregate all atom representations → Molecule representation\n→ Predict: Solubility = 2.3",
        "codeLanguage": "python"
      },
      "tags": [
        "sequence",
        "structured",
        "graph",
        "neural",
        "network"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "intermediate"
    },
    {
      "id": "image-classification",
      "name": "Image Classification",
      "parentId": "computer-vision",
      "sectionId": "16.1",
      "level": 1,
      "fullExplanation": "Image Classification covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about image classification and related techniques.",
      "example": {
        "description": "This section contains concepts related to image classification."
      },
      "tags": [
        "vision",
        "images",
        "image",
        "classification"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "object-detection",
      "name": "Object Detection",
      "parentId": "computer-vision",
      "sectionId": "16.2",
      "level": 1,
      "fullExplanation": "Object Detection covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about object detection and related techniques.",
      "example": {
        "description": "This section contains concepts related to object detection."
      },
      "tags": [
        "vision",
        "images",
        "object",
        "detection"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "image-segmentation",
      "name": "Image Segmentation",
      "parentId": "computer-vision",
      "sectionId": "16.3",
      "level": 1,
      "fullExplanation": "Image Segmentation covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about image segmentation and related techniques.",
      "example": {
        "description": "This section contains concepts related to image segmentation."
      },
      "tags": [
        "vision",
        "images",
        "image",
        "segmentation"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "face-recognition",
      "name": "Face Recognition",
      "parentId": "computer-vision",
      "sectionId": "16.4",
      "level": 1,
      "fullExplanation": "Face Recognition covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about face recognition and related techniques.",
      "example": {
        "description": "This section contains concepts related to face recognition."
      },
      "tags": [
        "vision",
        "images",
        "face",
        "recognition"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "text-preprocessing",
      "name": "Text Preprocessing",
      "parentId": "nlp",
      "sectionId": "17.1",
      "level": 1,
      "fullExplanation": "Text Preprocessing covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about text preprocessing and related techniques.",
      "example": {
        "description": "This section contains concepts related to text preprocessing."
      },
      "tags": [
        "nlp",
        "text",
        "preprocessing"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "language-understanding",
      "name": "Language Understanding",
      "parentId": "nlp",
      "sectionId": "17.2",
      "level": 1,
      "fullExplanation": "Language Understanding covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about language understanding and related techniques.",
      "example": {
        "description": "This section contains concepts related to language understanding."
      },
      "tags": [
        "nlp",
        "text",
        "language",
        "understanding"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "text-generation",
      "name": "Text Generation",
      "parentId": "nlp",
      "sectionId": "17.3",
      "level": 1,
      "fullExplanation": "Text Generation covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about text generation and related techniques.",
      "example": {
        "description": "This section contains concepts related to text generation."
      },
      "tags": [
        "nlp",
        "text",
        "generation"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "semantic-understanding",
      "name": "Semantic Understanding",
      "parentId": "nlp",
      "sectionId": "17.4",
      "level": 1,
      "fullExplanation": "Semantic Understanding covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about semantic understanding and related techniques.",
      "example": {
        "description": "This section contains concepts related to semantic understanding."
      },
      "tags": [
        "nlp",
        "text",
        "semantic",
        "understanding"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "image-classification",
      "name": "Image Classification",
      "parentId": "image-classification",
      "sectionId": "16.1.1",
      "level": 2,
      "fullExplanation": "Image classification assigns a single label to an entire image from a predefined set of categories. Input: image (H×W×C). Output: class probabilities. Pipeline: CNN extracts features, fully connected layers classify. Key benchmarks: ImageNet (1000 classes), CIFAR-10/100. Architectures evolved from AlexNet through VGG, ResNet, to modern EfficientNet and Vision Transformers.",
      "simpleExplanation": "Look at an image, say what it is. Is it a cat, dog, or car? The model outputs probabilities for each possible class. The class with highest probability is the prediction.",
      "example": {
        "description": "",
        "code": "Image classification pipeline:\n\nInput: 224×224×3 image of a cat\n\nCNN Feature Extraction:\nConv layers → 7×7×512 feature map\nGlobal pooling → 512-dim vector\n\nClassification:\n512 → 1000 (ImageNet classes)\nSoftmax → Probabilities\n\nOutput:\nCat: 0.85, Dog: 0.10, Tiger: 0.03, ...\nPrediction: Cat",
        "codeLanguage": "python"
      },
      "tags": [
        "vision",
        "images",
        "image",
        "classification"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "resnet",
      "name": "ResNet",
      "parentId": "image-classification",
      "sectionId": "16.1.2",
      "level": 2,
      "fullExplanation": "ResNet (Residual Networks) introduced skip connections enabling training of very deep networks (50-152+ layers). Instead of learning H(x), learn residual F(x) = H(x) - x, so output is F(x) + x. If optimal is identity, F(x) = 0 is easy to learn. Solves vanishing gradient problem by providing gradient highways. Variants: ResNet-50, ResNet-101, ResNeXt, ResNeSt.",
      "simpleExplanation": "Add shortcuts that skip layers. Instead of transforming everything, just add small changes to the original. Allows training of very deep networks—gradients can flow through shortcuts. The breakthrough that enabled 100+ layer networks.",
      "example": {
        "description": "",
        "code": "ResNet residual block:\n\nInput x\n    ↓\n[Conv → BN → ReLU]\n    ↓\n[Conv → BN]\n    ↓\n  + x ← Skip connection!\n    ↓\n ReLU\n    ↓\nOutput\n\nIf optimal is identity (no change needed):\nWeights learn to be zero → Output = 0 + x = x\n\nDeep network can \"skip\" unnecessary layers!",
        "codeLanguage": "python"
      },
      "tags": [
        "vision",
        "images",
        "resnet"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "vit",
      "name": "Vision Transformer (ViT)",
      "parentId": "image-classification",
      "sectionId": "16.1.3",
      "level": 2,
      "fullExplanation": "ViT applies transformer architecture to images by treating images as sequences of patches. Split image into 16×16 patches, linearly embed each patch, add positional embeddings, process with standard transformer encoder. Outperforms CNNs when pretrained on large datasets (ImageNet-21k, JFT-300M). Less inductive bias (no convolutions), requires more data. Foundation for modern vision models.",
      "simpleExplanation": "Use the same attention mechanism as language models, but for images. Cut image into patches (like words), process with transformer. No convolutions! Works great when you have lots of training data.",
      "example": {
        "description": "",
        "code": "ViT processing:\n\nImage: 224×224×3\n\n1. Split into patches:\n   14×14 grid of 16×16 patches = 196 patches\n\n2. Embed patches:\n   Each 16×16×3 patch → 768-dim vector\n   + Positional embedding\n   + [CLS] token prepended\n\n3. Transformer:\n   197 tokens × 768 dims\n   12 transformer layers\n   Self-attention across all patches\n\n4. Classification:\n   [CLS] token → MLP → Class prediction\n\nKey insight: Patches are like words!",
        "codeLanguage": "python"
      },
      "tags": [
        "vision",
        "images",
        "transformer",
        "(vit)"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "object-detection",
      "name": "Object Detection",
      "parentId": "object-detection",
      "sectionId": "16.2.1",
      "level": 2,
      "fullExplanation": "Object detection locates and classifies multiple objects in images. Output: bounding boxes (x, y, width, height) + class labels + confidence scores. Two-stage detectors (R-CNN family): propose regions, then classify. One-stage detectors (YOLO, SSD): directly predict boxes and classes. Metrics: mAP (mean Average Precision), IoU (Intersection over Union). Applications: autonomous driving, surveillance, retail.",
      "simpleExplanation": "Find all objects in an image and draw boxes around them. For each box, say what's inside and how confident you are. Unlike classification (one label for whole image), detection finds multiple objects at specific locations.",
      "example": {
        "description": "```\nObject detection output:\n\nImage: Street scene\n\nDetections:\n[Box1] Car,    confidence=0.95, bbox=[100,200,150,100]\n[Box2] Person, confidence=0.88, bbox=[300,150,50,120]\n[Box3] Car,    confidence=0.72, bbox=[450,210,140,90]\n[Box4] Dog,    confidence=0.65, bbox=[320,300,40,35]\n\nVisual:\n+"
      },
      "tags": [
        "vision",
        "images",
        "object",
        "detection"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "yolo",
      "name": "YOLO (You Only Look Once)",
      "parentId": "object-detection",
      "sectionId": "16.2.2",
      "level": 2,
      "fullExplanation": "YOLO treats detection as a single regression problem, predicting bounding boxes and class probabilities directly from full images in one pass. Divides image into grid; each cell predicts boxes and classes for objects whose center falls in that cell. Extremely fast (real-time), trades some accuracy for speed. Versions: YOLOv1-v8, each improving accuracy while maintaining speed.",
      "simpleExplanation": "Look at the whole image once, predict all boxes simultaneously. Super fast—can process video in real-time. Grid-based: each grid cell detects objects centered there. The go-to for real-time detection.",
      "example": {
        "description": "",
        "code": "YOLO processing:\n\n1. Divide image into 7×7 grid\n\n2. Each cell predicts:\n   - 2 bounding boxes (x, y, w, h, confidence)\n   - 20 class probabilities (for PASCAL VOC)\n\n3. Output tensor: 7×7×30\n   (7×7 grid × (2 boxes × 5 values + 20 classes))\n\n4. Post-processing:\n   - Filter low-confidence boxes\n   - Non-maximum suppression (remove overlaps)\n\nSpeed: 45-155 FPS depending on version\nReal-time on video! 🎥",
        "codeLanguage": "python"
      },
      "tags": [
        "vision",
        "images",
        "yolo",
        "(you",
        "only"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "nms",
      "name": "Non-Maximum Suppression (NMS)",
      "parentId": "object-detection",
      "sectionId": "16.2.3",
      "level": 2,
      "fullExplanation": "NMS removes duplicate detections for the same object. Multiple boxes often predict the same object; NMS keeps only the best. Algorithm: (1) Select highest confidence box, (2) Remove all boxes with IoU > threshold with selected box, (3) Repeat until no boxes remain. Threshold typically 0.5. Variants: Soft-NMS (decay scores instead of removing), learned NMS.",
      "simpleExplanation": "Remove duplicate boxes. If many boxes found the same car, keep only the best one. For each object, one box should remain. Compare overlap (IoU) between boxes; if too similar, remove the weaker one.",
      "example": {
        "description": "",
        "code": "NMS example:\n\nBefore NMS:\nBox A: Car, conf=0.95, [100,200,150,100]\nBox B: Car, conf=0.88, [105,195,145,95]  ← Overlaps A\nBox C: Car, conf=0.72, [400,200,150,100]\nBox D: Car, conf=0.45, [102,198,148,98]  ← Overlaps A\n\nIoU(A,B) = 0.85 (high overlap)\nIoU(A,C) = 0.01 (different objects)\nIoU(A,D) = 0.80 (high overlap)\n\nNMS with threshold 0.5:\n1. Select A (highest: 0.95)\n2. Remove B (IoU 0.85 > 0.5) ✗\n3. Keep C (IoU 0.01 < 0.5) ✓\n4. Remove D (IoU 0.80 > 0.5) ✗\n\nAfter NMS: Box A, Box C (one per object)",
        "codeLanguage": "python"
      },
      "tags": [
        "vision",
        "images",
        "maximum",
        "suppression",
        "(nms)"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "image-segmentation",
      "name": "Image Segmentation",
      "parentId": "image-segmentation",
      "sectionId": "16.3.1",
      "level": 2,
      "fullExplanation": "Image segmentation assigns labels to each pixel. Semantic segmentation: label per pixel, no instance distinction (all cats are \"cat\"). Instance segmentation: separate different instances (cat1, cat2). Panoptic: combines both. Architectures: FCN (fully convolutional), U-Net (encoder-decoder with skip connections), DeepLab (atrous convolutions). Applications: medical imaging, autonomous driving, photo editing.",
      "simpleExplanation": "Label every pixel in the image. Which pixels are sky? Which are road? Which are cars? Like coloring a coloring book where each region has a specific category. Instance segmentation goes further—distinguishes different cars from each other.",
      "example": {
        "description": "",
        "code": "Segmentation types:\n\nImage: Two cats on a sofa\n\nSemantic segmentation:\nEvery pixel labeled as: cat/sofa/background\nAll cat pixels have same label \"cat\"\n\nInstance segmentation:\nEvery pixel labeled with instance:\ncat_1, cat_2, sofa_1, background\n\nPanoptic segmentation:\n\"Stuff\" (amorphous): sofa, background\n\"Things\" (countable): cat_1, cat_2\n\nOutput: H×W mask with pixel-wise labels",
        "codeLanguage": "python"
      },
      "tags": [
        "vision",
        "images",
        "image",
        "segmentation"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "u-net",
      "name": "U-Net",
      "parentId": "image-segmentation",
      "sectionId": "16.3.2",
      "level": 2,
      "fullExplanation": "U-Net is an encoder-decoder architecture with skip connections for precise segmentation. Encoder (contracting path): captures context through downsampling. Decoder (expanding path): enables precise localization through upsampling. Skip connections concatenate encoder features to decoder, preserving spatial details lost in downsampling. Originally for biomedical segmentation, now used broadly.",
      "simpleExplanation": "U-shaped network that compresses then expands. Shortcuts copy detailed information from early layers to later layers. Combines \"what\" (from deep features) with \"where\" (from skip connections). Excellent for medical images and precise boundaries.",
      "example": {
        "description": "",
        "code": "U-Net architecture:\n\nEncoder (left side):\nInput: 572×572×1\n↓ Conv, Conv, Pool → 284×284×64\n↓ Conv, Conv, Pool → 140×140×128\n↓ Conv, Conv, Pool → 68×68×256\n↓ Conv, Conv, Pool → 32×32×512\n↓ Conv, Conv → 28×28×1024 (bottleneck)\n\nDecoder (right side):\n↑ Up-conv → 52×52×512\n  + Skip from encoder 256-level\n↑ Up-conv → 100×100×256\n  + Skip from encoder 128-level\n↑ Up-conv → 196×196×128\n  + Skip from encoder 64-level\n↑ Up-conv → 388×388×64\n↓ 1×1 Conv → 388×388×2 (output)",
        "codeLanguage": "python"
      },
      "tags": [
        "vision",
        "images"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "face-recognition",
      "name": "Face Recognition",
      "parentId": "face-recognition",
      "sectionId": "16.4.1",
      "level": 2,
      "fullExplanation": "Face recognition identifies or verifies individuals from facial images. Verification (1:1): \"Is this the same person?\" Identification (1:N): \"Who is this person?\" Pipeline: detect face, align, extract embedding, compare. Embeddings learned with contrastive losses (triplet loss, ArcFace). Challenges: pose, lighting, occlusion, aging. Applications: phone unlock, access control, surveillance.",
      "simpleExplanation": "Recognize who someone is from their face. Create a \"face fingerprint\" (embedding) that's similar for the same person, different for different people. Compare embeddings to identify or verify.",
      "example": {
        "description": "",
        "code": "Face recognition pipeline:\n\n1. Detection: Find face in image\n   Crop: 112×112 face region\n\n2. Alignment: Normalize pose\n   Align eyes, nose to standard positions\n\n3. Embedding: Extract features\n   Face → CNN → 512-dim vector\n   [0.23, -0.15, 0.82, ...]\n\n4. Comparison:\n   Euclidean distance or cosine similarity\n\n   Same person: distance < 0.6\n   Different person: distance > 0.8\n\nVerification:\nembed(photo) vs embed(ID_card)\nDistance = 0.3 → Same person ✓",
        "codeLanguage": "python"
      },
      "tags": [
        "vision",
        "images",
        "face",
        "recognition"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "triplet-loss",
      "name": "Triplet Loss",
      "parentId": "face-recognition",
      "sectionId": "16.4.2",
      "level": 2,
      "fullExplanation": "Triplet loss learns embeddings where similar samples are closer than dissimilar ones. Triplet: anchor (a), positive (same class as anchor, p), negative (different class, n). Loss: max(0, ||a-p||² - ||a-n||² + margin). Pushes positive closer, negative farther. Hard negative mining: select challenging negatives. Foundation for face recognition (FaceNet), general metric learning.",
      "simpleExplanation": "Learn embeddings using triplets: anchor, same-person example, different-person example. Make anchor closer to same-person than to different-person. Like teaching: \"this matches, that doesn't.\"",
      "example": {
        "description": "",
        "code": "Triplet loss training:\n\nTriplet:\nAnchor: Photo of Alice (front)\nPositive: Photo of Alice (side) - same person\nNegative: Photo of Bob - different person\n\nGoal:\ndistance(Alice_front, Alice_side) + margin\n    < distance(Alice_front, Bob)\n\nBefore training:\nd(anchor, positive) = 1.5\nd(anchor, negative) = 1.2\nLoss = max(0, 1.5 - 1.2 + 0.3) = 0.6 ✗\n\nAfter training:\nd(anchor, positive) = 0.4\nd(anchor, negative) = 1.5\nLoss = max(0, 0.4 - 1.5 + 0.3) = 0 ✓",
        "codeLanguage": "python"
      },
      "tags": [
        "vision",
        "images",
        "triplet",
        "loss"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "tokenization",
      "name": "Tokenization",
      "parentId": "text-preprocessing",
      "sectionId": "17.1.1",
      "level": 2,
      "fullExplanation": "Tokenization splits text into tokens (words, subwords, or characters). Word tokenization: split on whitespace/punctuation. Subword tokenization: BPE, WordPiece, SentencePiece learn vocabulary balancing frequency and coverage. Subwords handle unknown words by breaking into known pieces. Character-level: maximum coverage, longer sequences. Choice affects vocabulary size, OOV handling, and sequence length.",
      "simpleExplanation": "Break text into pieces the model can process. Words, word parts, or characters. Modern models use subwords: common words stay whole, rare words split into pieces. \"unhappiness\" → \"un\", \"happiness\" or \"un\", \"happ\", \"iness\".",
      "example": {
        "description": "",
        "code": "Tokenization methods:\n\nText: \"Tokenization is awesome!\"\n\nWord tokenization:\n[\"Tokenization\", \"is\", \"awesome\", \"!\"]\n\nSubword (BPE/WordPiece):\n[\"Token\", \"ization\", \"is\", \"awesome\", \"!\"]\n\nCharacter:\n[\"T\",\"o\",\"k\",\"e\",\"n\",\"i\",\"z\",\"a\",\"t\",\"i\",\"o\",\"n\",\" \",\"i\",\"s\",...]\n\nGPT-style (with unknown word):\n\"cryptocurrency\" → [\"crypt\", \"ocur\", \"rency\"]\nUnknown word handled by splitting!",
        "codeLanguage": "python"
      },
      "tags": [
        "nlp",
        "text",
        "tokenization"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "word-embeddings",
      "name": "Word Embeddings",
      "parentId": "text-preprocessing",
      "sectionId": "17.1.2",
      "level": 2,
      "fullExplanation": "Word embeddings map words to dense vectors capturing semantic relationships. Similar words have similar vectors. Training: Word2Vec (predict context from word or vice versa), GloVe (matrix factorization of co-occurrence). Properties: king - man + woman ≈ queen. Static embeddings: one vector per word. Contextual embeddings (BERT): different vectors based on context.",
      "simpleExplanation": "Convert words to numbers that capture meaning. Similar words get similar numbers. \"King\" and \"queen\" are close. \"King - man + woman = queen\" works with vectors! The foundation of modern NLP.",
      "example": {
        "description": "",
        "code": "Word embedding properties:\n\nEmbedding dimensions: 300\n\nSemantic similarity:\ncosine(embed(\"cat\"), embed(\"dog\")) = 0.8 (high)\ncosine(embed(\"cat\"), embed(\"car\")) = 0.2 (low)\n\nAnalogies:\nking - man + woman ≈ queen\nParis - France + Japan ≈ Tokyo\n\nembed(\"king\") = [0.2, -0.4, 0.1, ...]\nembed(\"man\") = [0.3, -0.2, 0.2, ...]\nembed(\"woman\") = [0.4, -0.1, 0.3, ...]\n\nking - man + woman = [-0.1, -0.2, -0.1, ...] + [0.4, -0.1, 0.3, ...]\n                   = [0.3, -0.3, 0.2, ...] ≈ embed(\"queen\")",
        "codeLanguage": "python"
      },
      "tags": [
        "nlp",
        "text",
        "word",
        "embeddings"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "bert",
      "name": "BERT",
      "parentId": "language-understanding",
      "sectionId": "17.2.1",
      "level": 2,
      "fullExplanation": "BERT (Bidirectional Encoder Representations from Transformers) pretrained bidirectional transformer on masked language modeling and next sentence prediction. Sees full context (left and right) unlike GPT (left only). Fine-tune for downstream tasks: classification, NER, QA by adding task-specific heads. Revolutionized NLP benchmarks. Variants: RoBERTa (better training), ALBERT (parameter sharing), DistilBERT (smaller).",
      "simpleExplanation": "Powerful language understanding model. Reads text in both directions to understand context. Pretrained on massive text, fine-tuned for specific tasks. The word \"bank\" gets different meanings in \"river bank\" vs \"bank account.\"",
      "example": {
        "description": "",
        "code": "BERT for text classification:\n\n1. Pretrained BERT understands language\n\n2. Fine-tune for sentiment:\n   Input: \"[CLS] This movie was great! [SEP]\"\n\n   BERT processes:\n   - [CLS] token aggregates sentence meaning\n   - Bidirectional attention captures context\n\n   [CLS] embedding → Linear layer → Softmax\n\n   Output: [Negative: 0.02, Positive: 0.98]\n\n3. Few epochs of fine-tuning:\n   BERT already understands \"great\" is positive\n   Just needs to learn classification task",
        "codeLanguage": "python"
      },
      "tags": [
        "nlp",
        "text",
        "bert"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "text-classification",
      "name": "Text Classification",
      "parentId": "language-understanding",
      "sectionId": "17.2.2",
      "level": 2,
      "fullExplanation": "Text classification assigns labels to text documents. Binary (spam/not spam), multiclass (topic categories), or multilabel (multiple tags). Traditional: TF-IDF + classifier. Neural: CNN on word embeddings, RNN/LSTM, Transformers (BERT). For BERT: use [CLS] token representation with classification head. Data augmentation, class imbalance handling crucial. Applications: sentiment analysis, intent detection, content moderation.",
      "simpleExplanation": "Assign a category to text. Is this email spam? Is this review positive or negative? What topic is this article about? Modern approaches use pretrained models fine-tuned on labeled examples.",
      "example": {
        "description": "",
        "code": "Sentiment classification:\n\nInput texts:\n\"I love this product!\" → Positive\n\"Terrible experience, never again\" → Negative\n\"It's okay, nothing special\" → Neutral\n\nBERT approach:\n1. Tokenize: [\"[CLS]\", \"I\", \"love\", \"this\", \"product\", \"!\", \"[SEP]\"]\n2. BERT forward pass\n3. Take [CLS] embedding (768 dims)\n4. Linear: 768 → 3 (classes)\n5. Softmax → [0.01, 0.98, 0.01]\n6. Prediction: Positive\n\nAccuracy on benchmarks: 95%+ with BERT",
        "codeLanguage": "python"
      },
      "tags": [
        "nlp",
        "text",
        "classification"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "question-answering",
      "name": "Question Answering",
      "parentId": "language-understanding",
      "sectionId": "17.2.3",
      "level": 2,
      "fullExplanation": "Question answering extracts or generates answers from context. Extractive QA: identify answer span in given passage (start/end positions). Generative QA: generate answer text (seq2seq). Reading comprehension: answer from passage. Open-domain QA: retrieve relevant documents first, then answer. BERT for extractive QA: predict start and end token positions. Benchmarks: SQuAD, Natural Questions.",
      "simpleExplanation": "Find answers in text. Given a question and a passage, point to where the answer is. Or generate an answer based on the passage. Like finding info in a textbook.",
      "example": {
        "description": "",
        "code": "Extractive QA with BERT:\n\nContext: \"The Eiffel Tower is a wrought iron lattice tower\n         in Paris. It was built in 1889.\"\n\nQuestion: \"When was the Eiffel Tower built?\"\n\nBERT processing:\nInput: \"[CLS] When was the Eiffel Tower built? [SEP]\n       The Eiffel Tower is a wrought iron lattice tower\n       in Paris. It was built in 1889. [SEP]\"\n\nModel predicts:\nStart position: token 18 (\"1889\")\nEnd position: token 18 (\"1889\")\n\nAnswer: \"1889\"",
        "codeLanguage": "python"
      },
      "tags": [
        "nlp",
        "text",
        "question",
        "answering"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "gpt",
      "name": "GPT (Generative Pre-trained Transformer)",
      "parentId": "text-generation",
      "sectionId": "17.3.1",
      "level": 2,
      "fullExplanation": "GPT is a decoder-only transformer trained autoregressively (predict next token). Pretrained on massive text corpora, scales to billions/trillions of parameters. Unlike BERT (bidirectional encoder), GPT is unidirectional (causal), enabling text generation. GPT-3/4: few-shot learning via prompting. Foundation for ChatGPT, coding assistants, content generation. RLHF makes it follow instructions.",
      "simpleExplanation": "The model behind ChatGPT. Predicts the next word given previous words. Trained on internet text, learns grammar, facts, reasoning. Large enough models can follow instructions, answer questions, write code.",
      "example": {
        "description": "",
        "code": "GPT text generation:\n\nPrompt: \"The secret to happiness is\"\n\nGeneration (token by token):\n\"The secret to happiness is\" → \"finding\"\n\"The secret to happiness is finding\" → \"joy\"\n\"The secret to happiness is finding joy\" → \"in\"\n\"The secret to happiness is finding joy in\" → \"small\"\n\"The secret to happiness is finding joy in small\" → \"moments\"\n\nOutput: \"The secret to happiness is finding joy in small moments.\"\n\nEach step:\n- Feed all previous tokens\n- Model outputs probability for each vocab word\n- Sample or take most likely\n- Append to sequence, repeat",
        "codeLanguage": "python"
      },
      "tags": [
        "nlp",
        "text",
        "(generative",
        "trained",
        "transformer)"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "autoregressive-generation",
      "name": "Autoregressive Generation",
      "parentId": "text-generation",
      "sectionId": "17.3.2",
      "level": 2,
      "fullExplanation": "Autoregressive generation produces outputs one token at a time, conditioning on all previous tokens. P(x₁,...,xₙ) = ∏P(xᵢ|x₁,...,xᵢ₋₁). Cannot parallelize during generation (sequential dependency). Decoding strategies: greedy (argmax), beam search (top-k paths), sampling (temperature), nucleus sampling (top-p). Enables coherent long-form generation. Used in GPT, LLaMA, and text generation models.",
      "simpleExplanation": "Generate one word at a time, each word based on all previous words. Can't generate word 10 until words 1-9 exist. Like writing left to right—each word influences the next.",
      "example": {
        "description": "",
        "code": "Autoregressive generation:\n\nP(\"I love ML\") = P(\"I\") × P(\"love\"|\"I\") × P(\"ML\"|\"I love\")\n\nStep-by-step:\n1. P(w₁): Sample first word → \"I\"\n2. P(w₂|\"I\"): Sample second word → \"love\"\n3. P(w₃|\"I love\"): Sample third word → \"ML\"\n\nDecoding strategies:\nGreedy: Always pick highest probability word\n        Deterministic but can be repetitive\n\nTemperature sampling (T=1.0):\n        Sample proportional to probabilities\n        More diverse but can be incoherent\n\nNucleus (top-p=0.9):\n        Sample from smallest set covering 90% probability\n        Good balance of quality and diversity",
        "codeLanguage": "python"
      },
      "tags": [
        "nlp",
        "text",
        "autoregressive",
        "generation"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "machine-translation",
      "name": "Machine Translation",
      "parentId": "text-generation",
      "sectionId": "17.3.3",
      "level": 2,
      "fullExplanation": "Machine translation converts text from source to target language. Statistical MT: phrase-based with language models. Neural MT: encoder-decoder with attention, now transformer-based. Training: parallel corpora (aligned sentence pairs). Challenges: morphology, word order, idioms, rare words. Evaluation: BLEU score (n-gram overlap with references). Modern systems approach human quality for common language pairs.",
      "simpleExplanation": "Translate text from one language to another. Encode source sentence, decode into target language. Attention allows focusing on relevant source words while generating each target word. Powers Google Translate and similar services.",
      "example": {
        "description": "",
        "code": "Neural Machine Translation:\n\nSource (English): \"I love machine learning\"\nTarget (French): \"J'aime l'apprentissage automatique\"\n\nTransformer translation:\n1. Encode source:\n   [\"I\", \"love\", \"machine\", \"learning\"]\n   → [h₁, h₂, h₃, h₄] encoder hidden states\n\n2. Decode with attention:\n   [START] + attend to source → \"J'\"\n   \"J'\" + attend to source → \"aime\"\n   ...\n\nAttention pattern:\n\"J'\" attends to → \"I\" (subject)\n\"aime\" attends to → \"love\" (verb)\n\"automatique\" attends to → \"machine\" (adj)",
        "codeLanguage": "python"
      },
      "tags": [
        "nlp",
        "text",
        "machine",
        "translation"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "semantic-similarity",
      "name": "Semantic Similarity",
      "parentId": "semantic-understanding",
      "sectionId": "17.4.1",
      "level": 2,
      "fullExplanation": "Semantic similarity measures meaning closeness between texts. Lexical approaches: word overlap, edit distance. Embedding approaches: cosine similarity of sentence embeddings. Sentence-BERT: fine-tuned for similarity with siamese/triplet networks. Applications: duplicate detection, search ranking, clustering, recommendation. Datasets: STS benchmark, paraphrase detection.",
      "simpleExplanation": "How similar is the meaning of two texts? \"I love dogs\" and \"I adore canines\" are semantically similar despite different words. Embed texts as vectors, measure distance between them.",
      "example": {
        "description": "",
        "code": "Semantic similarity:\n\nSentence 1: \"The cat is sleeping on the couch\"\nSentence 2: \"A feline is napping on the sofa\"\n\nLexical similarity (word overlap): Low\n(Only \"the\" and \"on\" match)\n\nSemantic similarity (SBERT):\nembed(S1) = [0.2, -0.5, 0.8, ...]\nembed(S2) = [0.3, -0.4, 0.7, ...]\ncosine(S1, S2) = 0.95 (very similar!)\n\nSame meaning, different words → High semantic similarity\n\nUse cases:\n- Find duplicate questions\n- Match queries to documents\n- Cluster similar complaints",
        "codeLanguage": "python"
      },
      "tags": [
        "nlp",
        "text",
        "semantic",
        "similarity"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "sentiment-analysis",
      "name": "Sentiment Analysis",
      "parentId": "semantic-understanding",
      "sectionId": "17.4.2",
      "level": 2,
      "fullExplanation": "Sentiment analysis detects subjective opinions and emotions in text. Levels: document, sentence, aspect. Polarity: positive/negative/neutral. Intensity: rating (1-5 stars). Aspects: sentiment toward specific features (\"Great camera, terrible battery\"). Methods: lexicon-based (word polarity dictionaries), ML classifiers, deep learning (BERT). Challenges: sarcasm, negation, domain adaptation.",
      "simpleExplanation": "Detect feelings in text. Is this review positive or negative? How strongly? About which aspects? Crucial for understanding customer feedback, social media monitoring, brand perception.",
      "example": {
        "description": "",
        "code": "Sentiment analysis levels:\n\nDocument-level:\n\"Great product! Fast shipping, excellent quality,\n will buy again.\"\n→ Positive (0.95)\n\nSentence-level:\n\"The food was delicious but service was slow.\"\n→ Sentence 1: Positive (0.9)\n→ Sentence 2: Negative (0.7)\n\nAspect-based:\n\"The camera is amazing but battery life is terrible.\"\n→ Camera: Positive (0.95)\n→ Battery: Negative (0.90)\n→ Overall: Mixed\n\nHandling sarcasm:\n\"Oh great, another software update that breaks everything.\"\n→ Naive: Positive (sees \"great\")\n→ Sarcasm-aware: Negative",
        "codeLanguage": "python"
      },
      "tags": [
        "nlp",
        "text",
        "sentiment",
        "analysis"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "information-extraction",
      "name": "Information Extraction",
      "parentId": "semantic-understanding",
      "sectionId": "17.4.3",
      "level": 2,
      "fullExplanation": "Information extraction structures unstructured text. Tasks: Named Entity Recognition (find entities), Relation Extraction (find relationships between entities), Event Extraction (identify events, participants, attributes), Coreference Resolution (link entity mentions). Builds knowledge bases from text. Pipeline or joint models. Applications: knowledge graph construction, news analysis, scientific literature mining.",
      "simpleExplanation": "Pull structured facts from text. \"Apple CEO Tim Cook announced...\" → Extract: (Apple, CEO, Tim Cook), (Tim Cook, announced, ...). Turn articles into database entries.",
      "example": {
        "description": "",
        "code": "Information extraction pipeline:\n\nText: \"Apple CEO Tim Cook announced the new iPhone 15\n      at the September event in Cupertino.\"\n\n1. Named Entity Recognition:\n   Apple → ORG\n   Tim Cook → PERSON\n   iPhone 15 → PRODUCT\n   September → DATE\n   Cupertino → LOCATION\n\n2. Relation Extraction:\n   (Tim Cook, CEO_of, Apple)\n   (iPhone 15, made_by, Apple)\n   (Event, located_in, Cupertino)\n\n3. Event Extraction:\n   Event: Product_Announcement\n   Agent: Tim Cook\n   Product: iPhone 15\n   Time: September\n   Location: Cupertino\n\n→ Structured knowledge base entries!",
        "codeLanguage": "python"
      },
      "tags": [
        "nlp",
        "text",
        "information",
        "extraction"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "time-series-fundamentals",
      "name": "Time Series Fundamentals",
      "parentId": "time-series",
      "sectionId": "18.1",
      "level": 1,
      "fullExplanation": "Time Series Fundamentals covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about time series fundamentals and related techniques.",
      "example": {
        "description": "This section contains concepts related to time series fundamentals."
      },
      "tags": [
        "time-series",
        "forecasting",
        "time",
        "series",
        "fundamentals"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "classical-methods",
      "name": "Classical Methods",
      "parentId": "time-series",
      "sectionId": "18.2",
      "level": 1,
      "fullExplanation": "Classical Methods covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about classical methods and related techniques.",
      "example": {
        "description": "This section contains concepts related to classical methods."
      },
      "tags": [
        "time-series",
        "forecasting",
        "classical",
        "methods"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "deep-learning-for-time-series",
      "name": "Deep Learning for Time Series",
      "parentId": "time-series",
      "sectionId": "18.3",
      "level": 1,
      "fullExplanation": "Deep Learning for Time Series covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about deep learning for time series and related techniques.",
      "example": {
        "description": "This section contains concepts related to deep learning for time series."
      },
      "tags": [
        "time-series",
        "forecasting",
        "deep",
        "learning",
        "time"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "collaborative-filtering",
      "name": "Collaborative Filtering",
      "parentId": "recommendations",
      "sectionId": "19.1",
      "level": 1,
      "fullExplanation": "Collaborative Filtering covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about collaborative filtering and related techniques.",
      "example": {
        "description": "This section contains concepts related to collaborative filtering."
      },
      "tags": [
        "recommendations",
        "collaborative",
        "filtering"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "content-based-filtering",
      "name": "Content-Based Filtering",
      "parentId": "recommendations",
      "sectionId": "19.2",
      "level": 1,
      "fullExplanation": "Content-Based Filtering covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about content-based filtering and related techniques.",
      "example": {
        "description": "This section contains concepts related to content-based filtering."
      },
      "tags": [
        "recommendations",
        "collaborative",
        "content",
        "based",
        "filtering"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "deep-learning-recommendations",
      "name": "Deep Learning Recommendations",
      "parentId": "recommendations",
      "sectionId": "19.3",
      "level": 1,
      "fullExplanation": "Deep Learning Recommendations covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about deep learning recommendations and related techniques.",
      "example": {
        "description": "This section contains concepts related to deep learning recommendations."
      },
      "tags": [
        "recommendations",
        "collaborative",
        "deep",
        "learning"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "time-series",
      "name": "Time Series",
      "parentId": "time-series-fundamentals",
      "sectionId": "18.1.1",
      "level": 2,
      "fullExplanation": "A time series is a sequence of data points indexed in time order. Components: trend (long-term direction), seasonality (regular patterns), cycles (irregular patterns), noise (random variation). Stationarity: statistical properties constant over time. Tasks: forecasting (predict future), classification, anomaly detection, imputation. Key property: temporal dependency—observations are not independent.",
      "simpleExplanation": "Data points ordered by time. Stock prices, temperatures, sales figures. The order matters—yesterday affects today, today affects tomorrow. Patterns like trends and seasonal cycles repeat.",
      "example": {
        "description": "",
        "code": "Time series components:\n\nMonthly ice cream sales:\nJan: 100, Feb: 120, Mar: 200, Apr: 300, May: 400, Jun: 500\nJul: 550, Aug: 520, Sep: 350, Oct: 200, Nov: 120, Dec: 100\n[Next year repeats similarly]\n\nDecomposition:\n- Trend: Slight increase year-over-year (+5%/year)\n- Seasonality: Summer peaks, winter troughs\n- Noise: Random day-to-day variation\n\nTime series vs regular data:\nRegular: (x₁, y₁), (x₂, y₂) - independent\nTime series: y₁ → y₂ → y₃ - sequentially dependent",
        "codeLanguage": "python"
      },
      "tags": [
        "time-series",
        "forecasting",
        "time",
        "series"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "stationarity",
      "name": "Stationarity",
      "parentId": "time-series-fundamentals",
      "sectionId": "18.1.2",
      "level": 2,
      "fullExplanation": "A stationary time series has constant statistical properties over time: constant mean, constant variance, and autocorrelation depending only on lag (not time). Many models assume stationarity. Non-stationary series can be transformed: differencing (remove trend), log transform (stabilize variance), seasonal differencing (remove seasonality). Tests: ADF (Augmented Dickey-Fuller), KPSS.",
      "simpleExplanation": "The series behaves the same way at all times. Average doesn't drift, variability stays constant. Many models require stationarity. If not stationary, transform it (e.g., use differences instead of raw values).",
      "example": {
        "description": "",
        "code": "Stationary vs Non-stationary:\n\nNon-stationary (trending):\nTime:  1   2   3   4   5   6\nValue: 10  15  20  25  30  35\nMean keeps increasing!\n\nAfter differencing (Δyₜ = yₜ - yₜ₋₁):\nDiff:  5   5   5   5   5\nNow constant mean = 5 ✓ Stationary\n\nNon-stationary (increasing variance):\nTime:  1   2   3   4   5\nValue: 10  12  15  25  50\nVariance explodes!\n\nAfter log transform:\nLog:   2.3 2.5 2.7 3.2 3.9\nVariance stabilized ✓\n\nADF test:\np < 0.05 → Stationary\np > 0.05 → Non-stationary (difference needed)",
        "codeLanguage": "python"
      },
      "tags": [
        "time-series",
        "forecasting",
        "stationarity"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "autocorrelation",
      "name": "Autocorrelation",
      "parentId": "time-series-fundamentals",
      "sectionId": "18.1.3",
      "level": 2,
      "fullExplanation": "Autocorrelation measures correlation of a time series with lagged versions of itself. ACF (Autocorrelation Function): correlation at each lag. PACF (Partial Autocorrelation): correlation at lag k after removing effects of shorter lags. ACF/PACF plots guide model selection: AR models show PACF cutoff, MA models show ACF cutoff. High autocorrelation indicates predictability.",
      "simpleExplanation": "How much does today relate to yesterday, last week, last year? High autocorrelation at lag 7 means weekly patterns. Helps choose which past values to use for prediction.",
      "example": {
        "description": "```\nACF analysis for daily sales:\n\nLag 1:  r = 0.85 (yesterday matters)\nLag 7:  r = 0.70 (weekly pattern!)\nLag 14: r = 0.65 (biweekly)\nLag 30: r = 0.50 (monthly)\n\nACF plot:\n     |*\n0.8  |****\n     |*******\n0.4  |***************\n     |*******************\n0.0  +"
      },
      "tags": [
        "time-series",
        "forecasting",
        "autocorrelation"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "moving-average",
      "name": "Moving Average",
      "parentId": "classical-methods",
      "sectionId": "18.2.1",
      "level": 2,
      "fullExplanation": "Moving average smooths time series by averaging nearby values. Simple Moving Average (SMA): mean of last n values. Weighted Moving Average (WMA): weighted mean, recent values weighted higher. Exponential Moving Average (EMA): exponentially decreasing weights into past. Smoothing removes noise, reveals trends. Also refers to MA(q) model in ARIMA context: current value as linear combination of past errors.",
      "simpleExplanation": "Average recent values to smooth out noise and see the trend. 7-day moving average shows weekly trend without daily fluctuations. Used in finance, weather, and data visualization.",
      "example": {
        "description": "",
        "code": "Moving average smoothing:\n\nDaily values: 10, 12, 8, 15, 11, 13, 9, 14, 10, 12\n\n3-day SMA:\nDay 3: (10+12+8)/3 = 10.0\nDay 4: (12+8+15)/3 = 11.7\nDay 5: (8+15+11)/3 = 11.3\n...\n\nResult:\nRaw:    10, 12, 8, 15, 11, 13, 9, 14, 10, 12\nSMA-3:  -, -, 10, 11.7, 11.3, 13, 11, 12, 11, 12\n\nSmoothed line shows trend, noise reduced!",
        "codeLanguage": "python"
      },
      "tags": [
        "time-series",
        "forecasting",
        "moving",
        "average"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "arima",
      "name": "ARIMA",
      "parentId": "classical-methods",
      "sectionId": "18.2.2",
      "level": 2,
      "fullExplanation": "ARIMA (AutoRegressive Integrated Moving Average) combines three components: AR(p): linear combination of past p values; I(d): differencing d times for stationarity; MA(q): linear combination of past q errors. Notation: ARIMA(p,d,q). Model selection via ACF/PACF analysis or AIC/BIC. SARIMA adds seasonal components: ARIMA(p,d,q)(P,D,Q)s. Standard benchmark for time series forecasting.",
      "simpleExplanation": "A powerful formula combining past values, differencing for stability, and past errors. ARIMA(1,1,1) uses yesterday's value, today's change, and yesterday's prediction error. The classic forecasting approach.",
      "example": {
        "description": "",
        "code": "ARIMA(1,1,1) model:\n\nComponents:\nAR(1): Uses previous value\nI(1): First difference (removes trend)\nMA(1): Uses previous error\n\nEquation:\nyₜ - yₜ₋₁ = φ(yₜ₋₁ - yₜ₋₂) + θεₜ₋₁ + εₜ\n\nForecast:\nŷₜ = yₜ₋₁ + φ(yₜ₋₁ - yₜ₋₂) + θεₜ₋₁\n\nPython:\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(data, order=(1,1,1))\nresults = model.fit()\nforecast = results.forecast(steps=10)",
        "codeLanguage": "python"
      },
      "tags": [
        "time-series",
        "forecasting",
        "arima"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "exponential-smoothing",
      "name": "Exponential Smoothing",
      "parentId": "classical-methods",
      "sectionId": "18.2.3",
      "level": 2,
      "fullExplanation": "Exponential smoothing forecasts using weighted averages of past observations, with exponentially decreasing weights. Simple ES (level only), Holt's (level + trend), Holt-Winters (level + trend + seasonality). Smoothing parameters α (level), β (trend), γ (seasonality) between 0-1. ETS framework: Error, Trend, Seasonality (additive or multiplicative). Fast, interpretable, competitive with complex models.",
      "simpleExplanation": "Smoothing where recent data matters most. Weights decay exponentially into the past. Three versions: simple (flat), Holt's (with trend), Holt-Winters (with trend and seasonality). Works surprisingly well.",
      "example": {
        "description": "",
        "code": "Simple Exponential Smoothing (α=0.3):\n\nForecast: ŷₜ₊₁ = αyₜ + (1-α)ŷₜ\n\nData: 100, 110, 105, 120, 115\n\nStep 1: ŷ₂ = 0.3(100) + 0.7(100) = 100\nStep 2: ŷ₃ = 0.3(110) + 0.7(100) = 103\nStep 3: ŷ₄ = 0.3(105) + 0.7(103) = 103.6\nStep 4: ŷ₅ = 0.3(120) + 0.7(103.6) = 108.5\nStep 5: ŷ₆ = 0.3(115) + 0.7(108.5) = 110.4\n\nHolt-Winters adds trend and seasonal components.",
        "codeLanguage": "python"
      },
      "tags": [
        "time-series",
        "forecasting",
        "exponential",
        "smoothing"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "lstm-time-series",
      "name": "LSTM for Time Series",
      "parentId": "deep-learning-for-time-series",
      "sectionId": "18.3.1",
      "level": 2,
      "fullExplanation": "LSTMs model time series by maintaining long-term memory across sequences. Input: sequence of past observations (window). Output: next value(s) or sequence. Handles variable-length sequences, learns complex patterns, captures long-range dependencies. Many-to-one for forecasting, many-to-many for sequence-to-sequence. Stacked LSTMs, bidirectional for feature extraction. Requires sequence padding, careful window selection.",
      "simpleExplanation": "Use LSTM's memory to learn patterns in sequential data. Feed in the last N days, predict tomorrow. LSTM remembers important patterns from weeks ago. Popular before transformers, still useful.",
      "example": {
        "description": "",
        "code": "LSTM for stock price prediction:\n\nInput: Past 30 days of prices\nOutput: Next day's price\n\nArchitecture:\nInput: (30, 5) - 30 days, 5 features (OHLCV)\nLSTM: 50 units, return sequences\nLSTM: 50 units\nDense: 1 (predicted price)\n\nTraining:\nX: [[day1-30], [day2-31], [day3-32], ...]\ny: [day31_price, day32_price, day33_price, ...]\n\nSliding window creates training samples.",
        "codeLanguage": "python"
      },
      "tags": [
        "time-series",
        "forecasting",
        "lstm",
        "time",
        "series"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "tcn",
      "name": "Temporal Convolutional Network (TCN)",
      "parentId": "deep-learning-for-time-series",
      "sectionId": "18.3.2",
      "level": 2,
      "fullExplanation": "TCN applies causal convolutions to sequences, using dilated convolutions for long-range dependencies. Causal: no future leakage (convolution only sees past). Dilated: exponentially increasing gaps (1, 2, 4, 8...) capture different timescales efficiently. Residual connections for deep networks. Parallelizable (unlike RNNs), stable gradients, fixed receptive field. Often matches or beats RNNs on sequence tasks.",
      "simpleExplanation": "CNNs for time series. Look only at past data (causal). Dilated convolutions see long history without huge kernels. Can train in parallel (faster than LSTM). Works great for many sequence tasks.",
      "example": {
        "description": "",
        "code": "TCN with dilated convolutions:\n\nInput sequence: [t1, t2, t3, t4, t5, t6, t7, t8]\n\nLayer 1 (dilation=1):\nEach output uses 2 adjacent inputs\n[t1,t2]→o1, [t2,t3]→o2, [t3,t4]→o3...\nReceptive field: 2\n\nLayer 2 (dilation=2):\n[o1,_,o3]→p1, [o2,_,o4]→p2...\nReceptive field: 4\n\nLayer 3 (dilation=4):\n[p1,_,_,_,p5]→q1...\nReceptive field: 8\n\nExponentially growing receptive field!\n8 time steps with just 3 layers.",
        "codeLanguage": "python"
      },
      "tags": [
        "time-series",
        "forecasting",
        "temporal",
        "convolutional",
        "network"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "transformers-time-series",
      "name": "Transformers for Time Series",
      "parentId": "deep-learning-for-time-series",
      "sectionId": "18.3.3",
      "level": 2,
      "fullExplanation": "Transformers apply self-attention to time series, enabling direct modeling of long-range dependencies. Temporal embedding replaces positional encoding. Informer, Autoformer, FEDformer address long-horizon forecasting with sparse attention. PatchTST treats time windows as patches. Advantages: parallelizable, explicit dependency modeling. Challenges: quadratic complexity, need for modifications for long sequences.",
      "simpleExplanation": "Self-attention for time series. Each time step can directly attend to any other time step—no sequential bottleneck. Modern architectures handle very long sequences efficiently. State-of-the-art for many forecasting benchmarks.",
      "example": {
        "description": "",
        "code": "Transformer for forecasting:\n\nInput: 96 historical time steps\nOutput: 24 future time steps\n\nArchitecture:\n1. Embed time steps (value embedding + temporal embedding)\n2. Transformer encoder (self-attention over history)\n3. Transformer decoder (cross-attention to encoder)\n4. Linear projection to predictions\n\nAttention patterns:\n- Self-attention finds similar patterns in history\n- \"Day 50 was similar to day 10, use that pattern\"\n- Long-range dependencies captured directly\n\nSpecialized variants:\n- Informer: ProbSparse attention (O(n log n))\n- Autoformer: Decomposition + Auto-correlation\n- PatchTST: Patch-based, channel-independent",
        "codeLanguage": "python"
      },
      "tags": [
        "time-series",
        "forecasting",
        "transformers",
        "time",
        "series"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "collaborative-filtering",
      "name": "Collaborative Filtering",
      "parentId": "collaborative-filtering",
      "sectionId": "19.1.1",
      "level": 2,
      "fullExplanation": "Collaborative filtering recommends items based on collective user behavior. User-based: find similar users, recommend what they liked. Item-based: find similar items to what user liked. Only uses interaction data (ratings, purchases), no content features needed. Challenges: cold start (new users/items), sparsity (most user-item pairs unknown), scalability. Foundation of recommendation systems (Netflix, Amazon).",
      "simpleExplanation": "\"Users who liked this also liked that.\" Find people with similar taste, recommend what they enjoyed. Or find items similar to what you liked. Uses only the pattern of who-bought-what.",
      "example": {
        "description": "",
        "code": "User-based collaborative filtering:\n\nUser ratings matrix:\n         Movie1  Movie2  Movie3  Movie4\nAlice:     5       4       ?       ?\nBob:       5       5       4       ?\nCarol:     2       1       5       5\n\nFinding recommendation for Alice (Movie3):\n\n1. Find similar users:\n   Alice-Bob similarity: High (both like M1, M2)\n   Alice-Carol similarity: Low (opposite preferences)\n\n2. Bob rated Movie3 = 4\n\n3. Predict: Alice would rate Movie3 ≈ 4\n\nItem-based:\nMovie1 and Movie2 are similar (same users like both)\nAlice likes Movie1, Movie2\nFind items similar to those → Recommend",
        "codeLanguage": "python"
      },
      "tags": [
        "recommendations",
        "collaborative",
        "filtering"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "matrix-factorization",
      "name": "Matrix Factorization",
      "parentId": "collaborative-filtering",
      "sectionId": "19.1.2",
      "level": 2,
      "fullExplanation": "Matrix factorization decomposes the user-item rating matrix into low-rank user and item embeddings. R ≈ UV^T where U (users × k) and V (items × k). Each user/item represented by k-dimensional latent vector. Trained to minimize reconstruction error on observed ratings. SVD-based or gradient descent. Handles sparsity efficiently, learns interpretable factors (genres, preferences).",
      "simpleExplanation": "Find hidden features that explain ratings. Users have preferences for hidden factors (action, romance). Items have amounts of those factors. Rating = dot product of user preferences and item factors. Few numbers per user/item, explains millions of ratings.",
      "example": {
        "description": "",
        "code": "Matrix Factorization:\n\nRatings matrix (1000 users × 5000 movies):\nMost entries unknown (sparse)\n\nFactorize into:\nUser matrix U: 1000 × 50 (50 latent factors)\nMovie matrix V: 5000 × 50\n\nUser 123's vector: [0.8, -0.2, 0.5, ...]\n(Likes: action, not romance, moderate comedy)\n\nMovie \"Die Hard\" vector: [0.9, -0.4, 0.3, ...]\n(Is: action, not romance, bit of comedy)\n\nPredicted rating:\nUser123 × DieHard = 0.8×0.9 + (-0.2)×(-0.4) + 0.5×0.3 + ...\n                  = 0.72 + 0.08 + 0.15 + ... = 4.2\n\nHigh predicted rating → Recommend!",
        "codeLanguage": "python"
      },
      "tags": [
        "recommendations",
        "collaborative",
        "matrix",
        "factorization"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "implicit-feedback",
      "name": "Implicit Feedback",
      "parentId": "collaborative-filtering",
      "sectionId": "19.1.3",
      "level": 2,
      "fullExplanation": "Implicit feedback infers preferences from user behavior (views, clicks, time spent) rather than explicit ratings. Challenges: no negative signal (not clicking ≠ dislike), varying confidence levels, noisy data. Approaches: treat interactions as positive examples, sample negatives, use confidence weighting. BPR (Bayesian Personalized Ranking) optimizes pairwise ranking. More data available than explicit ratings.",
      "simpleExplanation": "Users don't rate—they click, buy, watch. Use this implicit signal. Clicked = probably interested. Didn't click = maybe not interested, or maybe never saw it. Need special handling since no explicit \"dislike.\"",
      "example": {
        "description": "",
        "code": "Implicit feedback signals:\n\nUser Alice's behavior:\n- Viewed product A (5 times)\n- Purchased product B\n- Added product C to cart\n- Clicked product D (once)\n- Never interacted with E, F, G\n\nConfidence weighting:\nA: confidence = 5 (multiple views)\nB: confidence = 10 (purchased!)\nC: confidence = 3 (cart)\nD: confidence = 1 (single click)\nE, F, G: confidence = 0 (unknown)\n\nTraining:\nPositive pairs: (Alice, A), (Alice, B), ...\nNegative sampling: (Alice, E), (Alice, F)\nbut with lower confidence (might just be unseen)\n\nOptimize: Prefer positives over negatives",
        "codeLanguage": "python"
      },
      "tags": [
        "recommendations",
        "collaborative",
        "implicit",
        "feedback"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "content-based-filtering",
      "name": "Content-Based Filtering",
      "parentId": "content-based-filtering",
      "sectionId": "19.2.1",
      "level": 2,
      "fullExplanation": "Content-based filtering recommends items similar to what user previously liked, using item features. Build user profile from liked item features. New items scored by similarity to profile. No cold-start for items (features known immediately). Doesn't need other users' data. Limitations: limited novelty (similar items), feature engineering required, user cold start remains.",
      "simpleExplanation": "\"You liked action movies with Tom Cruise, here are more action movies with Tom Cruise.\" Uses item attributes (genre, actors, keywords) not other users' behavior. Recommends based on content similarity.",
      "example": {
        "description": "",
        "code": "Content-based movie recommendation:\n\nUser profile (from liked movies):\nAction: 0.8, Sci-Fi: 0.6, Romance: 0.1\nTom Cruise: 0.7, Keanu Reeves: 0.5\n1990s: 0.4, 2000s: 0.6\n\nNew movie: \"The Matrix\" (1999)\nFeatures: Action: 0.9, Sci-Fi: 1.0, Romance: 0.0\n          Keanu Reeves: 1.0, 1990s: 1.0\n\nSimilarity:\n= 0.8×0.9 + 0.6×1.0 + 0.1×0.0 + 0.5×1.0 + 0.4×1.0\n= 0.72 + 0.6 + 0 + 0.5 + 0.4 = 2.22 (high!)\n\nRecommend \"The Matrix\" ✓",
        "codeLanguage": "python"
      },
      "tags": [
        "recommendations",
        "collaborative",
        "content",
        "based",
        "filtering"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "hybrid-recommendation",
      "name": "Hybrid Recommendation",
      "parentId": "content-based-filtering",
      "sectionId": "19.2.2",
      "level": 2,
      "fullExplanation": "Hybrid systems combine collaborative and content-based methods to leverage strengths of both. Approaches: weighted (blend scores), switching (use one or other based on context), feature combination (use content features in collaborative model), cascade (one refines other's results). Addresses cold start (content works for new items), sparsity (collaborative captures user patterns). Modern systems are hybrid.",
      "simpleExplanation": "Best of both worlds. Use content features AND user behavior patterns. New items? Use content similarity. Established items? Use collaborative filtering. Blend the predictions for better results.",
      "example": {
        "description": "",
        "code": "Hybrid recommendation:\n\nFor User Alice, Item X:\n\nCollaborative score:\nSimilar users rated X: 4.2 average\n\nContent-based score:\nX's features match Alice's profile: 0.75 similarity\n\nHybrid combination (weighted):\nScore = 0.6 × CF_score + 0.4 × CB_score\n      = 0.6 × 4.2 + 0.4 × 3.75\n      = 2.52 + 1.50 = 4.02\n\nFor new item Y (no ratings):\nCF_score = unknown\nCB_score = 0.60\n\nUse CB only: Score = 3.0\n\nSwitch based on data availability!",
        "codeLanguage": "python"
      },
      "tags": [
        "recommendations",
        "collaborative",
        "hybrid",
        "recommendation"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "ncf",
      "name": "Neural Collaborative Filtering (NCF)",
      "parentId": "deep-learning-recommendations",
      "sectionId": "19.3.1",
      "level": 2,
      "fullExplanation": "NCF replaces matrix factorization's dot product with neural networks. User and item embeddings fed through MLPs to model complex interactions. Generalizes MF (can learn dot product) but learns non-linear patterns. GMF (Generalized MF) + MLP combined in NeuMF. Handles implicit feedback naturally. More expressive than linear models, standard neural recommendation baseline.",
      "simpleExplanation": "Matrix factorization with neural networks. Instead of just multiplying user and item vectors, run them through neural networks to capture complex patterns. More powerful than simple dot products.",
      "example": {
        "description": "",
        "code": "Neural Collaborative Filtering architecture:\n\nInput:\nUser ID → User Embedding (64 dims)\nItem ID → Item Embedding (64 dims)\n\nGMF Path:\nElement-wise product: user ⊙ item → 64 dims\n\nMLP Path:\nConcatenate: [user; item] → 128 dims\nDense: 128 → 64 → 32 → 16\n\nCombine:\n[GMF_output; MLP_output] → 80 dims\nDense: 80 → 1 (rating/probability)\n\nTraining:\nBinary cross-entropy (implicit: interact/no-interact)\nor MSE (explicit ratings)",
        "codeLanguage": "python"
      },
      "tags": [
        "recommendations",
        "collaborative",
        "neural",
        "filtering",
        "(ncf)"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "two-tower",
      "name": "Two-Tower Model",
      "parentId": "deep-learning-recommendations",
      "sectionId": "19.3.2",
      "level": 2,
      "fullExplanation": "Two-tower (dual encoder) architecture encodes users and items separately into a shared embedding space. User tower: user features → user embedding. Item tower: item features → item embedding. Similarity computed as dot product or cosine. Key advantage: item embeddings can be precomputed and indexed for fast retrieval. Used at scale (YouTube, Google). Retrieval stage in two-stage systems.",
      "simpleExplanation": "Two separate networks: one for users, one for items. Both output embeddings in the same space. Compare with dot product. Items can be pre-computed and stored—at prediction time, just compute user embedding and find nearest items. Super fast at scale.",
      "example": {
        "description": "",
        "code": "Two-Tower for YouTube recommendations:\n\nUser Tower:\nUser features: [watch_history, search_terms, demographics, ...]\n         ↓\n     Deep network\n         ↓\nUser embedding (256 dims)\n\nVideo Tower:\nVideo features: [title, description, channel, duration, ...]\n         ↓\n     Deep network\n         ↓\nVideo embedding (256 dims)\n\nServing:\n1. Precompute all video embeddings → Index (ANN)\n2. User request → Compute user embedding\n3. Approximate nearest neighbor search\n4. Top-K videos in milliseconds!\n\nBillions of videos, millions of users → Still fast",
        "codeLanguage": "python"
      },
      "tags": [
        "recommendations",
        "collaborative",
        "tower",
        "model"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "sequential-recommendation",
      "name": "Sequential Recommendation",
      "parentId": "deep-learning-recommendations",
      "sectionId": "19.3.3",
      "level": 2,
      "fullExplanation": "Sequential recommendation models the order of user interactions to predict next item. User history as sequence, predict next interaction. Architectures: RNN/LSTM, self-attention (SASRec), Transformers (BERT4Rec). Captures temporal dynamics, session patterns, evolving preferences. Contrasts with static collaborative filtering. Important for e-commerce, streaming, news.",
      "simpleExplanation": "Order matters! If you browsed shoes → socks → pants, what's next? Sequential models learn patterns in browsing sequences. Transformers attend to relevant past items to predict the next click.",
      "example": {
        "description": "",
        "code": "SASRec (Self-Attentive Sequential Recommendation):\n\nUser history: [item1, item2, item3, item4, item5]\n\nSelf-attention:\nPosition embeddings + Item embeddings\nCausal masking (can't see future)\nAttention: Which past items matter for next?\n\nitem5 attends to:\n- item4 (0.4) - recent, similar\n- item3 (0.3) - related category\n- item2 (0.1)\n- item1 (0.2) - same brand\n\nPrediction:\nWeighted history → Predict item6 distribution\nRecommend top-K from prediction\n\nLearns: \"After browsing cameras, users often buy memory cards\"",
        "codeLanguage": "python"
      },
      "tags": [
        "recommendations",
        "collaborative",
        "sequential",
        "recommendation"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "hyperparameter-tuning",
      "name": "Hyperparameter Tuning",
      "parentId": "practical",
      "sectionId": "20.2",
      "level": 1,
      "fullExplanation": "Hyperparameter Tuning covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about hyperparameter tuning and related techniques.",
      "example": {
        "description": "This section contains concepts related to hyperparameter tuning."
      },
      "tags": [
        "practical",
        "engineering",
        "hyperparameter",
        "tuning"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "model-debugging",
      "name": "Model Debugging",
      "parentId": "practical",
      "sectionId": "20.3",
      "level": 1,
      "fullExplanation": "Model Debugging covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about model debugging and related techniques.",
      "example": {
        "description": "This section contains concepts related to model debugging."
      },
      "tags": [
        "practical",
        "engineering",
        "model",
        "debugging"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "model-deployment",
      "name": "Model Deployment",
      "parentId": "mlops",
      "sectionId": "21.1",
      "level": 1,
      "fullExplanation": "Model Deployment covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about model deployment and related techniques.",
      "example": {
        "description": "This section contains concepts related to model deployment."
      },
      "tags": [
        "mlops",
        "deployment",
        "model"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "monitoring",
      "name": "Monitoring",
      "parentId": "mlops",
      "sectionId": "21.2",
      "level": 1,
      "fullExplanation": "Monitoring covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about monitoring and related techniques.",
      "example": {
        "description": "This section contains concepts related to monitoring."
      },
      "tags": [
        "mlops",
        "deployment",
        "monitoring"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "bias-fairness",
      "name": "Bias & Fairness",
      "parentId": "ethics",
      "sectionId": "22.1",
      "level": 1,
      "fullExplanation": "Bias & Fairness covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about bias & fairness and related techniques.",
      "example": {
        "description": "This section contains concepts related to bias & fairness."
      },
      "tags": [
        "ethics",
        "fairness",
        "bias"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "responsible-ai",
      "name": "Responsible AI",
      "parentId": "ethics",
      "sectionId": "22.2",
      "level": 1,
      "fullExplanation": "Responsible AI covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about responsible ai and related techniques.",
      "example": {
        "description": "This section contains concepts related to responsible ai."
      },
      "tags": [
        "ethics",
        "fairness",
        "responsible"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "regulatory-compliance",
      "name": "Regulatory Compliance",
      "parentId": "ethics",
      "sectionId": "22.3",
      "level": 1,
      "fullExplanation": "Regulatory Compliance covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about regulatory compliance and related techniques.",
      "example": {
        "description": "This section contains concepts related to regulatory compliance."
      },
      "tags": [
        "ethics",
        "fairness",
        "regulatory",
        "compliance"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "missing-data",
      "name": "Handling Missing Data",
      "parentId": "feature-engineering",
      "sectionId": "20.1.3",
      "level": 2,
      "fullExplanation": "Missing data handling strategies depend on missingness mechanism. MCAR (Missing Completely at Random): safe to drop. MAR (Missing at Random): imputation appropriate. MNAR (Missing Not at Random): missingness is informative. Methods: deletion (listwise, pairwise), imputation (mean, median, mode, regression, KNN, MICE), indicator variables (missingness as feature). Missing data can itself be a signal.",
      "simpleExplanation": "What to do when data has blanks? Delete rows (loses information), fill with average (simple), predict missing values (sophisticated), or add a \"was_missing\" feature. The best approach depends on why data is missing.",
      "example": {
        "description": "",
        "code": "Handling missing data:\n\nOriginal:\nAge: [25, NaN, 35, NaN, 45]\nIncome: [50K, 80K, NaN, 120K, 150K]\n\nStrategies:\n\n1. Mean imputation:\n   Age: [25, 35, 35, 35, 45] (mean=35)\n\n2. Median imputation:\n   More robust to outliers\n\n3. Regression imputation:\n   Predict missing Age from Income, other features\n\n4. Missing indicator:\n   Age: [25, 35, 35, 35, 45]\n   Age_missing: [0, 1, 0, 1, 0]\n   (Missingness might be informative!)\n\n5. Multiple imputation (MICE):\n   Generate several plausible values\n   Account for imputation uncertainty",
        "codeLanguage": "python"
      },
      "tags": [
        "practical",
        "engineering",
        "handling",
        "missing",
        "data"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "imbalanced-data",
      "name": "Handling Imbalanced Data",
      "parentId": "feature-engineering",
      "sectionId": "20.1.4",
      "level": 2,
      "fullExplanation": "Imbalanced datasets have skewed class distributions (e.g., 99% negative, 1% positive). Models tend to predict majority class. Solutions: resampling (oversample minority: SMOTE, undersample majority), class weights (penalize majority class errors less), threshold adjustment, anomaly detection framing, appropriate metrics (F1, AUC-ROC, not accuracy). Common in fraud detection, medical diagnosis, manufacturing.",
      "simpleExplanation": "When one class is rare (fraud, disease), models ignore it. Fixes: duplicate rare examples, remove common ones, or weight rare examples more. Use metrics that care about the rare class.",
      "example": {
        "description": "",
        "code": "Imbalanced fraud detection:\n\nDataset:\n99,000 legitimate transactions\n1,000 fraud transactions (1%)\n\nProblem:\nModel predicts \"not fraud\" always → 99% accuracy!\nBut 0% fraud detection (useless)\n\nSolutions:\n\n1. Class weights:\n   fraud weight = 99, normal weight = 1\n   Misclassifying fraud costs 99× more\n\n2. SMOTE (oversample):\n   Generate synthetic fraud examples\n   Balance to 50,000 / 50,000\n\n3. Undersample majority:\n   Random sample 1,000 normal transactions\n   Now 1,000 / 1,000 balanced\n\n4. Threshold adjustment:\n   Lower threshold for fraud prediction\n   P(fraud) > 0.1 instead of 0.5\n\n5. Evaluation:\n   Use Precision, Recall, F1, AUC-ROC\n   NOT accuracy!",
        "codeLanguage": "python"
      },
      "tags": [
        "practical",
        "engineering",
        "handling",
        "imbalanced",
        "data"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "hyperparameter-tuning",
      "name": "Hyperparameter Tuning",
      "parentId": "hyperparameter-tuning",
      "sectionId": "20.2.1",
      "level": 2,
      "fullExplanation": "Hyperparameters are settings configured before training (learning rate, regularization strength, tree depth). Tuning finds optimal values for validation performance. Methods: grid search (exhaustive), random search (often better), Bayesian optimization (intelligent exploration), Hyperband (early stopping). Use validation set or cross-validation for evaluation. Automated ML (AutoML) automates this process.",
      "simpleExplanation": "Find the best settings for your model. Too small learning rate = slow. Too large = unstable. Try different values, see what works best on held-out data. Grid search tries all combinations; random search samples intelligently.",
      "example": {
        "description": "",
        "code": "Hyperparameter tuning for Random Forest:\n\nHyperparameters:\n- n_estimators: [50, 100, 200, 500]\n- max_depth: [5, 10, 20, None]\n- min_samples_split: [2, 5, 10]\n\nGrid search: 4 × 4 × 3 = 48 combinations\nTry all, pick best validation score\n\nRandom search: Sample 20 random combinations\nOften finds good solution faster\n\nBayesian optimization:\n1. Try some random configs\n2. Build model of score vs hyperparameters\n3. Choose next config that balances exploration/exploitation\n4. Repeat, converge to optimum\n\nBest found: n_estimators=200, max_depth=10, min_samples_split=5\nValidation F1: 0.89",
        "codeLanguage": "python"
      },
      "tags": [
        "practical",
        "engineering",
        "hyperparameter",
        "tuning"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "grid-search",
      "name": "Grid Search",
      "parentId": "hyperparameter-tuning",
      "sectionId": "20.2.2",
      "level": 2,
      "fullExplanation": "Grid search exhaustively evaluates all combinations of specified hyperparameter values. Define grid of values for each hyperparameter. Evaluate each combination with cross-validation. Select best performing configuration. Pros: thorough, finds global optimum in grid. Cons: computationally expensive (exponential in hyperparameters), fixed resolution, doesn't adapt. Best for small search spaces.",
      "simpleExplanation": "Try every combination. If you have 3 options for each of 3 hyperparameters, try all 27 combinations. Guaranteed to find the best in your grid. But expensive if grid is large.",
      "example": {
        "description": "",
        "code": "Grid search example:\n\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'C': [0.1, 1, 10],\n    'kernel': ['rbf', 'linear'],\n    'gamma': [0.1, 1, 10]\n}\n\n# 3 × 2 × 3 = 18 combinations\n# × 5-fold CV = 90 model fits\n\ngrid_search = GridSearchCV(\n    SVC(),\n    param_grid,\n    cv=5,\n    scoring='f1'\n)\ngrid_search.fit(X_train, y_train)\n\nprint(grid_search.best_params_)\n# {'C': 1, 'kernel': 'rbf', 'gamma': 0.1}\n\nprint(grid_search.best_score_)\n# 0.92",
        "codeLanguage": "python"
      },
      "tags": [
        "practical",
        "engineering",
        "grid",
        "search"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "random-search",
      "name": "Random Search",
      "parentId": "hyperparameter-tuning",
      "sectionId": "20.2.3",
      "level": 2,
      "fullExplanation": "Random search samples hyperparameter configurations randomly from specified distributions. Given budget of N evaluations, samples N random configurations. More efficient than grid search: higher probability of finding good values in important dimensions (grid wastes effort on unimportant ones). Use continuous distributions for fine-grained search. Often matches grid search with fewer evaluations.",
      "simpleExplanation": "Randomly try combinations instead of trying all. Surprisingly effective! If one hyperparameter matters most, random search explores it more thoroughly than grid search. Faster and often equally good.",
      "example": {
        "description": "",
        "code": "Random search example:\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform, loguniform\n\nparam_distributions = {\n    'C': loguniform(0.01, 100),        # Log-uniform: 0.01 to 100\n    'gamma': loguniform(0.001, 10),    # Log-uniform: 0.001 to 10\n    'kernel': ['rbf', 'linear']\n}\n\nrandom_search = RandomizedSearchCV(\n    SVC(),\n    param_distributions,\n    n_iter=20,  # Only 20 tries (vs 18+ for grid)\n    cv=5,\n    scoring='f1'\n)\nrandom_search.fit(X_train, y_train)\n\n# Often finds equally good or better solution!\nprint(random_search.best_params_)\n# {'C': 2.35, 'gamma': 0.042, 'kernel': 'rbf'}",
        "codeLanguage": "python"
      },
      "tags": [
        "practical",
        "engineering",
        "random",
        "search"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "error-analysis",
      "name": "Error Analysis",
      "parentId": "model-debugging",
      "sectionId": "20.3.1",
      "level": 2,
      "fullExplanation": "Error analysis systematically examines model mistakes to identify improvement opportunities. Categorize errors by type, find patterns, prioritize high-impact fixes. Confusion matrix shows class-wise performance. Look at individual misclassified examples. Slice performance by segments (demographics, categories). Informs feature engineering, data collection, model architecture decisions.",
      "simpleExplanation": "Study your model's mistakes. Which examples does it get wrong? Are there patterns? Maybe it fails on long sentences, or blurry images. Understanding errors tells you how to improve.",
      "example": {
        "description": "",
        "code": "Error analysis for sentiment classifier:\n\nOverall accuracy: 85%\n\nConfusion matrix:\n              Predicted\n              Pos   Neg\nActual Pos    900   100\n       Neg    150   850\n\nExamine false negatives (100):\n- 40% contain sarcasm\n- 30% have negation (\"not bad\" = positive)\n- 20% are very short\n- 10% other\n\nExamine false positives (150):\n- 50% have positive words in negative context\n- 30% are complaints with polite language\n- 20% other\n\nActions:\n1. Add sarcasm detection features\n2. Handle negation explicitly\n3. Collect more short-text training data",
        "codeLanguage": "python"
      },
      "tags": [
        "practical",
        "engineering",
        "error",
        "analysis"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "learning-curves",
      "name": "Learning Curves",
      "parentId": "model-debugging",
      "sectionId": "20.3.2",
      "level": 2,
      "fullExplanation": "Learning curves plot performance vs training set size or training iterations. Training-validation gap indicates overfitting/underfitting. Both curves plateau high: good fit. Training high, validation low: overfitting (need regularization, more data, simpler model). Both plateau low: underfitting (need more features, complex model). Guides decisions on data collection and model complexity.",
      "simpleExplanation": "Track performance as training progresses. Training score rising but validation flat? Overfitting. Both low? Model too simple. Learning curves tell you if you need more data, simpler model, or more complexity.",
      "example": {
        "description": "",
        "code": "Learning curves interpretation:\n\n1. Good fit:\n   Training:    90% ───────────\n   Validation:  88% ───────────\n   Small gap, both high ✓\n\n2. Overfitting:\n   Training:    99% ───────────\n   Validation:  75% ───────────\n   Large gap! Regularize or get more data.\n\n3. Underfitting:\n   Training:    70% ───────────\n   Validation:  68% ───────────\n   Both low, small gap. Model too simple.\n\n4. Need more data:\n   Training:    95% ───────────\n   Validation:  80% ────────────↗ (still rising)\n   More data would help!\n\n5. Data won't help:\n   Training:    95% ───────────\n   Validation:  85% ─────────── (flat)\n   Validation plateaued. Need better model.",
        "codeLanguage": "python"
      },
      "tags": [
        "practical",
        "engineering",
        "learning",
        "curves"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "model-deployment",
      "name": "Model Deployment",
      "parentId": "model-deployment",
      "sectionId": "21.1.1",
      "level": 2,
      "fullExplanation": "Model deployment makes trained models available for predictions. Deployment patterns: batch (scheduled predictions on datasets), real-time API (serve individual predictions), edge (deploy to devices). Considerations: latency requirements, throughput, model size, infrastructure costs. Formats: ONNX, TensorRT, SavedModel. Serving: TensorFlow Serving, TorchServe, Triton, FastAPI.",
      "simpleExplanation": "Take your trained model and use it in the real world. Wrap it in an API, deploy to servers, respond to requests. Or run predictions on batches overnight. Or deploy to phones/sensors.",
      "example": {
        "description": "python\nfrom fastapi import FastAPI\nimport joblib\n\napp = FastAPI()\nmodel = joblib.load(\"model.pkl\")\n\n@app.post(\"/predict\")\ndef predict(features: dict):\n    prediction = model.predict([features])\n    return {\"prediction\": prediction}\n```\n```",
        "code": "Deployment patterns:\n\n1. Batch inference:\n   Every night: Load model → Process all new data → Save results\n   Use case: Email scoring, recommendation updates\n\n2. Real-time API:\n   User request → API → Model → Response (100ms)\n   Use case: Fraud detection, chatbots\n\n3. Edge deployment:\n   Model on device (phone, IoT)\n   Use case: Face unlock, voice assistant\n\nFastAPI example:",
        "codeLanguage": "python"
      },
      "tags": [
        "mlops",
        "deployment",
        "model"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "model-serving",
      "name": "Model Serving",
      "parentId": "model-deployment",
      "sectionId": "21.1.2",
      "level": 2,
      "fullExplanation": "Model serving infrastructure handles prediction requests at scale. Requirements: low latency, high throughput, model versioning, A/B testing, monitoring. Optimizations: batching requests, GPU utilization, model compression. Tools: TensorFlow Serving, TorchServe, Triton Inference Server, SageMaker. Containerization (Docker) enables consistent deployment. Load balancing distributes traffic.",
      "simpleExplanation": "The system that runs your model in production. Handle many requests per second, keep latency low, update models safely. Usually involves specialized servers optimized for ML inference.",
      "example": {
        "description": "",
        "code": "TensorFlow Serving:\n\n# Save model\nmodel.save('/models/my_model/1/')  # version 1\n\n# Deploy with TF Serving (Docker)\ndocker run -p 8501:8501 \\\n  -v /models:/models \\\n  -e MODEL_NAME=my_model \\\n  tensorflow/serving\n\n# Make prediction\ncurl -d '{\"instances\": [[1.0, 2.0, 3.0]]}' \\\n     -X POST http://localhost:8501/v1/models/my_model:predict\n\nFeatures:\n- Automatic batching\n- GPU support\n- Version management (/v1/, /v2/)\n- Health checks\n- Metrics (latency, throughput)",
        "codeLanguage": "python"
      },
      "tags": [
        "mlops",
        "deployment",
        "model",
        "serving"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "model-versioning",
      "name": "Model Versioning",
      "parentId": "model-deployment",
      "sectionId": "21.1.3",
      "level": 2,
      "fullExplanation": "Model versioning tracks model iterations, enabling rollback and comparison. Version: model code, hyperparameters, training data version, evaluation metrics, and artifacts. Tools: MLflow, DVC, Weights & Biases. Git for code, specialized tools for large files (models, data). Enables reproducibility, debugging, compliance. Blue-green deployment swaps versions safely.",
      "simpleExplanation": "Keep track of every model version. Which data was used? What were the metrics? If the new model fails, roll back to the previous version. Like git but for ML models.",
      "example": {
        "description": "",
        "code": "MLflow model versioning:\n\nimport mlflow\n\nwith mlflow.start_run():\n    # Log parameters\n    mlflow.log_param(\"learning_rate\", 0.01)\n    mlflow.log_param(\"epochs\", 100)\n\n    # Train model\n    model = train(...)\n\n    # Log metrics\n    mlflow.log_metric(\"accuracy\", 0.92)\n    mlflow.log_metric(\"f1\", 0.89)\n\n    # Log model\n    mlflow.sklearn.log_model(model, \"model\")\n\n# Track in registry\nmlflow.register_model(\n    \"runs:/abc123/model\",\n    \"production_model\"\n)\n\n# Transition stages\nmlflow.transition_model_version_stage(\n    name=\"production_model\",\n    version=3,\n    stage=\"Production\"\n)",
        "codeLanguage": "python"
      },
      "tags": [
        "mlops",
        "deployment",
        "model",
        "versioning"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "model-monitoring",
      "name": "Model Monitoring",
      "parentId": "monitoring",
      "sectionId": "21.2.1",
      "level": 2,
      "fullExplanation": "Model monitoring tracks production model performance over time. Monitor: prediction distributions, latency, error rates, feature distributions, ground truth (when available). Alerting on anomalies: prediction drift, data drift, performance degradation. Logging predictions for analysis. Dashboards for visualization. Enables proactive maintenance, triggering retraining when needed.",
      "simpleExplanation": "Watch your model in production. Is it still accurate? Are inputs changing? Is it getting slower? Catch problems before they affect users. Like application monitoring but for ML.",
      "example": {
        "description": "",
        "code": "Model monitoring dashboard:\n\nReal-time metrics:\n- Predictions/second: 150\n- P99 latency: 45ms\n- Error rate: 0.1%\n\nModel performance (daily):\n- Accuracy: 0.91 (baseline: 0.92) ⚠️\n- False positive rate: 0.05 (up from 0.03) ⚠️\n\nData monitoring:\n- Feature 'age' mean: 35.2 (baseline: 34.8) ✓\n- Feature 'income' missing: 5% (baseline: 2%) ⚠️\n- New category in 'region': \"EU-West\" detected ⚠️\n\nAlerts:\n[WARN] Accuracy dropped 1% - investigate\n[WARN] Feature 'income' missing rate increased\n[INFO] New category detected - may need retraining",
        "codeLanguage": "python"
      },
      "tags": [
        "mlops",
        "deployment",
        "model",
        "monitoring"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "data-drift",
      "name": "Data Drift",
      "parentId": "monitoring",
      "sectionId": "21.2.2",
      "level": 2,
      "fullExplanation": "Data drift occurs when input data distribution changes from training distribution. Causes: seasonal changes, population shifts, external events, upstream data issues. Types: feature drift (input distribution change), label drift (target distribution change), concept drift (relationship between features and target changes). Detection: statistical tests (KS, chi-squared), distribution comparison. Response: retrain, update thresholds.",
      "simpleExplanation": "The data your model sees changes over time. Trained on winter data, deployed in summer. Or user behavior shifts. If the model was built for different data, performance degrades. Detect drift early.",
      "example": {
        "description": "",
        "code": "Data drift detection:\n\nTraining data (2023):\n- Age distribution: mean=35, std=10\n- Income: mean=$60K, std=$20K\n- Category 'mobile': 60%\n\nProduction data (2024):\n- Age distribution: mean=28, std=12 ⚠️ Drift!\n- Income: mean=$58K, std=$22K ✓ OK\n- Category 'mobile': 75% ⚠️ Drift!\n\nKS test for age:\np-value = 0.001 (< 0.05) → Significant drift\n\nImpact:\n- Model trained on older users\n- Now seeing younger users\n- Model may underperform for new demographic\n\nActions:\n1. Investigate: Why younger users?\n2. Evaluate: Performance on new segment\n3. Retrain: With recent data",
        "codeLanguage": "python"
      },
      "tags": [
        "mlops",
        "deployment",
        "data",
        "drift"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "model-retraining",
      "name": "Model Retraining",
      "parentId": "monitoring",
      "sectionId": "21.2.3",
      "level": 2,
      "fullExplanation": "Model retraining updates models with new data to maintain performance. Triggers: scheduled (weekly, monthly), performance degradation, data drift detection, business requirements. Strategies: full retrain, incremental/online learning, fine-tuning. Validation before deployment: compare to baseline, A/B test. Automated pipelines (CI/CD for ML) enable frequent safe retraining.",
      "simpleExplanation": "Models go stale—data changes, world changes. Retrain regularly or when performance drops. Use recent data, validate thoroughly, deploy carefully. Automate the pipeline for consistent updates.",
      "example": {
        "description": "",
        "code": "Automated retraining pipeline:\n\nTrigger: Weekly schedule OR drift alert OR metric drop\n\nPipeline:\n1. Data preparation\n   - Collect last 90 days of data\n   - Join with labels\n   - Feature engineering\n\n2. Training\n   - Train new model\n   - Track with MLflow\n\n3. Evaluation\n   - Compare to current production model\n   - Must beat baseline by >0.5%\n\n4. Validation\n   - Test on held-out recent data\n   - Check for biases\n\n5. Deployment\n   - Shadow mode first (parallel predictions)\n   - Gradual rollout (10% → 50% → 100%)\n   - Automatic rollback if metrics drop\n\n6. Monitoring\n   - Watch new model performance\n   - Compare to previous version",
        "codeLanguage": "python"
      },
      "tags": [
        "mlops",
        "deployment",
        "model",
        "retraining"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "algorithmic-bias",
      "name": "Algorithmic Bias",
      "parentId": "bias-fairness",
      "sectionId": "22.1.1",
      "level": 2,
      "fullExplanation": "Algorithmic bias occurs when ML systems produce unfair outcomes for certain groups. Sources: biased training data (historical discrimination), biased labels (human prejudice), biased features (proxies for protected attributes), biased evaluation (not measuring on all groups). Types: disparate treatment (explicit use of protected attributes), disparate impact (neutral features causing unfair outcomes). Requires proactive mitigation.",
      "simpleExplanation": "AI systems can be unfair, even accidentally. Trained on biased history, they repeat biases. Hiring algorithms that prefer men. Loan algorithms that reject minorities. Even without using \"race\" as a feature, other features can be proxies.",
      "example": {
        "description": "",
        "code": "Bias sources:\n\nHistorical bias:\nTraining data: Past hiring decisions (90% male engineers hired)\nModel learns: Prefer male candidates\nEven without \"gender\" feature: prefers male-associated schools, activities\n\nMeasurement bias:\nTraining data: Arrest records (more policing in minority areas)\nModel learns: Minority areas are \"high crime\"\nReality: More arrests ≠ more crime, just more policing\n\nRepresentation bias:\nTraining data: Mostly light-skinned faces\nModel: Worse performance on darker skin\nImpact: Facial recognition fails for minorities\n\nAggregation bias:\nOne model for everyone\nBut relationship differs by group\nModel good for majority, poor for minorities",
        "codeLanguage": "python"
      },
      "tags": [
        "ethics",
        "fairness",
        "algorithmic",
        "bias"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "fairness-metrics",
      "name": "Fairness Metrics",
      "parentId": "bias-fairness",
      "sectionId": "22.1.2",
      "level": 2,
      "fullExplanation": "Fairness metrics quantify bias across groups. Demographic parity: equal positive prediction rates across groups. Equalized odds: equal TPR and FPR across groups. Predictive parity: equal precision across groups. Individual fairness: similar individuals treated similarly. These metrics can conflict—satisfying all simultaneously is often mathematically impossible. Choose metrics based on application context.",
      "simpleExplanation": "How do you measure if a model is fair? Many definitions exist. Equal acceptance rates? Equal accuracy? Equal error rates? Different metrics capture different notions of fairness. Often can't satisfy all.",
      "example": {
        "description": "",
        "code": "Fairness metrics for loan approval:\n\nGroups: Group A (majority), Group B (minority)\n\nDemographic Parity:\nP(Approved | A) = P(Approved | B)\n\"Equal approval rates regardless of group\"\nA: 70% approved, B: 65% approved → Unfair\n\nEqualized Odds:\nP(Approved | Qualified, A) = P(Approved | Qualified, B)\nP(Approved | Unqualified, A) = P(Approved | Unqualified, B)\n\"Equal TPR and FPR across groups\"\n\nPredictive Parity:\nP(Repays | Approved, A) = P(Repays | Approved, B)\n\"Approval means same thing for both groups\"\n\nTrade-off:\nCannot have demographic parity AND predictive parity\nif base rates differ (different qualification rates)",
        "codeLanguage": "python"
      },
      "tags": [
        "ethics",
        "fairness",
        "metrics"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "bias-mitigation",
      "name": "Bias Mitigation",
      "parentId": "bias-fairness",
      "sectionId": "22.1.3",
      "level": 2,
      "fullExplanation": "Bias mitigation techniques reduce unfairness at different pipeline stages. Pre-processing: reweighting, resampling, transforming data. In-processing: constrained optimization, adversarial debiasing, fair representations. Post-processing: threshold adjustment per group, outcome calibration. Trade-offs: fairness vs accuracy, different fairness definitions. Requires iterative approach: measure, mitigate, evaluate.",
      "simpleExplanation": "Fix bias at different stages. Before training: fix the data. During training: add fairness constraints. After training: adjust predictions. No perfect solution—requires careful trade-offs and ongoing monitoring.",
      "example": {
        "description": "",
        "code": "Bias mitigation strategies:\n\nPre-processing:\n- Reweighting: Increase minority sample weights\n- Oversampling: Generate more minority examples\n- Data augmentation: Balance representation\n\nIn-processing:\n- Add fairness constraint to loss function:\n  L = accuracy_loss + λ × fairness_penalty\n- Adversarial debiasing:\n  Learn features that don't predict protected attribute\n- Fair representation learning:\n  Encode inputs to be useful but fair\n\nPost-processing:\n- Separate thresholds per group:\n  Group A: P > 0.6 → Approve\n  Group B: P > 0.5 → Approve (different threshold)\n- Calibrated to equalize positive rates\n\nEvaluation:\n- Check multiple fairness metrics\n- Check accuracy for all groups\n- Monitor in production",
        "codeLanguage": "python"
      },
      "tags": [
        "ethics",
        "fairness",
        "bias",
        "mitigation"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "explainability",
      "name": "Explainability",
      "parentId": "responsible-ai",
      "sectionId": "22.2.1",
      "level": 2,
      "fullExplanation": "Explainability makes model predictions understandable. Global explanations: overall model behavior (feature importance, decision boundaries). Local explanations: individual prediction reasons (LIME, SHAP). Intrinsically interpretable models: linear regression, decision trees. Post-hoc explanations for complex models. Important for trust, debugging, compliance, and stakeholder communication.",
      "simpleExplanation": "Why did the model make this prediction? Feature X was most important. This input pattern led to this output. Essential for trust, especially in high-stakes decisions like healthcare or lending.",
      "example": {
        "description": "",
        "code": "SHAP explanation for loan denial:\n\nPrediction: Denied (P=0.25 vs threshold 0.5)\n\nFeature contributions:\nBase rate: 0.50 (average approval probability)\n\nIncome: -0.15 (below average income)\nCredit score: -0.08 (slightly low)\nEmployment: +0.05 (stable job)\nDebt ratio: -0.12 (high debt)\nAge: +0.05 (established credit history)\n\nFinal: 0.50 - 0.15 - 0.08 + 0.05 - 0.12 + 0.05 = 0.25\n\nExplanation: \"Denied primarily due to\nbelow-average income (-0.15) and\nhigh debt ratio (-0.12)\"\n\nActionable: \"Reduce debt or increase income to improve chances\"",
        "codeLanguage": "python"
      },
      "tags": [
        "ethics",
        "fairness",
        "explainability"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "privacy-ml",
      "name": "Privacy in ML",
      "parentId": "responsible-ai",
      "sectionId": "22.2.2",
      "level": 2,
      "fullExplanation": "Privacy-preserving ML protects sensitive data throughout the pipeline. Techniques: differential privacy (noise addition limiting individual exposure), federated learning (train on distributed data without centralizing), secure multi-party computation (compute on encrypted data), model privacy (prevent extraction of training data from models). Regulations: GDPR, CCPA. Trade-offs: privacy vs utility.",
      "simpleExplanation": "Protect sensitive data used in ML. Don't memorize individual examples. Don't leak information through predictions. Train without seeing raw data. Important for healthcare, finance, and user data.",
      "example": {
        "description": "",
        "code": "Privacy techniques:\n\n1. Differential Privacy:\n   Add noise to training or outputs\n   Guarantee: Any individual's data has limited impact\n   ε (epsilon): Privacy budget (lower = more private)\n\n2. Federated Learning:\n   Data stays on devices (phones)\n   Only model updates sent to server\n   Server never sees raw data\n\n   Phone 1: Train local model → Send gradients\n   Phone 2: Train local model → Send gradients\n   Server: Aggregate gradients → Update global model\n\n3. Membership Inference Protection:\n   Attack: \"Was this person in training data?\"\n   Defense: Regularization, differential privacy\n\n4. Model extraction defense:\n   Prevent copying model via query access\n   Watermarking, query limits",
        "codeLanguage": "python"
      },
      "tags": [
        "ethics",
        "fairness",
        "privacy"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "ai-safety",
      "name": "AI Safety",
      "parentId": "responsible-ai",
      "sectionId": "22.2.3",
      "level": 2,
      "fullExplanation": "AI safety ensures systems behave as intended without causing harm. Concerns: specification gaming (optimizing proxy instead of true objective), distributional shift (failure on unseen inputs), adversarial attacks (malicious inputs causing failures), emergent behaviors (unexpected capabilities at scale). Approaches: robustness testing, red-teaming, formal verification, RLHF alignment, interpretability research. Increasingly important as AI capabilities grow.",
      "simpleExplanation": "Make sure AI does what we want, not just what we said. Systems can find loopholes, fail unexpectedly, or be manipulated. Test thoroughly, consider failure modes, design for safety.",
      "example": {
        "description": "",
        "code": "AI safety concerns:\n\n1. Specification Gaming:\n   Goal: \"Maximize user engagement\"\n   Learned: \"Show outrage-inducing content\"\n   Problem: Optimized metric, not true intent\n\n2. Distributional Shift:\n   Training: Normal conditions\n   Deployment: Edge cases, adversarial inputs\n   Result: Unexpected failures\n\n3. Adversarial Examples:\n   Small perturbation to input\n   → Completely wrong output\n   Example: Sticker on stop sign → \"Speed limit 45\"\n\n4. Emergent Capabilities:\n   Large models develop unexpected abilities\n   May include capabilities we didn't intend\n\nSafety measures:\n- Red-teaming: Attack your own system\n- Robustness testing: Edge cases, stress tests\n- Human oversight: Keep humans in the loop\n- Alignment research: Ensure AI goals match human values",
        "codeLanguage": "python"
      },
      "tags": [
        "ethics",
        "fairness",
        "safety"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "gdpr-ml",
      "name": "GDPR and ML",
      "parentId": "regulatory-compliance",
      "sectionId": "22.3.1",
      "level": 2,
      "fullExplanation": "GDPR (General Data Protection Regulation) affects ML through data rights and automated decision requirements. Key provisions: right to explanation for automated decisions, right to contest, data minimization, purpose limitation, consent requirements, right to deletion (may require retraining). ML implications: document data usage, provide explanations, enable data deletion, justify automated decisions, privacy by design.",
      "simpleExplanation": "European privacy law affects ML. You must explain automated decisions. Users can request their data be deleted. Can't use data for purposes they didn't consent to. Design systems with privacy in mind.",
      "example": {
        "description": "",
        "code": "GDPR compliance for ML:\n\n1. Automated Decision Making (Article 22):\n   If decision has significant impact:\n   - User can request human review\n   - Must explain logic of decision\n   → Maintain explainability, human oversight\n\n2. Right to Erasure (Article 17):\n   User requests data deletion\n   → Retrain model without their data?\n   → Machine unlearning techniques\n\n3. Purpose Limitation (Article 5):\n   Data collected for \"customer service\"\n   → Cannot use for \"credit scoring\" without consent\n\n4. Data Minimization:\n   Only collect necessary features\n   → Feature selection, not just accuracy\n\n5. Documentation:\n   - What data is used\n   - Why decisions are made\n   - How model works\n   → Model cards, data sheets",
        "codeLanguage": "python"
      },
      "tags": [
        "ethics",
        "fairness",
        "gdpr"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "model-documentation",
      "name": "Model Documentation",
      "parentId": "regulatory-compliance",
      "sectionId": "22.3.2",
      "level": 2,
      "fullExplanation": "Model documentation records essential information for accountability and governance. Model cards: intended use, limitations, performance across groups, ethical considerations. Data sheets: data collection, preprocessing, known biases. Required for regulatory compliance, reproducibility, and responsible deployment. Standardized formats emerging (Google Model Cards, Microsoft Datasheets for Datasets).",
      "simpleExplanation": "Document everything about your model. What is it for? What are its limits? How well does it work for different groups? Essential for responsible AI and often legally required.",
      "example": {
        "description": "```\nModel Card Example:\n\nMODEL DETAILS\n- Name: Customer Churn Predictor v2.1\n- Type: Gradient Boosted Classifier\n- Date: 2024-01-15\n- Owner: Data Science Team\n\nINTENDED USE\n- Predict customer churn probability\n- Used by retention team for outreach\n- NOT for: automated service cancellation\n\nTRAINING DATA\n- 500K customers, 2022-2023\n- Demographics: US customers only\n- Known bias: Underrepresents rural customers\n\nPERFORMANCE\nOverall accuracy: 0.85\n\n| Group | Accuracy | FPR | FNR |\n|"
      },
      "tags": [
        "ethics",
        "fairness",
        "model",
        "documentation"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "linear-models",
      "name": "Linear Models",
      "parentId": "supervised-regression",
      "sectionId": "7.1",
      "level": 1,
      "fullExplanation": "Linear Models covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about linear models and related techniques.",
      "example": {
        "description": "This section contains concepts related to linear models."
      },
      "tags": [
        "regression",
        "supervised",
        "linear",
        "models"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "kernel-methods",
      "name": "Kernel Methods",
      "parentId": "supervised-regression",
      "sectionId": "7.2",
      "level": 1,
      "fullExplanation": "Kernel Methods covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about kernel methods and related techniques.",
      "example": {
        "description": "This section contains concepts related to kernel methods."
      },
      "tags": [
        "regression",
        "supervised",
        "kernel",
        "methods"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "tree-based-regression",
      "name": "Tree-Based Regression",
      "parentId": "supervised-regression",
      "sectionId": "7.3",
      "level": 1,
      "fullExplanation": "Tree-Based Regression covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about tree-based regression and related techniques.",
      "example": {
        "description": "This section contains concepts related to tree-based regression."
      },
      "tags": [
        "regression",
        "supervised",
        "tree",
        "based"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "instance-based-regression",
      "name": "Instance-Based Regression",
      "parentId": "supervised-regression",
      "sectionId": "7.4",
      "level": 1,
      "fullExplanation": "Instance-Based Regression covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about instance-based regression and related techniques.",
      "example": {
        "description": "This section contains concepts related to instance-based regression."
      },
      "tags": [
        "regression",
        "supervised",
        "instance",
        "based"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "linear-classifiers",
      "name": "Linear Classifiers",
      "parentId": "supervised-classification",
      "sectionId": "8.1",
      "level": 1,
      "fullExplanation": "Linear Classifiers covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about linear classifiers and related techniques.",
      "example": {
        "description": "This section contains concepts related to linear classifiers."
      },
      "tags": [
        "classification",
        "supervised",
        "linear",
        "classifiers"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "support-vector-machines",
      "name": "Support Vector Machines",
      "parentId": "supervised-classification",
      "sectionId": "8.2",
      "level": 1,
      "fullExplanation": "Support Vector Machines covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about support vector machines and related techniques.",
      "example": {
        "description": "This section contains concepts related to support vector machines."
      },
      "tags": [
        "classification",
        "supervised",
        "support",
        "vector",
        "machines"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "probabilistic-classifiers",
      "name": "Probabilistic Classifiers",
      "parentId": "supervised-classification",
      "sectionId": "8.3",
      "level": 1,
      "fullExplanation": "Probabilistic Classifiers covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about probabilistic classifiers and related techniques.",
      "example": {
        "description": "This section contains concepts related to probabilistic classifiers."
      },
      "tags": [
        "classification",
        "supervised",
        "probabilistic",
        "classifiers"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "instance-based-classification",
      "name": "Instance-Based Classification",
      "parentId": "supervised-classification",
      "sectionId": "8.4",
      "level": 1,
      "fullExplanation": "Instance-Based Classification covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about instance-based classification and related techniques.",
      "example": {
        "description": "This section contains concepts related to instance-based classification."
      },
      "tags": [
        "classification",
        "supervised",
        "instance",
        "based"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "decision-trees",
      "name": "Decision Trees",
      "parentId": "supervised-classification",
      "sectionId": "8.5",
      "level": 1,
      "fullExplanation": "Decision Trees covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about decision trees and related techniques.",
      "example": {
        "description": "This section contains concepts related to decision trees."
      },
      "tags": [
        "classification",
        "supervised",
        "decision",
        "trees"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "linear-regression",
      "name": "Linear Regression",
      "parentId": "linear-models",
      "sectionId": "7.1.1",
      "level": 2,
      "fullExplanation": "Linear Regression models the relationship between a dependent variable and one or more independent variables by fitting a linear equation. The model assumes: y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε, where β are coefficients and ε is the error term. Parameters are typically estimated using Ordinary Least Squares (OLS), minimizing the sum of squared residuals. Assumes linearity, independence, homoscedasticity, and normally distributed errors.",
      "simpleExplanation": "Draw the best straight line through your data points. The line predicts the output based on inputs. If you know square footage, you can predict house price by following the line. Simple, interpretable, and often surprisingly effective.",
      "example": {
        "description": "Predicting house price from square footage:\n- Data: [(1000 sqft, $200K), (1500 sqft, $300K), (2000 sqft, $400K)]\n- Model learns: Price = $100 × sqft + $100K\n- New house 1800 sqft → Predicted price = $100 × 1800 + $100K = $280K",
        "code": "from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)",
        "codeLanguage": "python"
      },
      "tags": [
        "regression",
        "supervised",
        "linear"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "multiple-linear-regression",
      "name": "Multiple Linear Regression",
      "parentId": "linear-models",
      "sectionId": "7.1.2",
      "level": 2,
      "fullExplanation": "Multiple Linear Regression extends simple linear regression to multiple predictor variables: y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ. Each coefficient βᵢ represents the change in y for a one-unit change in xᵢ, holding all other variables constant. Enables modeling complex relationships while maintaining interpretability. Requires checking for multicollinearity between predictors.",
      "simpleExplanation": "Use multiple inputs to make a prediction. Instead of just square footage, use square footage AND bedrooms AND location to predict price. Each factor contributes independently to the final prediction.",
      "example": {
        "description": "House price with multiple features:\n- Price = $50×sqft + $10,000×bedrooms - $5,000×age + $20,000×(good_school_district)\n- 1500 sqft, 3 bed, 10 years old, good schools:\n- Price = $50×1500 + $10K×3 + (-$5K×10) + $20K = $75K + $30K - $50K + $20K = $125K"
      },
      "tags": [
        "regression",
        "supervised",
        "multiple",
        "linear"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "polynomial-regression",
      "name": "Polynomial Regression",
      "parentId": "linear-models",
      "sectionId": "7.1.3",
      "level": 2,
      "fullExplanation": "Polynomial Regression fits a polynomial equation to the data: y = β₀ + β₁x + β₂x² + ... + βₙxⁿ. Despite the nonlinear relationship with x, it's still linear in parameters (coefficients), so linear regression techniques apply. Higher-degree polynomials can fit complex curves but risk overfitting. Degree selection is crucial—use cross-validation.",
      "simpleExplanation": "When a straight line doesn't fit, use a curve. Add x², x³, etc. as features. A degree-2 polynomial can fit a parabola. Higher degrees fit more complex shapes but may overfit.",
      "example": {
        "description": "Data follows a parabola (throwing a ball):\n- Height = -5t² + 20t + 1 (physics equation)\n- Linear regression: Poor fit (straight line through curve)\n- Polynomial degree 2: Perfect fit (captures the parabola)\n- Polynomial degree 10: Overfits (wiggles through every point)",
        "code": "from sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree=2)\nX_poly = poly.fit_transform(X)  # Creates [1, x, x²]\nmodel.fit(X_poly, y)",
        "codeLanguage": "python"
      },
      "tags": [
        "regression",
        "supervised",
        "polynomial"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "ridge-regression",
      "name": "Ridge Regression",
      "parentId": "linear-models",
      "sectionId": "7.1.4",
      "level": 2,
      "fullExplanation": "Ridge Regression (L2 regularization) adds a penalty term to the loss function: Loss = Σ(yᵢ - ŷᵢ)² + λΣβⱼ². The penalty shrinks coefficients toward zero, reducing variance at the cost of introducing bias. Particularly effective when predictors are correlated (multicollinearity) or when there are more features than observations. λ controls regularization strength.",
      "simpleExplanation": "Linear regression that keeps weights small. Adds a penalty for large coefficients, preventing the model from relying too heavily on any single feature. Good when features are correlated or you have many features.",
      "example": {
        "description": "Predicting with correlated features (height in cm AND height in inches):\n- Regular regression: Unstable coefficients, one huge positive, one huge negative\n- Ridge regression: Both coefficients moderate, model more stable",
        "code": "from sklearn.linear_model import Ridge\nmodel = Ridge(alpha=1.0)  # alpha is λ\nmodel.fit(X_train, y_train)",
        "codeLanguage": "python"
      },
      "tags": [
        "regression",
        "supervised",
        "ridge"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "lasso-regression",
      "name": "Lasso Regression",
      "parentId": "linear-models",
      "sectionId": "7.1.5",
      "level": 2,
      "fullExplanation": "Lasso Regression (Least Absolute Shrinkage and Selection Operator) uses L1 regularization: Loss = Σ(yᵢ - ŷᵢ)² + λΣ|βⱼ|. Unlike Ridge, Lasso can shrink coefficients exactly to zero, performing automatic feature selection. Useful when you suspect many features are irrelevant. The L1 penalty creates sparse models.",
      "simpleExplanation": "Linear regression that eliminates useless features. The L1 penalty drives some coefficients to exactly zero, effectively removing those features. Great for feature selection—tells you which features actually matter.",
      "example": {
        "description": "100 features, but only 10 actually matter:\n- Regular regression: All 100 have non-zero coefficients\n- Lasso: Only 10-15 features have non-zero coefficients, rest are exactly 0\n- Interpretation: \"These 10 features drive the prediction\"",
        "code": "from sklearn.linear_model import Lasso\nmodel = Lasso(alpha=0.1)\nmodel.fit(X_train, y_train)\n# Check which features were selected\nimportant_features = X.columns[model.coef_ != 0]",
        "codeLanguage": "python"
      },
      "tags": [
        "regression",
        "supervised",
        "lasso"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "elastic-net",
      "name": "Elastic Net",
      "parentId": "linear-models",
      "sectionId": "7.1.6",
      "level": 2,
      "fullExplanation": "Elastic Net combines L1 and L2 regularization: Loss = Σ(yᵢ - ŷᵢ)² + λ₁Σ|βⱼ| + λ₂Σβⱼ². It inherits both feature selection from Lasso and coefficient shrinkage from Ridge. Particularly useful when features are correlated—Lasso might arbitrarily select one, while Elastic Net tends to keep correlated features together. Two hyperparameters: overall regularization and L1/L2 ratio.",
      "simpleExplanation": "Best of both Ridge and Lasso. Gets feature selection (some zeros) AND keeps correlated features together. Use when you want sparsity but features might be related.",
      "example": {
        "description": "Gene expression data: 10,000 genes, many correlated in pathways\n- Lasso: Picks one gene from each pathway arbitrarily\n- Elastic Net: Keeps groups of related genes together, still sparse",
        "code": "from sklearn.linear_model import ElasticNet\nmodel = ElasticNet(alpha=1.0, l1_ratio=0.5)  # 50% L1, 50% L2\nmodel.fit(X_train, y_train)",
        "codeLanguage": "python"
      },
      "tags": [
        "regression",
        "supervised",
        "elastic"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "svr",
      "name": "Support Vector Regression (SVR)",
      "parentId": "kernel-methods",
      "sectionId": "7.2.1",
      "level": 2,
      "fullExplanation": "Support Vector Regression applies SVM principles to regression. It finds a function that deviates from actual targets by at most ε (epsilon) for all training data, while being as flat as possible. Points within the ε-tube don't contribute to the loss (ε-insensitive loss). Support vectors are points at or outside the tube boundaries. Kernels enable nonlinear regression.",
      "simpleExplanation": "Regression using SVM concepts. Instead of minimizing all errors, it ignores small errors (within a tolerance tube) and focuses on keeping the function smooth. Points that define the tube edges are \"support vectors.\"",
      "example": {
        "description": "Predicting stock prices with tolerance:\n- ε = $0.50 (we don't care about errors smaller than 50 cents)\n- SVR finds a smooth curve where most points are within $0.50\n- Only points outside the tube affect the model\n- Result: Robust to small noise, smooth predictions",
        "code": "from sklearn.svm import SVR\nmodel = SVR(kernel='rbf', epsilon=0.1, C=100)\nmodel.fit(X_train, y_train)",
        "codeLanguage": "python"
      },
      "tags": [
        "regression",
        "supervised",
        "support",
        "vector",
        "(svr)"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "kernel-trick",
      "name": "Kernel Trick",
      "parentId": "kernel-methods",
      "sectionId": "7.2.2",
      "level": 2,
      "fullExplanation": "The kernel trick enables algorithms to operate in high-dimensional feature spaces without explicitly computing the transformation. Instead of mapping x → φ(x) and computing φ(x)ᵀφ(y), we compute k(x,y) = φ(x)ᵀφ(y) directly. This allows learning nonlinear patterns with linear algorithms. Common kernels: RBF (Gaussian), polynomial, sigmoid.",
      "simpleExplanation": "A mathematical shortcut to work in higher dimensions without the cost. Imagine transforming 2D data to 1000D to find a pattern—expensive! The kernel trick gets the same result without actually doing the transformation.",
      "example": {
        "description": "XOR problem (not linearly separable in 2D):\n- Original: [(0,0)→0, (0,1)→1, (1,0)→1, (1,1)→0]\n- Cannot draw a straight line to separate\n\nWith polynomial kernel (implicitly maps to higher dimension):\n- Kernel computes similarity as if data were in higher dimension\n- Linear separator found in implicit high-dimensional space\n- Nonlinear boundary in original 2D space"
      },
      "tags": [
        "regression",
        "supervised",
        "kernel",
        "trick"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "rbf-kernel",
      "name": "RBF Kernel",
      "parentId": "kernel-methods",
      "sectionId": "7.2.3",
      "level": 2,
      "fullExplanation": "The Radial Basis Function (RBF) kernel, also called Gaussian kernel, measures similarity as: k(x,y) = exp(-γ||x-y||²). Points close together have similarity near 1; distant points have similarity near 0. The γ parameter controls the kernel's reach—higher γ means tighter, more local influence. RBF kernel can approximate any continuous function given enough data.",
      "simpleExplanation": "Similarity based on distance—close points are similar, far points are different. Like a spotlight: each point illuminates its neighborhood. γ controls spotlight width—small γ = wide reach, large γ = focused locally.",
      "example": {
        "description": "Two points in 2D:\n- x = (0, 0), y = (1, 1)\n- Distance ||x-y||² = 2\n- With γ = 0.5: k(x,y) = exp(-0.5 × 2) = 0.37\n- With γ = 2.0: k(x,y) = exp(-2 × 2) = 0.02\n\nHigher γ makes distant points less similar (tighter kernel)."
      },
      "tags": [
        "regression",
        "supervised",
        "kernel"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "decision-tree-regressor",
      "name": "Decision Tree Regressor",
      "parentId": "tree-based-regression",
      "sectionId": "7.3.1",
      "level": 2,
      "fullExplanation": "Decision Tree Regressor recursively partitions the feature space into regions and predicts the mean target value within each region. Splits are chosen to minimize variance (or MSE) in resulting regions. Trees are interpretable and handle nonlinear relationships naturally. Prone to overfitting without pruning or depth limits. Predictions are piecewise constant.",
      "simpleExplanation": "A flowchart that makes predictions. \"Is square footage > 1500? If yes, is it in a good neighborhood? If yes, predict $400K.\" Each leaf contains an average value from training data in that region.",
      "example": {
        "description": "Predicting house prices:\n\n\nNew house: 1800 sqft, 4 bedrooms → Follow: Yes → Yes → Predict $450K",
        "code": "[sqft > 1500?]\n                    /            \\\n                 No               Yes\n                 /                  \\\n        [age > 20?]           [bedrooms > 3?]\n         /      \\               /         \\\n      $150K    $200K        $350K       $450K",
        "codeLanguage": "python"
      },
      "tags": [
        "regression",
        "supervised",
        "decision",
        "tree",
        "regressor"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "regression-tree-splitting",
      "name": "Regression Tree Splitting",
      "parentId": "tree-based-regression",
      "sectionId": "7.3.2",
      "level": 2,
      "fullExplanation": "Regression trees split nodes to minimize prediction error, typically measured by MSE or MAE. For each feature, the algorithm considers all possible split points and selects the one that minimizes the weighted average variance of resulting child nodes. This greedy approach finds locally optimal splits. Stopping criteria include minimum samples, maximum depth, or minimum impurity decrease.",
      "simpleExplanation": "Try every possible way to split the data, pick the one that groups similar target values together. \"Splitting at sqft=1500 creates groups with less price variation than splitting at sqft=1200.\"",
      "example": {
        "description": "Data: Houses with prices [$100K, $150K, $400K, $450K]\nFeatures: sqft [1000, 1200, 1800, 2000]\n\nTrying splits:\n- Split at sqft=1100: Left [$100K], Right [$150K, $400K, $450K]\n  - Variance: Left=0, Right=high → Not great\n- Split at sqft=1500: Left [$100K, $150K], Right [$400K, $450K]\n  - Variance: Left=low, Right=low → Good split!\n\nChoose sqft=1500 because it creates more homogeneous groups."
      },
      "tags": [
        "regression",
        "supervised",
        "tree",
        "splitting"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "knn-regression",
      "name": "K-Nearest Neighbors Regression",
      "parentId": "instance-based-regression",
      "sectionId": "7.4.1",
      "level": 2,
      "fullExplanation": "KNN Regression predicts the target value as the average (or weighted average) of the K nearest training examples. Distance is typically Euclidean, but other metrics work. No explicit training phase—all computation happens at prediction time. Sensitive to feature scaling and curse of dimensionality. K controls bias-variance: small K = low bias, high variance; large K = high bias, low variance.",
      "simpleExplanation": "To predict, find the K most similar examples and average their values. Predicting a house price? Find the 5 most similar houses that sold, average their prices. Simple, intuitive, no training needed.",
      "example": {
        "description": "Predict price for 1600 sqft house, using K=3:\n\nTraining data:\n- 1500 sqft → $300K (distance: 100)\n- 1550 sqft → $310K (distance: 50)\n- 1650 sqft → $330K (distance: 50)\n- 2000 sqft → $450K (distance: 400)\n\n3 nearest: 1550, 1650, 1500 sqft houses\nPrediction: ($310K + $330K + $300K) / 3 = $313K",
        "code": "from sklearn.neighbors import KNeighborsRegressor\nmodel = KNeighborsRegressor(n_neighbors=5)\nmodel.fit(X_train, y_train)",
        "codeLanguage": "python"
      },
      "tags": [
        "regression",
        "supervised",
        "nearest",
        "neighbors"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "weighted-knn",
      "name": "Weighted KNN",
      "parentId": "instance-based-regression",
      "sectionId": "7.4.2",
      "level": 2,
      "fullExplanation": "Weighted KNN assigns higher influence to closer neighbors. Common weighting: inverse distance (w = 1/d) or inverse squared distance (w = 1/d²). Closer neighbors contribute more to the prediction than farther ones. Reduces the impact of K selection—even with large K, distant neighbors have minimal influence. Smooths predictions compared to uniform weighting.",
      "simpleExplanation": "Closer neighbors count more. Instead of equal votes, a house 10 feet away has more influence than one 100 feet away. Closer = more similar = should matter more.",
      "example": {
        "description": "Predict price, K=3 with distance weighting:\n\nNeighbors:\n- 1550 sqft → $310K, distance=50, weight=1/50=0.02\n- 1650 sqft → $330K, distance=50, weight=1/50=0.02\n- 1500 sqft → $300K, distance=100, weight=1/100=0.01\n\nWeighted average:\n= (0.02×$310K + 0.02×$330K + 0.01×$300K) / (0.02+0.02+0.01)\n= ($6.2K + $6.6K + $3K) / 0.05\n= $316K"
      },
      "tags": [
        "regression",
        "supervised",
        "weighted"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "logistic-regression",
      "name": "Logistic Regression",
      "parentId": "linear-classifiers",
      "sectionId": "8.1.1",
      "level": 2,
      "fullExplanation": "Logistic Regression models the probability of a binary outcome using the logistic (sigmoid) function: P(y=1|x) = 1/(1+e^(-z)) where z = β₀ + β₁x₁ + ... + βₙxₙ. Output is a probability between 0 and 1. Decision boundary is linear in feature space. Trained by maximizing likelihood (minimizing log loss). Despite its name, it's a classification algorithm.",
      "simpleExplanation": "Predict the probability of something being true. Instead of predicting a number, predict \"80% chance this email is spam.\" The sigmoid function squashes any value into 0-1 range, giving you a probability.",
      "example": {
        "description": "Email spam classification:\n- Features: word counts, sender reputation, etc.\n- Model outputs: P(spam) = 0.85\n- Threshold 0.5: Predict spam (0.85 > 0.5)",
        "code": "from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\nprobabilities = model.predict_proba(X_test)  # Get probabilities\npredictions = model.predict(X_test)  # Get class labels",
        "codeLanguage": "python"
      },
      "tags": [
        "classification",
        "supervised",
        "logistic",
        "regression"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "sigmoid-function",
      "name": "Sigmoid Function",
      "parentId": "linear-classifiers",
      "sectionId": "8.1.2",
      "level": 2,
      "fullExplanation": "The sigmoid function σ(z) = 1/(1+e^(-z)) maps any real number to the (0,1) interval. Properties: σ(0) = 0.5, σ(-∞) → 0, σ(+∞) → 1. Smooth and differentiable everywhere, enabling gradient-based optimization. Derivative: σ'(z) = σ(z)(1-σ(z)). Used in logistic regression and as activation function in neural networks.",
      "simpleExplanation": "An S-shaped curve that squashes any number into 0-1. Big positive numbers become close to 1, big negative numbers become close to 0, and 0 maps to 0.5. Perfect for converting scores to probabilities.",
      "example": {
        "description": "```\nInput z:  -5    -2     0    +2    +5\nOutput:  0.01  0.12  0.50  0.88  0.99\n\nGraph:\n1.0 |              ****\n    |           ***\n0.5 |        **\n    |     ***\n0.0 |*****"
      },
      "tags": [
        "classification",
        "supervised",
        "sigmoid",
        "function"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "softmax-function",
      "name": "Softmax Function",
      "parentId": "linear-classifiers",
      "sectionId": "8.1.3",
      "level": 2,
      "fullExplanation": "Softmax generalizes sigmoid to multiple classes, converting a vector of real numbers into a probability distribution. For class i: P(i) = e^(zᵢ) / Σⱼe^(zⱼ). All outputs sum to 1 and are positive. Larger input values get higher probabilities. Used as the final layer in multiclass classification neural networks. Combined with cross-entropy loss for training.",
      "simpleExplanation": "Turn a list of scores into probabilities that sum to 1. If a model outputs [2, 1, 0] for three classes, softmax converts to [0.67, 0.24, 0.09]—class 1 is most likely, but you still see relative confidence in others.",
      "example": {
        "description": "Model outputs (logits): [2.0, 1.0, 0.5]\n\nSoftmax calculation:\n- e^2.0 = 7.39\n- e^1.0 = 2.72\n- e^0.5 = 1.65\n- Sum = 11.76\n\nProbabilities:\n- P(class 0) = 7.39/11.76 = 0.63\n- P(class 1) = 2.72/11.76 = 0.23\n- P(class 2) = 1.65/11.76 = 0.14\n\nSum = 1.0 ✓"
      },
      "tags": [
        "classification",
        "supervised",
        "softmax",
        "function"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "multiclass-classification",
      "name": "Multiclass Classification",
      "parentId": "linear-classifiers",
      "sectionId": "8.1.4",
      "level": 2,
      "fullExplanation": "Multiclass classification assigns instances to one of more than two classes. Approaches: (1) One-vs-Rest (OvR): Train K binary classifiers, each separating one class from all others. (2) One-vs-One (OvO): Train K(K-1)/2 classifiers for each pair of classes. (3) Native multiclass: Some algorithms naturally handle multiple classes (softmax regression, decision trees, naive Bayes).",
      "simpleExplanation": "Predict one category from several options (not just yes/no). Examples: classifying images as cat/dog/bird, or emails as work/personal/spam. Different strategies exist to extend binary classifiers to multiple classes.",
      "example": {
        "description": "Classifying animals into 3 classes: Cat, Dog, Bird\n\nOne-vs-Rest approach:\n- Classifier 1: Cat vs (Dog+Bird)\n- Classifier 2: Dog vs (Cat+Bird)\n- Classifier 3: Bird vs (Cat+Dog)\n\nNew image → Run all 3 classifiers → Pick highest confidence:\n- \"Cat vs Rest\": 0.8 ← Winner\n- \"Dog vs Rest\": 0.3\n- \"Bird vs Rest\": 0.1\n→ Predict: Cat"
      },
      "tags": [
        "classification",
        "supervised",
        "multiclass"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "multilabel-classification",
      "name": "Multilabel Classification",
      "parentId": "linear-classifiers",
      "sectionId": "8.1.5",
      "level": 2,
      "fullExplanation": "Multilabel classification assigns multiple labels to each instance simultaneously. Unlike multiclass (exactly one label), an instance can have zero, one, or many labels. Approaches: binary relevance (independent classifier per label), classifier chains (classifiers conditioned on previous predictions), or neural networks with sigmoid outputs. Common in tagging, medical diagnosis, document categorization.",
      "simpleExplanation": "An item can belong to multiple categories at once. A movie can be both \"Comedy\" AND \"Romance\" AND \"Drama.\" Train a separate yes/no classifier for each label, or use methods that capture label dependencies.",
      "example": {
        "description": "Movie genre classification (multilabel):\n- Movie: \"The Proposal\"\n- Labels: [Comedy: Yes, Romance: Yes, Action: No, Drama: No, Thriller: No]\n\nvs. Multiclass (only one allowed):\n- Movie: \"The Proposal\"\n- Label: Romance (must pick just one)\n\nMultilabel allows: [\"Comedy\", \"Romance\"] simultaneously."
      },
      "tags": [
        "classification",
        "supervised",
        "multilabel"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "svm",
      "name": "Support Vector Machine (SVM)",
      "parentId": "support-vector-machines",
      "sectionId": "8.2.1",
      "level": 2,
      "fullExplanation": "SVM finds the hyperplane that maximizes the margin between classes. The margin is the distance from the hyperplane to the nearest data points (support vectors). Maximizing margin improves generalization. For linearly inseparable data, soft-margin SVM allows some misclassifications (controlled by C parameter). Kernel trick enables nonlinear boundaries.",
      "simpleExplanation": "Find the widest possible \"street\" separating two classes. The street's edges touch the closest points from each side (support vectors). Wider street = more confident separation = better generalization.",
      "example": {
        "description": "Separating spam from non-spam:\n```\n        Spam\n    x   x\n  x   x        <- Support vectors (on margin edge)"
      },
      "tags": [
        "classification",
        "supervised",
        "support",
        "vector",
        "machine"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "maximum-margin",
      "name": "Maximum Margin",
      "parentId": "support-vector-machines",
      "sectionId": "8.2.2",
      "level": 2,
      "fullExplanation": "Maximum margin is the SVM's optimization objective: find the hyperplane with the largest distance to the nearest training points. Mathematically, maximize 2/||w|| subject to yᵢ(w·xᵢ + b) ≥ 1. Larger margins correlate with better generalization (PAC learning bounds). Only support vectors affect the solution—other points could be removed without changing the boundary.",
      "simpleExplanation": "Make the gap between classes as wide as possible. A tiny gap means small changes could cause misclassification. A wide gap means robust separation—new points falling in the gap are clearly uncertain rather than wrongly classified.",
      "example": {
        "description": "Two possible separating lines:\n```\nOption A (small margin):        Option B (large margin):\n    x x x                           x x x"
      },
      "tags": [
        "classification",
        "supervised",
        "maximum",
        "margin"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "soft-margin-svm",
      "name": "Soft Margin SVM",
      "parentId": "support-vector-machines",
      "sectionId": "8.2.3",
      "level": 2,
      "fullExplanation": "Soft margin SVM allows some training points to violate the margin or be misclassified, introducing slack variables ξᵢ. Objective: minimize ||w||²/2 + C·Σξᵢ. The C parameter controls the tradeoff—large C penalizes violations heavily (harder margin), small C allows more violations (softer margin). Essential for real-world noisy data where perfect separation is impossible or would overfit.",
      "simpleExplanation": "Allow some mistakes to get a better overall boundary. Real data has noise and outliers. Instead of contorting the boundary to classify every point, accept a few errors for a simpler, more generalizable model.",
      "example": {
        "description": "Data with one outlier:\n\n\nHard margin twists to include outlier → overfits\nSoft margin ignores outlier → better boundary",
        "code": "Hard margin (C=∞):              Soft margin (C=1):\n   x x x                           x x x\n      x (outlier)                     x ← allowed to be wrong\n __________                      __________\n  o o o o                         o o o o",
        "codeLanguage": "python"
      },
      "tags": [
        "classification",
        "supervised",
        "soft",
        "margin"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "svm-c-parameter",
      "name": "C Parameter",
      "parentId": "support-vector-machines",
      "sectionId": "8.2.4",
      "level": 2,
      "fullExplanation": "The C parameter in SVM controls regularization by setting the penalty for margin violations. High C: Strong penalty for violations, model tries to classify all points correctly (risk of overfitting). Low C: Weak penalty, allows more violations for a simpler model (risk of underfitting). C is typically tuned via cross-validation. It's the inverse of regularization strength.",
      "simpleExplanation": "How much do we care about misclassifying training points? High C = \"classify everything correctly, even if boundary is complex.\" Low C = \"simple boundary is more important than perfect training accuracy.\"",
      "example": {
        "description": "Same dataset, different C values:",
        "code": "C = 0.01 (low):                 C = 1000 (high):\nSimple boundary                 Complex boundary\nSome training errors            All training points correct\nGeneralizes well                May overfit\n\nTrain acc: 92%                  Train acc: 100%\nTest acc: 90%                   Test acc: 85%",
        "codeLanguage": "python"
      },
      "tags": [
        "classification",
        "supervised",
        "parameter"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "naive-bayes",
      "name": "Naive Bayes",
      "parentId": "probabilistic-classifiers",
      "sectionId": "8.3.1",
      "level": 2,
      "fullExplanation": "Naive Bayes applies Bayes' theorem with the \"naive\" assumption that features are conditionally independent given the class: P(y|x₁,...,xₙ) ∝ P(y)∏P(xᵢ|y). Despite the unrealistic independence assumption, it works surprisingly well in practice, especially for text classification. Fast training and prediction, handles high-dimensional data well, and provides probability estimates.",
      "simpleExplanation": "Classify by multiplying probabilities, assuming features are independent. \"Given this is spam, what's the probability of seeing 'free'? What about 'money'? What about 'offer'?\" Multiply all together. Simple but effective, especially for text.",
      "example": {
        "description": "Email spam classification:\n- P(spam) = 0.3, P(not spam) = 0.7\n- P(\"free\" | spam) = 0.8, P(\"free\" | not spam) = 0.1\n- P(\"money\" | spam) = 0.7, P(\"money\" | not spam) = 0.05\n\nEmail contains \"free\" and \"money\":\n- P(spam | words) ∝ 0.3 × 0.8 × 0.7 = 0.168\n- P(not spam | words) ∝ 0.7 × 0.1 × 0.05 = 0.0035\n\nNormalized: P(spam) = 0.168/(0.168+0.0035) = 98%"
      },
      "tags": [
        "classification",
        "supervised",
        "naive",
        "bayes"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "gaussian-naive-bayes",
      "name": "Gaussian Naive Bayes",
      "parentId": "probabilistic-classifiers",
      "sectionId": "8.3.2",
      "level": 2,
      "fullExplanation": "Gaussian Naive Bayes assumes continuous features follow Gaussian (normal) distributions within each class. For each class, it estimates mean μ and variance σ² per feature. Likelihood: P(xᵢ|y) = (1/√(2πσ²)) × exp(-(xᵢ-μ)²/(2σ²)). Best for continuous data that's approximately normally distributed. Fast and works well with small training sets.",
      "simpleExplanation": "Assume each feature follows a bell curve within each class. Measure the average height of spam emails vs non-spam emails. A new email's height is compared to both bell curves to see which it fits better.",
      "example": {
        "description": "Classifying flowers by petal length:\n- Setosa: mean=1.5cm, std=0.2cm\n- Versicolor: mean=4.0cm, std=0.5cm\n\nNew flower with petal length 1.6cm:\n- P(1.6cm | Setosa) = Gaussian(1.6, μ=1.5, σ=0.2) = high\n- P(1.6cm | Versicolor) = Gaussian(1.6, μ=4.0, σ=0.5) = very low\n\n→ Predict Setosa"
      },
      "tags": [
        "classification",
        "supervised",
        "gaussian",
        "naive",
        "bayes"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "multinomial-naive-bayes",
      "name": "Multinomial Naive Bayes",
      "parentId": "probabilistic-classifiers",
      "sectionId": "8.3.3",
      "level": 2,
      "fullExplanation": "Multinomial Naive Bayes models feature counts following multinomial distribution—ideal for text represented as word counts or TF-IDF. P(xᵢ|y) represents the probability of observing word i given class y. Handles variable document lengths naturally. Widely used for text classification, spam filtering, and sentiment analysis. Requires non-negative feature values.",
      "simpleExplanation": "Perfect for counting things, especially words. \"How often does 'sale' appear in spam vs legitimate emails?\" The more times a word appears, the more evidence it provides. Standard choice for text classification.",
      "example": {
        "description": "Document classification:\n- Training: Learn P(word|category) for each word\n- Spam emails have more \"buy\", \"free\", \"offer\"\n- Ham emails have more \"meeting\", \"project\", \"schedule\"\n\nNew email: \"Free offer! Buy now!\"\n- Each word adds evidence toward spam\n- Result: Classified as spam",
        "code": "from sklearn.naive_bayes import MultinomialNB\nmodel = MultinomialNB()\nmodel.fit(X_tfidf, y_train)",
        "codeLanguage": "python"
      },
      "tags": [
        "classification",
        "supervised",
        "multinomial",
        "naive",
        "bayes"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "knn-classification",
      "name": "K-Nearest Neighbors Classification",
      "parentId": "instance-based-classification",
      "sectionId": "8.4.1",
      "level": 2,
      "fullExplanation": "KNN Classification assigns the class most common among the K nearest training examples (majority voting). Ties can be broken randomly or by distance weighting. Non-parametric and makes no assumptions about data distribution. Decision boundaries can be complex and nonlinear. Sensitive to feature scaling, irrelevant features, and curse of dimensionality. K should be odd for binary classification to avoid ties.",
      "simpleExplanation": "Look at the K most similar examples, take a vote. If 4 of 5 nearest neighbors are \"cat,\" predict \"cat.\" No training needed—just memorize all examples and compare at prediction time.",
      "example": {
        "description": "Classify a new point with K=5:",
        "code": "New point: ?\n\n    5 nearest neighbors:\n    - Point 1: Cat (distance: 0.5)\n    - Point 2: Cat (distance: 0.7)\n    - Point 3: Dog (distance: 0.8)\n    - Point 4: Cat (distance: 0.9)\n    - Point 5: Dog (distance: 1.0)\n\n    Vote: Cat=3, Dog=2\n    → Predict: Cat",
        "codeLanguage": "python"
      },
      "tags": [
        "classification",
        "supervised",
        "nearest",
        "neighbors"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "choosing-k",
      "name": "Choosing K",
      "parentId": "instance-based-classification",
      "sectionId": "8.4.2",
      "level": 2,
      "fullExplanation": "K selection is crucial in KNN. Small K (e.g., 1): High variance, sensitive to noise, complex decision boundaries. Large K: High bias, smoother boundaries, may include points from other regions. Common approach: test multiple K values using cross-validation, select K with best validation performance. Often K=√n is a starting point. Use odd K for binary classification.",
      "simpleExplanation": "K=1: Just copy the closest neighbor—noisy but fits training data perfectly.\nK=100: Average many neighbors—smooth but might miss local patterns.\nUse cross-validation to find the sweet spot.",
      "example": {
        "description": "Testing different K values:\n\n\nK=5 achieves best test accuracy, balancing bias and variance.",
        "code": "K=1:  Training acc=100%, Test acc=85% (overfitting)\nK=5:  Training acc=95%,  Test acc=92% (good balance) ✓\nK=15: Training acc=90%,  Test acc=91% (slightly oversmoothed)\nK=50: Training acc=85%,  Test acc=88% (underfitting)",
        "codeLanguage": "python"
      },
      "tags": [
        "classification",
        "supervised",
        "choosing"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "decision-tree-classifier",
      "name": "Decision Tree Classifier",
      "parentId": "decision-trees",
      "sectionId": "8.5.1",
      "level": 2,
      "fullExplanation": "Decision Tree Classifier recursively partitions the feature space using binary splits that maximize class purity. Each internal node tests a feature, each branch represents an outcome, each leaf predicts a class. Splitting criteria: Gini impurity or information gain (entropy). Trees are interpretable, handle nonlinear relationships, and require no feature scaling. Prone to overfitting without pruning.",
      "simpleExplanation": "A series of yes/no questions leading to a prediction. \"Is age > 30? If yes, is income > $50K? If yes, predict: will buy.\" Easy to understand and explain to non-technical stakeholders.",
      "example": {
        "description": "Loan approval decision tree:\n\n\nApplicant: Income=$60K, Credit=720\n→ Path: Yes → Yes → Decision: Approve",
        "code": "[Income > $50K?]\n                 /            \\\n              Yes              No\n               /                \\\n    [Credit > 700?]        [Employed?]\n       /       \\              /      \\\n    Approve  Review       Review    Deny",
        "codeLanguage": "python"
      },
      "tags": [
        "classification",
        "supervised",
        "decision",
        "tree",
        "classifier"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "gini-impurity",
      "name": "Gini Impurity",
      "parentId": "decision-trees",
      "sectionId": "8.5.2",
      "level": 2,
      "fullExplanation": "Gini impurity measures class distribution impurity: Gini = 1 - Σpᵢ². Range: 0 (pure, all one class) to 0.5 for binary (equal split). Decision trees minimize weighted Gini impurity when splitting. Lower is better—pure nodes have Gini=0. Computationally simpler than entropy, often produces similar results. Used by default in scikit-learn.",
      "simpleExplanation": "How mixed are the classes in a group? Gini=0 means all same class (pure). Gini=0.5 means 50-50 split (maximum impurity). Trees split to create purer groups.",
      "example": {
        "description": "Node A: 100 samples, 90 class A, 10 class B\nGini = 1 - (0.9² + 0.1²) = 1 - 0.82 = 0.18 (fairly pure)\n\nNode B: 100 samples, 50 class A, 50 class B\nGini = 1 - (0.5² + 0.5²) = 1 - 0.5 = 0.50 (maximum impurity)\n\nNode C: 100 samples, 100 class A, 0 class B\nGini = 1 - (1.0² + 0²) = 0 (perfectly pure)"
      },
      "tags": [
        "classification",
        "supervised",
        "gini",
        "impurity"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "information-gain",
      "name": "Information Gain (Entropy)",
      "parentId": "decision-trees",
      "sectionId": "8.5.3",
      "level": 2,
      "fullExplanation": "Information gain measures entropy reduction from a split. Entropy: H = -Σpᵢ log₂(pᵢ), measuring class uncertainty (0 = pure, 1 = maximum for binary). Information gain = H(parent) - weighted_average(H(children)). Higher gain means better split. ID3 and C4.5 algorithms use this criterion. Slightly more computationally expensive than Gini but theoretically grounded.",
      "simpleExplanation": "How much does a split reduce uncertainty? Before split: \"could be cat or dog\" (uncertain). After split: left branch is all cats, right is all dogs (certain). Information gain measures this uncertainty reduction.",
      "example": {
        "description": "Before split: 50 cats, 50 dogs\nEntropy = -0.5×log₂(0.5) - 0.5×log₂(0.5) = 1.0 (maximum)\n\nAfter split on \"has whiskers\":\n- Left (whiskers=yes): 48 cats, 2 dogs → Entropy ≈ 0.18\n- Right (whiskers=no): 2 cats, 48 dogs → Entropy ≈ 0.18\n\nWeighted entropy after = 0.5×0.18 + 0.5×0.18 = 0.18\nInformation Gain = 1.0 - 0.18 = 0.82 (excellent split!)"
      },
      "tags": [
        "classification",
        "supervised",
        "information",
        "gain",
        "(entropy)"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "pruning",
      "name": "Pruning",
      "parentId": "decision-trees",
      "sectionId": "8.5.4",
      "level": 2,
      "fullExplanation": "Pruning reduces tree complexity to prevent overfitting. Pre-pruning (early stopping): Limit depth, minimum samples per leaf, minimum impurity decrease during growth. Post-pruning: Grow full tree, then remove nodes that don't improve validation performance. Cost-complexity pruning (CCP) balances tree size against training error using a parameter α. Simpler trees generalize better.",
      "simpleExplanation": "Cut back the tree to prevent memorization. A tree with a leaf for every training example is overfitting. Pruning removes unnecessary branches, keeping the tree simple and generalizable.",
      "example": {
        "description": "Unpruned tree:\n\n\nAfter pruning:\n```\n           [root]\n          /      \\\n       [A]        [B]\n        |          |\n      Class1    Class2   <- 2 leaves, simple\n```\n\nPruned tree: Lower training accuracy but higher test accuracy.",
        "code": "[root]\n          /      \\\n       [A]        [B]\n      / \\         / \\\n    [C] [D]     [E] [F]\n   / \\   |     /|\\   |\n  1   2  3   4 5 6   7   <- 7 leaves, complex",
        "codeLanguage": "python"
      },
      "tags": [
        "classification",
        "supervised",
        "pruning"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "feature-importance",
      "name": "Feature Importance",
      "parentId": "decision-trees",
      "sectionId": "8.5.5",
      "level": 2,
      "fullExplanation": "Feature importance in trees measures each feature's contribution to reducing impurity across all splits. Calculated as the total reduction in Gini/entropy weighted by the number of samples reaching each split. Features used higher in the tree (more samples affected) and causing larger purity gains rank higher. Useful for understanding which features drive predictions and for feature selection.",
      "simpleExplanation": "Which features matter most? Features that appear early in the tree and create clean splits are important. Features that rarely appear or barely improve purity are less important. Gives insight into what drives predictions.",
      "example": {
        "description": "Decision tree for loan approval:\n\n\nInterpretation: Income and credit score drive most decisions.\nAction: Could probably remove eye color feature entirely.",
        "code": "Feature Importances:\n- Income: 0.45 (used at root, major split)\n- Credit Score: 0.30 (second level, important)\n- Age: 0.15 (lower level splits)\n- Zip Code: 0.08 (rarely used)\n- Eye Color: 0.02 (almost never helps)",
        "codeLanguage": "python"
      },
      "tags": [
        "classification",
        "supervised",
        "feature",
        "importance"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "beginner"
    },
    {
      "id": "rag-fundamentals",
      "name": "RAG Fundamentals",
      "parentId": "rag",
      "sectionId": "23.1",
      "level": 1,
      "fullExplanation": "RAG Fundamentals covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about rag fundamentals and related techniques.",
      "example": {
        "description": "This section contains concepts related to rag fundamentals."
      },
      "tags": [
        "rag",
        "retrieval",
        "fundamentals"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "document-processing",
      "name": "Document Processing",
      "parentId": "rag",
      "sectionId": "23.2",
      "level": 1,
      "fullExplanation": "Document Processing covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about document processing and related techniques.",
      "example": {
        "description": "This section contains concepts related to document processing."
      },
      "tags": [
        "rag",
        "retrieval",
        "document",
        "processing"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "embeddings",
      "name": "Embeddings",
      "parentId": "rag",
      "sectionId": "23.3",
      "level": 1,
      "fullExplanation": "Embeddings covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about embeddings and related techniques.",
      "example": {
        "description": "This section contains concepts related to embeddings."
      },
      "tags": [
        "rag",
        "retrieval",
        "embeddings"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "vector-stores",
      "name": "Vector Stores",
      "parentId": "rag",
      "sectionId": "23.4",
      "level": 1,
      "fullExplanation": "Vector Stores covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about vector stores and related techniques.",
      "example": {
        "description": "This section contains concepts related to vector stores."
      },
      "tags": [
        "rag",
        "retrieval",
        "vector",
        "stores"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "retrieval-methods",
      "name": "Retrieval Methods",
      "parentId": "rag",
      "sectionId": "23.5",
      "level": 1,
      "fullExplanation": "Retrieval Methods covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about retrieval methods and related techniques.",
      "example": {
        "description": "This section contains concepts related to retrieval methods."
      },
      "tags": [
        "rag",
        "retrieval",
        "methods"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "query-processing",
      "name": "Query Processing",
      "parentId": "rag",
      "sectionId": "23.6",
      "level": 1,
      "fullExplanation": "Query Processing covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about query processing and related techniques.",
      "example": {
        "description": "This section contains concepts related to query processing."
      },
      "tags": [
        "rag",
        "retrieval",
        "query",
        "processing"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "re-ranking",
      "name": "Re-Ranking",
      "parentId": "rag",
      "sectionId": "23.7",
      "level": 1,
      "fullExplanation": "Re-Ranking covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about re-ranking and related techniques.",
      "example": {
        "description": "This section contains concepts related to re-ranking."
      },
      "tags": [
        "rag",
        "retrieval",
        "ranking"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "context-management",
      "name": "Context Management",
      "parentId": "rag",
      "sectionId": "23.8",
      "level": 1,
      "fullExplanation": "Context Management covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about context management and related techniques.",
      "example": {
        "description": "This section contains concepts related to context management."
      },
      "tags": [
        "rag",
        "retrieval",
        "context",
        "management"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "generation",
      "name": "Generation",
      "parentId": "rag",
      "sectionId": "23.9",
      "level": 1,
      "fullExplanation": "Generation covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about generation and related techniques.",
      "example": {
        "description": "This section contains concepts related to generation."
      },
      "tags": [
        "rag",
        "retrieval",
        "generation"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "advanced-rag-patterns",
      "name": "Advanced RAG Patterns",
      "parentId": "rag",
      "sectionId": "23.10",
      "level": 1,
      "fullExplanation": "Advanced RAG Patterns covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about advanced rag patterns and related techniques.",
      "example": {
        "description": "This section contains concepts related to advanced rag patterns."
      },
      "tags": [
        "rag",
        "retrieval",
        "advanced",
        "patterns"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "rag-evaluation",
      "name": "RAG Evaluation",
      "parentId": "rag",
      "sectionId": "23.11",
      "level": 1,
      "fullExplanation": "RAG Evaluation covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about rag evaluation and related techniques.",
      "example": {
        "description": "This section contains concepts related to rag evaluation."
      },
      "tags": [
        "rag",
        "retrieval",
        "evaluation"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "rag-infrastructure",
      "name": "RAG Infrastructure",
      "parentId": "rag",
      "sectionId": "23.12",
      "level": 1,
      "fullExplanation": "RAG Infrastructure covers important concepts in this area of machine learning.",
      "simpleExplanation": "Learn about rag infrastructure and related techniques.",
      "example": {
        "description": "This section contains concepts related to rag infrastructure."
      },
      "tags": [
        "rag",
        "retrieval",
        "infrastructure"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "rag-overview",
      "name": "RAG Overview",
      "parentId": "rag-fundamentals",
      "sectionId": "23.1.1",
      "level": 2,
      "fullExplanation": "Retrieval-Augmented Generation (RAG) is an architecture that enhances Large Language Model outputs by retrieving relevant information from external knowledge sources before generating responses. Instead of relying solely on knowledge encoded in model parameters during training, RAG dynamically fetches context-specific information at inference time, grounds responses in retrieved documents, and reduces hallucinations by providing factual anchors.",
      "simpleExplanation": "Give the AI a reference book while it answers. Instead of relying only on what it memorized during training, RAG lets the AI look up relevant documents first, then answer based on what it found. Like an open-book exam instead of a closed-book one.",
      "example": {
        "description": "Without RAG:\n- User: \"What's our company's refund policy?\"\n- LLM: Makes up a generic policy (hallucination)\n\nWith RAG:\n1. Query: \"What's our company's refund policy?\"\n2. Retrieve: Find company_policy.pdf, section on refunds\n3. Context: \"Refunds are processed within 14 days for unused items...\"\n4. Generate: \"According to your company policy, refunds are processed within 14 days for unused items...\""
      },
      "tags": [
        "rag",
        "retrieval",
        "overview"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "rag-vs-finetuning",
      "name": "RAG vs Fine-Tuning",
      "parentId": "rag-fundamentals",
      "sectionId": "23.1.2",
      "level": 2,
      "fullExplanation": "RAG and fine-tuning are complementary approaches to customizing LLMs. Fine-tuning modifies model weights through training on domain-specific data, embedding knowledge permanently but requiring retraining for updates. RAG keeps the base model frozen and dynamically retrieves current information at inference time. RAG excels for frequently changing data and factual accuracy; fine-tuning excels for style, format, and reasoning patterns.",
      "simpleExplanation": "Fine-tuning = Teaching the model new things permanently (surgery on its brain).\nRAG = Giving the model reference materials to consult (giving it a handbook).\n\nFine-tuning is expensive and knowledge becomes outdated. RAG is cheaper and always uses current documents.",
      "example": {
        "description": "Company knowledge base with 10,000 documents:\n\nFine-tuning approach:\n- Train model on all documents ($$$)\n- New document added → Need to retrain\n- Model might still hallucinate\n\nRAG approach:\n- Keep base model as-is\n- Index documents in vector database\n- Query retrieves relevant docs\n- Model answers from retrieved context\n- New document → Just add to index (instant)\n\n| Aspect | Fine-Tuning | RAG |\n|"
      },
      "tags": [
        "rag",
        "retrieval",
        "fine",
        "tuning"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "rag-architecture",
      "name": "RAG Architecture",
      "parentId": "rag-fundamentals",
      "sectionId": "23.1.3",
      "level": 2,
      "fullExplanation": "The RAG architecture consists of two main components: a Retriever and a Generator. The Retriever converts queries and documents into embeddings, stores document embeddings in a vector database, and retrieves semantically similar documents given a query. The Generator (LLM) receives the original query plus retrieved context and synthesizes a grounded response. Components are connected through a pipeline orchestrating the flow.",
      "simpleExplanation": "Two parts working together:\n1. **Retriever** (the librarian): Finds relevant documents based on your question\n2. **Generator** (the expert): Reads the documents and writes an answer\n\nThe retriever narrows down millions of documents to a handful of relevant ones; the generator crafts a response using them.",
      "example": {
        "description": "",
        "code": "┌─────────────────────────────────────────────────────────────┐\n│                      RAG ARCHITECTURE                        │\n├─────────────────────────────────────────────────────────────┤\n│                                                             │\n│   ┌─────────┐      ┌─────────────┐      ┌──────────────┐   │\n│   │  Query  │ ──── │  Embedding  │ ──── │   Vector     │   │\n│   │         │      │   Model     │      │   Database   │   │\n│   └─────────┘      └─────────────┘      └──────┬───────┘   │\n│                                                 │           │\n│                                          Top-K Documents    │\n│                                                 │           │\n│   ┌─────────┐      ┌─────────────┐      ┌──────▼───────┐   │\n│   │ Response│ ◄─── │     LLM     │ ◄─── │   Context    │   │\n│   │         │      │  Generator  │      │  + Query     │   │\n│   └─────────┘      └─────────────┘      └──────────────┘   │\n│                                                             │\n└─────────────────────────────────────────────────────────────┘",
        "codeLanguage": "python"
      },
      "tags": [
        "rag",
        "retrieval",
        "architecture"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "rag-pipeline",
      "name": "RAG Pipeline",
      "parentId": "rag-fundamentals",
      "sectionId": "23.1.4",
      "level": 2,
      "fullExplanation": "The RAG pipeline consists of two phases: Indexing (offline) and Querying (online). Indexing: load documents → chunk into segments → generate embeddings → store in vector database. Querying: receive user query → embed query → retrieve similar chunks → (optionally) re-rank → construct prompt with context → generate response → (optionally) cite sources. Each stage can be customized and optimized independently.",
      "simpleExplanation": "Two phases:\n1. **Indexing (preparation)**: Break documents into pieces, convert to numbers, store for quick lookup\n2. **Querying (live)**: Take question, find relevant pieces, give to LLM with question, get answer\n\nLike organizing a library (indexing) then looking up books to answer questions (querying).",
      "example": {
        "description": "",
        "code": "INDEXING PHASE (Once, offline):\nDocuments → Chunking → Embedding → Vector Store\n   │            │           │            │\n   ▼            ▼           ▼            ▼\n[PDF,TXT]  [500 token   [768-dim     [Pinecone,\n           chunks]      vectors]     Chroma]\n\nQUERYING PHASE (Every request):\nQuery → Embed → Search → Retrieve → Rerank → Prompt → LLM → Response\n  │        │       │         │         │        │       │        │\n  ▼        ▼       ▼         ▼         ▼        ▼       ▼        ▼\n\"What    [768d]  Vector    Top-10   Top-3    Query  GPT-4  \"The\nis X?\"          Store     chunks   best     +Ctx          answer...\"",
        "codeLanguage": "python"
      },
      "tags": [
        "rag",
        "retrieval",
        "pipeline"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "grounding",
      "name": "Grounding",
      "parentId": "rag-fundamentals",
      "sectionId": "23.1.5",
      "level": 2,
      "fullExplanation": "Grounding refers to anchoring LLM outputs to verifiable information sources. In RAG, the model's response should be derivable from and consistent with the retrieved documents. Grounding improves factual accuracy, enables source attribution, and allows users to verify claims. Ungrounded responses (hallucinations) cannot be traced to any source.",
      "simpleExplanation": "Making sure the AI's answer actually comes from real documents, not imagination. Every claim should be traceable to a source. If the AI says something, you can point to where it learned that.",
      "example": {
        "description": "Ungrounded (hallucination):\n- \"The company was founded in 1985\" (made up)\n\nGrounded:\n- \"According to the company website [Source 1], the company was founded in 1992\"\n\nGrounding check:\n- Claim: \"Founded in 1992\"\n- Source says: \"Established in 1992\"\n- Grounded: ✓ Yes"
      },
      "tags": [
        "rag",
        "retrieval",
        "grounding"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "hallucination-mitigation",
      "name": "Hallucination Mitigation",
      "parentId": "rag-fundamentals",
      "sectionId": "23.1.6",
      "level": 2,
      "fullExplanation": "Hallucination in RAG occurs when the model generates information not supported by retrieved context or fabricates sources. Mitigation strategies include: explicit instructions to only use provided context, chain-of-thought prompting requiring source citation, confidence thresholds for uncertain answers, fact-checking against retrieved documents, and using models fine-tuned for faithfulness (like those trained with RLHF for attribution).",
      "simpleExplanation": "Stop the AI from making stuff up. Strategies:\n1. Tell it \"only use provided documents\"\n2. Make it cite sources for every claim\n3. Have it say \"I don't know\" when context doesn't contain the answer\n4. Double-check its answers against the documents",
      "example": {
        "description": "Prompt engineering for hallucination mitigation:",
        "code": "System: You are a helpful assistant. Answer questions using ONLY \nthe provided context. If the answer is not in the context, say \n\"I don't have enough information to answer this question.\"\nDo not make up information.\n\nContext:\n[Retrieved documents here]\n\nUser: What is the CEO's favorite color?\n\nBad response: \"The CEO's favorite color is blue.\"\nGood response: \"I don't have enough information to answer this \nquestion. The provided documents don't mention the CEO's color \npreferences.\"",
        "codeLanguage": "python"
      },
      "tags": [
        "rag",
        "retrieval",
        "hallucination",
        "mitigation"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "chunking",
      "name": "Chunking",
      "parentId": "document-processing",
      "sectionId": "23.2.1",
      "level": 2,
      "fullExplanation": "Chunking divides documents into smaller segments suitable for embedding and retrieval. Chunks must be small enough to fit in context windows and contain focused information, but large enough to preserve meaning. Chunking strategy significantly impacts retrieval quality—poor chunking leads to incomplete context or irrelevant matches. Considerations include chunk size, overlap, boundary detection, and metadata preservation.",
      "simpleExplanation": "Cutting documents into smaller pieces. You can't feed a 500-page book to the AI, so you slice it into digestible chunks. Too small = loses context; too big = too much noise. Finding the right size is crucial.",
      "example": {
        "description": "Original document (3000 words):\n\n\nChunked (500 words each with overlap):\n```\nChunk 1: Chapter 1 part 1 (words 1-500)\nChunk 2: Chapter 1 part 2 (words 400-900) ← 100 word overlap\nChunk 3: Chapter 1 part 3 + Chapter 2 start (words 800-1300)\n... etc\n```",
        "code": "Chapter 1: Introduction\n[1000 words about company history]\n\nChapter 2: Products  \n[1000 words about product line]\n\nChapter 3: Pricing\n[1000 words about pricing tiers]",
        "codeLanguage": "python"
      },
      "tags": [
        "rag",
        "retrieval",
        "chunking"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "chunk-size",
      "name": "Chunk Size",
      "parentId": "document-processing",
      "sectionId": "23.2.2",
      "level": 2,
      "fullExplanation": "Chunk size determines the granularity of retrieval. Smaller chunks (100-200 tokens) provide precise retrieval but may lack context. Larger chunks (500-1000 tokens) preserve more context but may contain irrelevant information and retrieve less precisely. Optimal size depends on content type, query patterns, and embedding model capabilities. Empirical testing is essential—no universal optimal size exists.",
      "simpleExplanation": "How big each piece should be. Small chunks = precise but might miss context. Big chunks = more context but might include irrelevant stuff. Usually 200-500 words works well, but test for your specific content.",
      "example": {
        "description": "Query: \"What is the refund policy for electronics?\"\n\nSmall chunks (100 tokens):\n- Retrieves: \"Electronics can be returned within 30 days\"\n- Missing: conditions, exceptions, process\n\nLarge chunks (1000 tokens):\n- Retrieves: Full refund policy + unrelated warranty info + shipping policy\n- Problem: Noise dilutes relevant information\n\nMedium chunks (300 tokens):\n- Retrieves: Complete refund policy section\n- Just right: enough context, focused content"
      },
      "tags": [
        "rag",
        "retrieval",
        "chunk",
        "size"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "chunk-overlap",
      "name": "Chunk Overlap",
      "parentId": "document-processing",
      "sectionId": "23.2.3",
      "level": 2,
      "fullExplanation": "Chunk overlap includes portions of adjacent chunks to prevent information loss at boundaries. Without overlap, a sentence split between two chunks loses coherence in both. Typical overlap is 10-20% of chunk size. Overlap improves retrieval recall but increases storage and computation. Critical for content where important information spans chunk boundaries.",
      "simpleExplanation": "Let chunks share some text with their neighbors. If you cut right in the middle of an important sentence, both halves lose meaning. Overlap ensures nothing falls through the cracks between chunks.",
      "example": {
        "description": "Without overlap:\n\nNeither chunk has complete information!\n\nWith 50-word overlap:\n```\nChunk 1: \"...The refund policy requires items to be returned \n          in original packaging within 30 days.\"\nChunk 2: \"returned in original packaging within 30 days. \n          Exceptions include...\"\n```\nBoth chunks contain the complete policy statement.",
        "code": "Chunk 1: \"...The refund policy requires items to be\"\nChunk 2: \"returned in original packaging within 30 days.\"",
        "codeLanguage": "python"
      },
      "tags": [
        "rag",
        "retrieval",
        "chunk",
        "overlap"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "semantic-chunking",
      "name": "Semantic Chunking",
      "parentId": "document-processing",
      "sectionId": "23.2.4",
      "level": 2,
      "fullExplanation": "Semantic chunking splits documents based on meaning rather than fixed character/token counts. It identifies natural boundaries using sentence embeddings—when semantic similarity between consecutive sentences drops significantly, a new chunk begins. This preserves coherent topics within chunks and avoids splitting related content. More expensive than fixed-size but often improves retrieval quality.",
      "simpleExplanation": "Cut documents where topics naturally change, not at arbitrary word counts. Use AI to detect \"this paragraph is about something different than the last one\" and split there. Keeps related ideas together.",
      "example": {
        "description": "Document about a product:\n\n\nFixed-size chunking: Might split paragraph 3 and 4 together (mixing features and pricing)\n\nSemantic chunking:\n- Chunk 1: Paragraphs 1-3 (all features)\n- Chunk 2: Paragraphs 4-5 (all pricing)\n- Chunk 3: Paragraphs 6-8 (all reviews)\n\nEach chunk is topically coherent.",
        "code": "[Paragraph 1-3: Product features]\n[Paragraph 4-5: Pricing information]  \n[Paragraph 6-8: Customer reviews]",
        "codeLanguage": "python"
      },
      "tags": [
        "rag",
        "retrieval",
        "semantic",
        "chunking"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "recursive-chunking",
      "name": "Recursive Chunking",
      "parentId": "document-processing",
      "sectionId": "23.2.5",
      "level": 2,
      "fullExplanation": "Recursive chunking attempts to split documents using a hierarchy of separators, falling back to smaller separators only when needed. First tries to split on double newlines (paragraphs), then single newlines, then sentences, then words. This preserves document structure while ensuring chunks don't exceed size limits. Adapts to document format automatically.",
      "simpleExplanation": "Try to split at natural breaks first. First try paragraphs. If still too big, try sentences. If still too big, try words. This keeps the most natural structure possible while meeting size limits.",
      "example": {
        "description": "Document: 2000 tokens\n\nRecursive process:\n1. Split by \"\\n\\n\" (paragraphs) → 5 chunks of ~400 tokens each ✓\n   - Chunk 1: 350 tokens ✓\n   - Chunk 2: 800 tokens ✗ (too big)\n   \n2. For chunk 2, split by \"\\n\" (lines) → 3 sub-chunks\n   - Sub-chunk 2a: 300 tokens ✓\n   - Sub-chunk 2b: 250 tokens ✓\n   - Sub-chunk 2c: 250 tokens ✓\n\nFinal: 7 chunks, all properly sized, natural boundaries preserved."
      },
      "tags": [
        "rag",
        "retrieval",
        "recursive",
        "chunking"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "text-embeddings",
      "name": "Text Embeddings",
      "parentId": "embeddings",
      "sectionId": "23.3.1",
      "level": 2,
      "fullExplanation": "Text embeddings are dense vector representations that capture semantic meaning of text. Similar meanings map to nearby vectors in embedding space, enabling semantic search beyond keyword matching. Embeddings are generated by neural networks trained on large corpora to understand language relationships. The embedding model's quality directly impacts retrieval accuracy.",
      "simpleExplanation": "Convert text into numbers that capture meaning. \"Dog\" and \"puppy\" get similar numbers because they mean similar things. \"Dog\" and \"refrigerator\" get very different numbers. This lets us find documents by meaning, not just matching words.",
      "example": {
        "description": "",
        "code": "from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Generate embeddings\ntexts = [\n    \"How to return a product\",     # [0.12, -0.34, 0.56, ...]\n    \"Refund policy and procedures\", # [0.11, -0.32, 0.58, ...] (similar!)\n    \"Best pizza recipes\"           # [-0.45, 0.67, -0.23, ...] (different!)\n]\n\nembeddings = model.encode(texts)\n\n# Similarity: texts[0] and texts[1] are close in vector space\n# texts[2] is far from both",
        "codeLanguage": "python"
      },
      "tags": [
        "rag",
        "retrieval",
        "text",
        "embeddings"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "embedding-models",
      "name": "Embedding Models",
      "parentId": "embeddings",
      "sectionId": "23.3.2",
      "level": 2,
      "fullExplanation": "Embedding models are neural networks trained to produce semantic text representations. Options range from open-source (Sentence Transformers, E5, BGE) to proprietary (OpenAI, Cohere, Voyage). Key factors: dimension size (256-3072), max input length, domain specialization, multilingual support, and cost. Model choice significantly impacts retrieval quality—larger models often perform better but cost more.",
      "simpleExplanation": "Different \"translators\" that convert text to numbers. Some are free, some cost money. Some are better for general text, others for specific domains. Bigger models usually work better but are slower and more expensive.",
      "example": {
        "description": "Popular embedding models comparison:\n\n| Model | Dimensions | Max Tokens | Best For |\n|"
      },
      "tags": [
        "rag",
        "retrieval",
        "embedding",
        "models"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "cosine-similarity",
      "name": "Cosine Similarity",
      "parentId": "embeddings",
      "sectionId": "23.3.3",
      "level": 2,
      "fullExplanation": "Cosine similarity measures the angle between two vectors, regardless of magnitude. Formula: cos(θ) = (A·B)/(||A||||B||). Range: -1 (opposite) to 1 (identical direction). Value of 0 means orthogonal (unrelated). Preferred for text embeddings because it's invariant to document length—a long document and short query can still match if they're about the same topic.",
      "simpleExplanation": "Measure how similar two vectors are by their direction, ignoring length. If two arrows point the same way, similarity = 1. Opposite directions = -1. Perpendicular = 0. We use this because a short question and long answer can point the same \"direction\" (topic) even though they're different lengths.",
      "example": {
        "description": "",
        "code": "import numpy as np\n\ndef cosine_similarity(a, b):\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n\n# Query: \"refund policy\"\nquery_embedding = [0.8, 0.6]\n\n# Document 1: \"How to get a refund\"\ndoc1_embedding = [0.75, 0.65]\n\n# Document 2: \"Best pizza recipes\"  \ndoc2_embedding = [-0.5, 0.7]\n\nsim1 = cosine_similarity(query_embedding, doc1_embedding)  # 0.99 (very similar!)\nsim2 = cosine_similarity(query_embedding, doc2_embedding)  # 0.14 (not similar)\n\n# Retrieve doc1, not doc2",
        "codeLanguage": "python"
      },
      "tags": [
        "rag",
        "retrieval",
        "cosine",
        "similarity"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "vector-database",
      "name": "Vector Database",
      "parentId": "vector-stores",
      "sectionId": "23.4.1",
      "level": 2,
      "fullExplanation": "Vector databases are specialized systems optimized for storing, indexing, and querying high-dimensional vectors. Unlike traditional databases that search by exact match, vector databases find approximate nearest neighbors efficiently using specialized index structures (HNSW, IVF). Features include metadata filtering, hybrid search, real-time updates, and scalability to billions of vectors.",
      "simpleExplanation": "A special database for storing embeddings (number lists). Regular databases find exact matches; vector databases find \"similar\" items even without exact matches. Like Google finding relevant results even if your search words don't exactly match any webpage.",
      "example": {
        "description": "",
        "code": "import chromadb\n\n# Create client and collection\nclient = chromadb.Client()\ncollection = client.create_collection(\"my_documents\")\n\n# Add documents with embeddings\ncollection.add(\n    documents=[\"Refund policy details...\", \"Shipping information...\"],\n    metadatas=[{\"source\": \"policy.pdf\"}, {\"source\": \"shipping.pdf\"}],\n    ids=[\"doc1\", \"doc2\"]\n)\n\n# Query by similarity\nresults = collection.query(\n    query_texts=[\"How do I return an item?\"],\n    n_results=2\n)\n# Returns most similar documents",
        "codeLanguage": "python"
      },
      "tags": [
        "rag",
        "retrieval",
        "vector",
        "database"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "hnsw",
      "name": "HNSW (Hierarchical Navigable Small World)",
      "parentId": "vector-stores",
      "sectionId": "23.4.2",
      "level": 2,
      "fullExplanation": "HNSW is a graph-based index structure for approximate nearest neighbor search. It builds a hierarchical graph where each layer contains a subset of vectors, with the bottom layer containing all vectors. Search starts from top layer (few vectors, long-range connections), progressively moving to lower layers (more vectors, short-range connections). Provides excellent recall with logarithmic search complexity.",
      "simpleExplanation": "A clever way to organize vectors for fast searching. Imagine a multi-story building: top floor has few landmarks for big jumps, lower floors have more detail for precise navigation. Start at the top, quickly get close to target, then refine on lower floors.",
      "example": {
        "description": "",
        "code": "HNSW Index Structure:\n\nLayer 2 (sparse):    [A]─────────────────[B]\n                      │                    │\nLayer 1 (medium):    [A]───[C]───[D]───[B]\n                      │    │     │     │\nLayer 0 (dense):    [A][E][C][F][D][G][B][H]\n\nSearch for query Q:\n1. Layer 2: Start at A, jump to B (closer to Q)\n2. Layer 1: From B, navigate B→D (closer to Q)\n3. Layer 0: From D, find exact nearest neighbors [F, G]\n\nMuch faster than checking all vectors!",
        "codeLanguage": "python"
      },
      "tags": [
        "rag",
        "retrieval",
        "hnsw",
        "(hierarchical",
        "navigable"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "vector-db-options",
      "name": "Vector Database Options",
      "parentId": "vector-stores",
      "sectionId": "23.4.3",
      "level": 2,
      "fullExplanation": "Major vector database options include: Pinecone (managed, production-ready), Weaviate (open-source, hybrid search), Chroma (lightweight, embedded), Milvus (scalable, open-source), Qdrant (Rust-based, performant), pgvector (PostgreSQL extension), and FAISS (library, not database). Choice depends on scale, deployment preference (managed vs self-hosted), features needed, and existing infrastructure.",
      "simpleExplanation": "Different tools for storing embeddings:\n- **Pinecone**: Easy cloud service, just works, costs money\n- **Chroma**: Simple, free, good for starting out\n- **Weaviate**: Free, powerful, needs more setup\n- **pgvector**: Use your existing PostgreSQL database\n- **FAISS**: Facebook's library, very fast, not a full database",
      "example": {
        "description": "Choosing a vector database:\n\n| Need | Recommendation |\n|"
      },
      "tags": [
        "rag",
        "retrieval",
        "vector",
        "database",
        "options"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "dense-retrieval",
      "name": "Dense Retrieval",
      "parentId": "retrieval-methods",
      "sectionId": "23.5.1",
      "level": 2,
      "fullExplanation": "Dense retrieval uses neural network embeddings to represent queries and documents as dense vectors, finding matches through vector similarity. It captures semantic meaning—synonyms and paraphrases can match even without shared words. Requires embedding model inference for queries and pre-computed document embeddings. Excels at semantic matching but may miss exact keyword matches.",
      "simpleExplanation": "Find documents by meaning, not words. \"Automobile\" query finds \"car\" documents even though the words are different. Uses AI embeddings to understand what text means, then finds documents with similar meanings.",
      "example": {
        "description": "Query: \"canine companions\"\n\nDense retrieval matches:\n- \"Dogs make great pets\" (semantically similar: dogs = canines)\n- \"Puppy training guide\" (related concept)\n- \"Best dog food brands\" (same topic)\n\nKeyword search would find: Nothing (no word matches)\n\nDense retrieval understands \"canine companions\" ≈ \"dogs\""
      },
      "tags": [
        "rag",
        "retrieval",
        "dense"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "sparse-retrieval",
      "name": "Sparse Retrieval",
      "parentId": "retrieval-methods",
      "sectionId": "23.5.2",
      "level": 2,
      "fullExplanation": "Sparse retrieval uses traditional keyword-based methods where documents are represented as high-dimensional sparse vectors (mostly zeros) with non-zero values for terms present. Algorithms like BM25 and TF-IDF weight term importance. Fast, interpretable, and excellent for exact matches and rare terms. Doesn't understand synonyms or semantics.",
      "simpleExplanation": "Find documents by matching keywords. Fast and simple—look for documents containing the exact words in your query. Great for finding specific terms like product codes or names, but misses synonyms.",
      "example": {
        "description": "Query: \"iPhone 15 Pro Max specs\"\n\nSparse retrieval (BM25):\n✓ Excellent at finding documents with exact phrase \"iPhone 15 Pro Max\"\n✓ Good at rare/specific terms\n✗ Won't find documents saying \"Apple's latest smartphone specifications\"",
        "code": "from rank_bm25 import BM25Okapi\n\ncorpus = [\"iPhone 15 Pro specs...\", \"Latest Apple phone...\", \"Android review\"]\ntokenized = [doc.split() for doc in corpus]\nbm25 = BM25Okapi(tokenized)\n\nquery = \"iPhone 15 Pro\"\nscores = bm25.get_scores(query.split())\n# [0.8, 0.2, 0.0] - First doc matches best",
        "codeLanguage": "python"
      },
      "tags": [
        "rag",
        "retrieval",
        "sparse"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "hybrid-retrieval",
      "name": "Hybrid Retrieval",
      "parentId": "retrieval-methods",
      "sectionId": "23.5.3",
      "level": 2,
      "fullExplanation": "Hybrid retrieval combines dense (semantic) and sparse (keyword) retrieval, leveraging strengths of both. Methods include: linear combination of scores, reciprocal rank fusion, or re-ranking sparse results with dense model. Captures both semantic similarity and exact matches. Often outperforms either method alone, especially for diverse query types.",
      "simpleExplanation": "Best of both worlds: use keyword matching AND meaning matching together. Some queries need exact words (\"error code 404\"), others need meaning (\"how to fix website problem\"). Hybrid handles both by combining scores from both methods.",
      "example": {
        "description": "Query: \"Python pandas dataframe error 1234\"\n\nDense retrieval alone:\n- Finds general DataFrame tutorials (semantic match)\n- Misses specific error code\n\nSparse retrieval alone:\n- Finds error 1234 docs\n- Misses relevant DataFrame debugging without exact words\n\nHybrid retrieval:\n- Combines both sets\n- Re-ranks to prioritize docs with BOTH semantic relevance AND keyword matches\n- Best results!",
        "code": "# Hybrid scoring\ndef hybrid_search(query, alpha=0.5):\n    dense_results = dense_search(query)   # Semantic\n    sparse_results = sparse_search(query)  # Keyword\n    \n    # Combine scores\n    for doc in all_docs:\n        doc.score = alpha * dense_results[doc] + (1-alpha) * sparse_results[doc]\n    \n    return sorted(all_docs, key=lambda x: x.score, reverse=True)",
        "codeLanguage": "python"
      },
      "tags": [
        "rag",
        "retrieval",
        "hybrid"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "mmr",
      "name": "Maximum Marginal Relevance (MMR)",
      "parentId": "retrieval-methods",
      "sectionId": "23.5.4",
      "level": 2,
      "fullExplanation": "MMR balances relevance and diversity in retrieval results. It iteratively selects documents that are both relevant to the query AND different from already-selected documents. Formula: MMR = argmax[λ·Sim(d,q) - (1-λ)·max(Sim(d,d_selected))]. Prevents redundant results when top matches contain similar information. λ controls relevance-diversity tradeoff.",
      "simpleExplanation": "Avoid repetition in results. If your top 5 results all say the same thing, that's wasteful. MMR picks diverse results: the most relevant one first, then the next most relevant that adds NEW information, and so on.",
      "example": {
        "description": "Query: \"machine learning benefits\"\n\nWithout MMR (top 3 by relevance only):\n1. \"ML improves efficiency and accuracy\" \n2. \"Machine learning increases efficiency\" (redundant!)\n3. \"ML makes processes more efficient\" (redundant!)\n\nWith MMR (λ=0.7):\n1. \"ML improves efficiency and accuracy\" (most relevant)\n2. \"ML enables personalization at scale\" (relevant + different)\n3. \"ML reduces costs through automation\" (relevant + different)\n\nEach result adds unique value."
      },
      "tags": [
        "rag",
        "retrieval",
        "maximum",
        "marginal",
        "relevance"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "query-expansion",
      "name": "Query Expansion",
      "parentId": "query-processing",
      "sectionId": "23.6.1",
      "level": 2,
      "fullExplanation": "Query expansion augments the original query with additional related terms to improve retrieval recall. Techniques include: synonym addition, LLM-based expansion, pseudo-relevance feedback (extract terms from initial results), and knowledge graph expansion. Helps when user queries are brief or use different terminology than documents.",
      "simpleExplanation": "Make the query bigger and better. User types \"laptop\"; expand to \"laptop computer notebook PC portable\". Now you'll find documents using any of these words. Especially helpful for short or vague queries.",
      "example": {
        "description": "Original query: \"headache medicine\"\n\nExpanded query: \"headache medicine pain relief aspirin ibuprofen acetaminophen migraine treatment\"\n\n\n\nResult: Finds documents about \"pain relievers\" and \"aspirin\" that the original query would miss.",
        "code": "def expand_query(query, llm):\n    prompt = f\"\"\"Generate 5 related search terms for: \"{query}\"\n    Return as comma-separated list.\"\"\"\n    \n    expansions = llm.generate(prompt)\n    # \"pain reliever, aspirin, ibuprofen, migraine remedy, analgesic\"\n    \n    return f\"{query} {expansions}\"",
        "codeLanguage": "python"
      },
      "tags": [
        "rag",
        "retrieval",
        "query",
        "expansion"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "hyde",
      "name": "HyDE (Hypothetical Document Embeddings)",
      "parentId": "query-processing",
      "sectionId": "23.6.2",
      "level": 2,
      "fullExplanation": "HyDE generates a hypothetical answer to the query using an LLM, then uses that answer's embedding for retrieval instead of the query embedding. Hypothetical documents are more semantically similar to actual documents than short queries are. The generated answer doesn't need to be factually correct—it just needs to be in the same semantic space as relevant documents.",
      "simpleExplanation": "Guess what a good answer might look like, then search for documents similar to that guess. A question like \"What causes rain?\" is short and vague. But if we first generate \"Rain is caused by water evaporating and condensing...\" that's much easier to match against actual documents.",
      "example": {
        "description": "Query: \"Why is the sky blue?\"\n\nStandard retrieval:\n- Embed short query → often matches poorly with long documents\n\nHyDE approach:\n1. Generate hypothetical answer:\n   \"The sky appears blue due to Rayleigh scattering, where \n    sunlight interacts with molecules in the atmosphere,\n    scattering shorter blue wavelengths more than other colors.\"\n\n2. Embed this hypothetical document (not the query)\n\n3. Retrieve documents similar to hypothetical answer\n\nResult: Better matches because we're comparing document-like text to documents."
      },
      "tags": [
        "rag",
        "retrieval",
        "hyde",
        "(hypothetical",
        "document"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "query-decomposition",
      "name": "Query Decomposition",
      "parentId": "query-processing",
      "sectionId": "23.6.3",
      "level": 2,
      "fullExplanation": "Query decomposition breaks complex questions into simpler sub-questions that can be answered independently, then synthesizes results. Useful for multi-hop questions requiring information from multiple documents. Methods include LLM-based decomposition and structured query parsing. Each sub-query retrieves focused context, and the final answer combines all pieces.",
      "simpleExplanation": "Split hard questions into easier parts. \"Compare iPhone and Samsung prices and features\" becomes three questions: \"iPhone price?\", \"Samsung price?\", \"Feature comparison?\". Answer each, then combine into final response.",
      "example": {
        "description": "Complex query: \"How did Apple's revenue compare to Microsoft's in 2023, and which company had better growth?\"\n\nDecomposed:\n1. \"What was Apple's revenue in 2023?\"\n2. \"What was Microsoft's revenue in 2023?\"\n3. \"What was Apple's revenue growth rate?\"\n4. \"What was Microsoft's revenue growth rate?\"\n\nEach sub-query retrieves specific documents. Final synthesis:\n\"Apple's revenue was $383B (2% growth), Microsoft's was $211B (7% growth). While Apple had higher total revenue, Microsoft showed stronger growth.\""
      },
      "tags": [
        "rag",
        "retrieval",
        "query",
        "decomposition"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "reranking-overview",
      "name": "Re-Ranking Overview",
      "parentId": "re-ranking",
      "sectionId": "23.7.1",
      "level": 2,
      "fullExplanation": "Re-ranking is a two-stage retrieval approach: first retrieve many candidates quickly (e.g., top-100 via dense/sparse search), then use a more expensive model to re-score and reorder them. Re-rankers see the full query-document pair, enabling more accurate relevance scoring than embedding similarity alone. Improves precision among retrieved results.",
      "simpleExplanation": "Get rough results first, then sort them more carefully. Stage 1: Quickly grab 100 possibly-relevant documents. Stage 2: Carefully read each one and put the best at the top. Like a quick skim followed by careful reading.",
      "example": {
        "description": "Query: \"How to train a neural network\"\n\nStage 1 - Fast retrieval (embedding similarity):\nReturns 100 documents, roughly relevant\n\nStage 2 - Re-ranking (cross-encoder):\n- Doc A: \"Neural Network Training Guide\" → Score: 0.95\n- Doc B: \"Deep Learning Basics\" → Score: 0.82\n- Doc C: \"Brain Neuron Biology\" → Score: 0.25 (not actually relevant!)\n\nRe-ranked top 3 are much better than initial top 3."
      },
      "tags": [
        "rag",
        "retrieval",
        "ranking",
        "overview"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "cross-encoder",
      "name": "Cross-Encoder Re-Ranking",
      "parentId": "re-ranking",
      "sectionId": "23.7.2",
      "level": 2,
      "fullExplanation": "Cross-encoders process query and document together through a single transformer, enabling rich interaction between them. Unlike bi-encoders (separate embeddings), cross-encoders see both texts simultaneously and output a relevance score directly. Much more accurate but O(n) inference cost—must run for each query-document pair. Used for re-ranking, not initial retrieval.",
      "simpleExplanation": "Read the question and document together to judge relevance. More accurate than comparing separate embeddings because it can see how specific parts of the question relate to specific parts of the document. Slow (can't pre-compute) but very accurate.",
      "example": {
        "description": "",
        "code": "from sentence_transformers import CrossEncoder\n\n# Cross-encoder sees both together\nmodel = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n\nquery = \"What is the capital of France?\"\ndocuments = [\n    \"Paris is the capital and largest city of France.\",\n    \"France is a country in Western Europe.\",\n    \"The Eiffel Tower is in Paris.\"\n]\n\n# Score each pair\nscores = model.predict([(query, doc) for doc in documents])\n# [0.98, 0.23, 0.45]\n\n# Document 1 wins (directly answers the question)",
        "codeLanguage": "python"
      },
      "tags": [
        "rag",
        "retrieval",
        "cross",
        "encoder",
        "ranking"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "lost-in-middle",
      "name": "Lost in the Middle Problem",
      "parentId": "re-ranking",
      "sectionId": "23.7.3",
      "level": 2,
      "fullExplanation": "\"Lost in the Middle\" refers to LLMs' tendency to focus on information at the beginning and end of their context window, underweighting information in the middle. This affects RAG: even if relevant information is retrieved, placing it in the middle of the context may cause it to be ignored. Mitigation: put most relevant chunks at the start or end, or use retrieval strategies that optimize placement.",
      "simpleExplanation": "LLMs pay most attention to the start and end of their context, ignoring the middle. If you put the most important document in the middle of 10 retrieved docs, the LLM might miss it. Solution: put the best stuff first or last.",
      "example": {
        "description": "Context with 5 retrieved chunks:\n\n\nBetter ordering:\n```\nChunk 3: THE ACTUAL ANSWER [HIGH ATTENTION]\nChunk 5: Related but incomplete\nChunk 1: Weather information\nChunk 2: Irrelevant content\nChunk 4: More irrelevant content\n```",
        "code": "Chunk 1 (first): Weather information [HIGH ATTENTION]\nChunk 2: Irrelevant content\nChunk 3: THE ACTUAL ANSWER [LOW ATTENTION - Lost!]\nChunk 4: More irrelevant content  \nChunk 5 (last): Related but incomplete [HIGH ATTENTION]",
        "codeLanguage": "python"
      },
      "tags": [
        "rag",
        "retrieval",
        "lost",
        "middle",
        "problem"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "context-window",
      "name": "Context Window",
      "parentId": "context-management",
      "sectionId": "23.8.1",
      "level": 2,
      "fullExplanation": "The context window is the maximum number of tokens an LLM can process in a single request, including system prompt, retrieved documents, user query, and generated response. Limits range from 4K (older models) to 128K+ (modern models like GPT-4 Turbo, Claude). Larger context allows more retrieved documents but increases cost and latency. Strategic context management optimizes information density within limits.",
      "simpleExplanation": "How much text the AI can read at once. Like a desk that can only hold so many papers. Bigger context = more documents can be included, but costs more. You need to fit your question, relevant documents, and leave room for the answer.",
      "example": {
        "description": "GPT-4 Turbo with 128K context window:\n\nBudget allocation:\n- System prompt: 500 tokens\n- Retrieved documents: 100,000 tokens (plenty!)\n- User query: 100 tokens\n- Response: 4,000 tokens\n- Buffer: 23,400 tokens\n\nGPT-3.5 with 4K context window:\n- System prompt: 200 tokens\n- Retrieved documents: 2,500 tokens (only ~2 pages!)\n- User query: 100 tokens\n- Response: 1,000 tokens\n- Buffer: 200 tokens\n\nMust choose retrieved content more carefully with smaller windows."
      },
      "tags": [
        "rag",
        "retrieval",
        "context",
        "window"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "rag-prompt-construction",
      "name": "Prompt Construction for RAG",
      "parentId": "generation",
      "sectionId": "23.9.1",
      "level": 2,
      "fullExplanation": "RAG prompts combine retrieved context with the user query in a structured format. Key elements: clear instructions on how to use context, retrieved documents with metadata, the user question, and output format guidance. Good prompts instruct the model to cite sources, acknowledge uncertainty, and stay grounded in provided context rather than using training knowledge.",
      "simpleExplanation": "Build the message to the AI carefully. Include: \"Here are some relevant documents, answer the question using only these, cite your sources, say 'I don't know' if the answer isn't in the documents.\"",
      "example": {
        "description": "Output example:\n\"According to [Document 2], the refund policy allows returns within 30 days. [Document 1] clarifies that items must be in original packaging.\"",
        "code": "def construct_rag_prompt(query, retrieved_docs):\n    context = \"\\n\\n\".join([\n        f\"[Document {i+1}]: {doc.content}\" \n        for i, doc in enumerate(retrieved_docs)\n    ])\n    \n    return f\"\"\"You are a helpful assistant. Answer questions based ONLY \non the provided context. If the answer is not in the context, say \n\"I don't have enough information to answer this question.\"\n\nCite your sources using [Document X] notation.\n\nContext:\n{context}\n\nQuestion: {query}\n\nAnswer:\"\"\"",
        "codeLanguage": "python"
      },
      "tags": [
        "rag",
        "retrieval",
        "prompt",
        "construction"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "citation-generation",
      "name": "Citation Generation",
      "parentId": "generation",
      "sectionId": "23.9.2",
      "level": 2,
      "fullExplanation": "Citation generation requires the LLM to reference source documents when making claims. Methods include: inline citations [1], footnotes, or explicit source attribution. This enables fact-checking, builds user trust, and makes hallucinations detectable. Advanced systems verify citations against sources post-generation and flag unsupported claims.",
      "simpleExplanation": "Make the AI show its work. Every claim should say where it came from. \"The policy allows 30-day returns [Source: policy.pdf]\". Users can verify claims, and you can catch when the AI makes stuff up.",
      "example": {
        "description": "Without citations:\n\"The product costs $99 and includes free shipping.\"\n(Is this true? Which document said this?)\n\nWith citations:\n\"The product costs $99 [Document 1: pricing.pdf] and includes free shipping for orders over $50 [Document 3: shipping.pdf].\"\n\nVerification:\n- Claim 1: \"$99\" → Check pricing.pdf → ✓ Verified\n- Claim 2: \"free shipping\" → Check shipping.pdf → ✓ Verified"
      },
      "tags": [
        "rag",
        "retrieval",
        "citation",
        "generation"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "naive-rag",
      "name": "Naive RAG",
      "parentId": "advanced-rag-patterns",
      "sectionId": "23.10.1",
      "level": 2,
      "fullExplanation": "Naive RAG is the basic retrieval-augmented generation pattern: embed query → retrieve top-K chunks → concatenate with query → generate response. Simple to implement but has limitations: no query understanding, fixed retrieval strategy, no result refinement, and sensitivity to retrieval quality. Serves as baseline for more advanced approaches.",
      "simpleExplanation": "The simplest RAG: search for documents, stuff them in the prompt, ask the LLM. Quick to build but not very smart. Doesn't improve the query, doesn't check if results are good, doesn't verify the answer.",
      "example": {
        "description": "Limitations:\n- No query improvement\n- Fixed k=4 retrieval\n- No re-ranking\n- No answer verification",
        "code": "# Naive RAG\ndef naive_rag(query, vectorstore, llm):\n    # 1. Retrieve (simple similarity search)\n    docs = vectorstore.similarity_search(query, k=4)\n    \n    # 2. Construct prompt (basic concatenation)\n    context = \"\\n\".join([doc.content for doc in docs])\n    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n    \n    # 3. Generate (no verification)\n    return llm.generate(prompt)",
        "codeLanguage": "python"
      },
      "tags": [
        "rag",
        "retrieval",
        "naive"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "advanced-rag",
      "name": "Advanced RAG",
      "parentId": "advanced-rag-patterns",
      "sectionId": "23.10.2",
      "level": 2,
      "fullExplanation": "Advanced RAG adds pre-retrieval and post-retrieval optimizations: query rewriting/expansion before retrieval, hybrid search combining dense and sparse methods, re-ranking retrieved results, context compression to remove irrelevant parts, and answer refinement/verification. Each stage can be independently optimized. Significantly improves quality over naive RAG.",
      "simpleExplanation": "Smarter RAG with extra steps: improve the question first, use better search, sort results carefully, compress context to essentials, check the answer makes sense. More complex but much better results.",
      "example": {
        "description": "",
        "code": "# Advanced RAG Pipeline\ndef advanced_rag(query, vectorstore, llm, reranker):\n    # PRE-RETRIEVAL\n    # 1. Query expansion\n    expanded_query = llm.expand_query(query)\n    \n    # 2. Hybrid search (dense + sparse)\n    dense_results = vectorstore.similarity_search(expanded_query, k=20)\n    sparse_results = bm25_search(expanded_query, k=20)\n    candidates = merge_results(dense_results, sparse_results)\n    \n    # POST-RETRIEVAL  \n    # 3. Re-rank\n    reranked = reranker.rerank(query, candidates)[:5]\n    \n    # 4. Compress context\n    compressed = compress_context(query, reranked)\n    \n    # 5. Generate with verification\n    answer = llm.generate_with_citations(query, compressed)\n    \n    # 6. Verify citations\n    verified_answer = verify_citations(answer, reranked)\n    \n    return verified_answer",
        "codeLanguage": "python"
      },
      "tags": [
        "rag",
        "retrieval",
        "advanced"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "self-rag",
      "name": "Self-RAG",
      "parentId": "advanced-rag-patterns",
      "sectionId": "23.10.3",
      "level": 2,
      "fullExplanation": "Self-RAG trains the LLM to decide when retrieval is needed and to critique its own outputs. The model generates special tokens indicating: whether retrieval is necessary, whether retrieved docs are relevant, whether the response is supported by evidence, and response utility. This enables adaptive retrieval (only when needed) and self-correction without external verification.",
      "simpleExplanation": "The AI decides when it needs to look things up and judges its own answers. Instead of always retrieving, it first asks \"do I need external info?\" Then it checks \"is this document relevant?\" and \"is my answer supported?\" Self-aware RAG.",
      "example": {
        "description": "",
        "code": "Query: \"What is 2+2?\"\nSelf-RAG: [No Retrieval Needed] → Generates \"4\" directly\n\nQuery: \"What was Apple's Q4 2023 revenue?\"\nSelf-RAG: [Retrieval Needed] → Retrieves documents\n         [Document Relevant: Yes] → Uses document\n         [Response Supported: Yes] → \"$89.5 billion [Source]\"\n         [Utility: High] → Returns response\n\nQuery: \"What is the meaning of life?\"\nSelf-RAG: [Retrieval Needed] → Retrieves documents\n         [Document Relevant: No] → Discards retrieved docs\n         [Response Supported: Partially] → Flags uncertainty",
        "codeLanguage": "python"
      },
      "tags": [
        "rag",
        "retrieval",
        "self"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "crag",
      "name": "Corrective RAG (CRAG)",
      "parentId": "advanced-rag-patterns",
      "sectionId": "23.10.4",
      "level": 2,
      "fullExplanation": "Corrective RAG (CRAG) evaluates retrieval quality and takes corrective actions when retrieval fails. It grades retrieved documents as Correct, Ambiguous, or Incorrect. For incorrect retrievals, CRAG triggers web search or other knowledge sources as fallback. For ambiguous cases, it combines retrieved and web-searched information. Improves robustness when initial retrieval quality is poor.",
      "simpleExplanation": "RAG that checks if retrieval worked and fixes it if not. After retrieving documents, it asks \"are these actually relevant?\" If not, it tries a different source like web search. Self-correcting when the first attempt fails.",
      "example": {
        "description": "",
        "code": "def corrective_rag(query, vectorstore, llm, web_search):\n    # Initial retrieval\n    docs = vectorstore.similarity_search(query, k=5)\n    \n    # Grade retrieval quality\n    grade = llm.grade_relevance(query, docs)\n    \n    if grade == \"CORRECT\":\n        # Good retrieval, proceed normally\n        return generate_answer(query, docs)\n    \n    elif grade == \"INCORRECT\":\n        # Retrieval failed, use web search\n        web_docs = web_search(query)\n        return generate_answer(query, web_docs)\n    \n    elif grade == \"AMBIGUOUS\":\n        # Uncertain, combine both sources\n        web_docs = web_search(query)\n        combined = docs + web_docs\n        return generate_answer(query, combined)",
        "codeLanguage": "python"
      },
      "tags": [
        "rag",
        "retrieval",
        "corrective",
        "(crag)"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "agentic-rag",
      "name": "Agentic RAG",
      "parentId": "advanced-rag-patterns",
      "sectionId": "23.10.5",
      "level": 2,
      "fullExplanation": "Agentic RAG uses autonomous agents that can plan multi-step retrieval strategies, use tools, and iteratively refine results. Agents decide: which sources to query, how to reformulate queries based on initial results, when to stop retrieving, and how to synthesize information from multiple retrieval rounds. Enables complex research tasks requiring reasoning about information needs.",
      "simpleExplanation": "RAG with an AI agent that plans and executes its own research strategy. Instead of one search, the agent might: search, read results, decide it needs more info, search differently, find contradictions, search again to resolve them, then write final answer. Like a research assistant, not just a search engine.",
      "example": {
        "description": "Query: \"Why did Company X's stock drop last week?\"\n\nAgent execution:\n1. Search internal docs → Found Q4 earnings report\n2. Agent realizes: \"Need external news context\"\n3. Search news → Found negative analyst reports\n4. Agent notices: \"Conflicting information, need more sources\"\n5. Search SEC filings → Found insider selling disclosure\n6. Agent: \"Have enough context now\"\n7. Synthesize: \"Stock dropped due to weak Q4 earnings, negative analyst coverage, and insider selling...\"",
        "code": "# Agentic RAG with planning\nclass RAGAgent:\n    def answer(self, query):\n        plan = self.plan_research(query)\n        # Plan: [\"Search company docs\", \"Find financial reports\", \n        #        \"Cross-reference with news\"]\n        \n        knowledge = {}\n        for step in plan:\n            results = self.execute_step(step)\n            knowledge = self.integrate(knowledge, results)\n            \n            # Agent decides if more research needed\n            if self.is_sufficient(query, knowledge):\n                break\n            else:\n                # Adapt plan based on findings\n                plan = self.replan(query, knowledge, plan)\n        \n        return self.synthesize_answer(query, knowledge)",
        "codeLanguage": "python"
      },
      "tags": [
        "rag",
        "retrieval",
        "agentic"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "graph-rag",
      "name": "Graph RAG",
      "parentId": "advanced-rag-patterns",
      "sectionId": "23.10.6",
      "level": 2,
      "fullExplanation": "Graph RAG enhances retrieval using knowledge graph structures. Documents are processed to extract entities and relationships, forming a graph. Retrieval traverses the graph to find connected information that vector similarity might miss. Enables multi-hop reasoning—finding information through chains of relationships. Particularly useful for complex queries requiring connected facts.",
      "simpleExplanation": "RAG using a web of connected facts instead of just similar documents. Build a map of \"Company X → employs → Person Y → invented → Product Z\". When asked about Product Z, traverse the graph to find all connected relevant information, even if the documents don't mention Z directly.",
      "example": {
        "description": "Query: \"What products has the Stanford AI Lab influenced?\"\n\nTraditional RAG: Search for \"Stanford AI Lab products\" → Might miss connections\n\nGraph RAG:\n\n\nGraph traversal finds:\n- ImageNet (direct connection)\n- AlexNet (influenced by ImageNet)\n- Tesla Autopilot (via Karpathy connection)\n\nRetrieves documents for each discovered entity.",
        "code": "Knowledge Graph:\n[Stanford AI Lab]──created──[ImageNet]\n        │                        │\n    employs                  influenced\n        │                        │\n    [Fei-Fei Li]           [AlexNet]\n        │                        │\n     advised               enabled\n        │                        │\n    [Andrej Karpathy]      [CNN Revolution]\n        │\n     created\n        │\n    [Tesla Autopilot]",
        "codeLanguage": "python"
      },
      "tags": [
        "rag",
        "retrieval",
        "graph"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "rag-evaluation-overview",
      "name": "RAG Evaluation Overview",
      "parentId": "rag-evaluation",
      "sectionId": "23.11.1",
      "level": 2,
      "fullExplanation": "RAG evaluation assesses both retrieval and generation components. Retrieval metrics: precision, recall, MRR for finding relevant documents. Generation metrics: faithfulness (grounded in context), relevance (answers the question), correctness (factually accurate). End-to-end metrics evaluate the complete pipeline. Evaluation can be automated (LLM-as-judge) or human-based.",
      "simpleExplanation": "Measure if RAG is working well. Two parts: (1) Is retrieval finding the right documents? (2) Is generation using them correctly? Need metrics for both, plus overall \"did we answer the question correctly?\"",
      "example": {
        "description": "RAG Evaluation Framework:\n\n| Component | Metric | What it Measures |\n|"
      },
      "tags": [
        "rag",
        "retrieval",
        "evaluation",
        "overview"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "faithfulness-metric",
      "name": "Faithfulness",
      "parentId": "rag-evaluation",
      "sectionId": "23.11.2",
      "level": 2,
      "fullExplanation": "Faithfulness measures whether the generated response is factually consistent with the retrieved context—no hallucinated information beyond what's in the documents. Evaluation: decompose response into claims, verify each claim against context. Score is the proportion of claims supported by context. Critical metric for preventing hallucinations.",
      "simpleExplanation": "Does the answer stick to what the documents actually say? If the document says \"revenue was $100M\" but the answer says \"$150M,\" that's unfaithful. Every claim in the answer should be traceable to the retrieved context.",
      "example": {
        "description": "Retrieved context:\n\"Company X had revenue of $100M in 2023. They employ 500 people.\"\n\nGenerated answer:\n\"Company X generated $100M revenue in 2023 with 500 employees. \n They are the market leader.\"\n\nFaithfulness evaluation:\n- Claim 1: \"$100M revenue\" → In context ✓\n- Claim 2: \"500 employees\" → In context ✓  \n- Claim 3: \"market leader\" → NOT in context ✗\n\nFaithfulness score: 2/3 = 0.67"
      },
      "tags": [
        "rag",
        "retrieval",
        "faithfulness"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "answer-relevance-metric",
      "name": "Answer Relevance",
      "parentId": "rag-evaluation",
      "sectionId": "23.11.3",
      "level": 2,
      "fullExplanation": "Answer relevance measures how well the generated response addresses the user's question, regardless of factual accuracy. Evaluation: generate potential questions that the answer addresses, compare semantic similarity with original question. High relevance means the answer is on-topic and complete. Low relevance indicates tangential or incomplete responses.",
      "simpleExplanation": "Does the answer actually answer the question? Even if the answer is factually correct and grounded, if it doesn't address what was asked, it's not relevant. \"What time is it?\" answered with \"The sky is blue\" has zero relevance.",
      "example": {
        "description": "Question: \"What is the company's refund policy?\"\n\nAnswer A: \"Returns are accepted within 30 days for full refund. Items must be unopened.\"\nRelevance: HIGH (directly answers the question)\n\nAnswer B: \"The company was founded in 1990 in California.\"\nRelevance: LOW (doesn't address refund policy at all)\n\nAnswer C: \"For product inquiries, contact support@company.com\"\nRelevance: MEDIUM (related but doesn't answer the specific question)"
      },
      "tags": [
        "rag",
        "retrieval",
        "answer",
        "relevance"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "context-relevance-metric",
      "name": "Context Relevance",
      "parentId": "rag-evaluation",
      "sectionId": "23.11.4",
      "level": 2,
      "fullExplanation": "Context relevance evaluates whether retrieved documents are actually relevant to answering the query. Measures retrieval quality independent of generation. Irrelevant context wastes context window space and can confuse the generator. Evaluation: proportion of retrieved chunks that contain information useful for answering the question.",
      "simpleExplanation": "Are the retrieved documents actually useful? If you asked about refund policies but retrieval returned documents about company history, context relevance is low. Good retrieval = high context relevance.",
      "example": {
        "description": "Query: \"How to reset password?\"\n\nRetrieved documents:\n1. \"To reset your password, go to Settings > Security > Reset\" ✓ Relevant\n2. \"Password must be 8+ characters with numbers\" ✓ Relevant\n3. \"Company was founded in 2010\" ✗ Not relevant\n4. \"Our office is in San Francisco\" ✗ Not relevant\n\nContext Relevance: 2/4 = 0.50 (poor retrieval quality)"
      },
      "tags": [
        "rag",
        "retrieval",
        "context",
        "relevance"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "ragas",
      "name": "RAGAS Framework",
      "parentId": "rag-evaluation",
      "sectionId": "23.11.5",
      "level": 2,
      "fullExplanation": "RAGAS (Retrieval-Augmented Generation Assessment) is an open-source framework for evaluating RAG pipelines. It provides metrics for context relevance, faithfulness, answer relevance, and context recall. Uses LLM-as-judge for scalable automated evaluation. Enables systematic comparison of RAG configurations and continuous monitoring in production.",
      "simpleExplanation": "A toolkit for measuring RAG quality automatically. Instead of manually checking every answer, RAGAS uses AI to evaluate faithfulness, relevance, and correctness at scale. Industry standard for RAG evaluation.",
      "example": {
        "description": "",
        "code": "from ragas import evaluate\nfrom ragas.metrics import (\n    context_precision,\n    context_recall,\n    faithfulness,\n    answer_relevancy,\n)\nfrom datasets import Dataset\n\n# Prepare evaluation data\neval_data = {\n    \"question\": [\"What is the refund policy?\"],\n    \"answer\": [\"Refunds are available within 30 days...\"],\n    \"contexts\": [[\"Document about refund policy...\"]],\n    \"ground_truth\": [\"30-day refund for unopened items\"]\n}\ndataset = Dataset.from_dict(eval_data)\n\n# Evaluate\nresults = evaluate(\n    dataset,\n    metrics=[context_precision, context_recall, \n             faithfulness, answer_relevancy]\n)\n\nprint(results)\n# {'context_precision': 0.92, 'context_recall': 0.88,\n#  'faithfulness': 0.95, 'answer_relevancy': 0.90}",
        "codeLanguage": "python"
      },
      "tags": [
        "rag",
        "retrieval",
        "ragas",
        "framework"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "langchain",
      "name": "LangChain",
      "parentId": "rag-infrastructure",
      "sectionId": "23.12.1",
      "level": 2,
      "fullExplanation": "LangChain is a framework for building LLM-powered applications, including RAG systems. It provides abstractions for document loaders, text splitters, embedding models, vector stores, retrievers, and chains. Enables composable pipelines connecting these components. Extensive integrations with 100+ tools, models, and databases. Most popular framework for RAG development.",
      "simpleExplanation": "A toolbox for building AI applications. Instead of writing everything from scratch, LangChain gives you ready-made pieces: document loaders, embedding generators, vector database connectors. Snap them together like Lego blocks to build RAG systems quickly.",
      "example": {
        "description": "",
        "code": "from langchain_community.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import Chroma\nfrom langchain.chains import RetrievalQA\nfrom langchain_openai import ChatOpenAI\n\n# 1. Load documents\nloader = PyPDFLoader(\"company_docs.pdf\")\ndocuments = loader.load()\n\n# 2. Split into chunks\nsplitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nchunks = splitter.split_documents(documents)\n\n# 3. Create vector store\nembeddings = OpenAIEmbeddings()\nvectorstore = Chroma.from_documents(chunks, embeddings)\n\n# 4. Create RAG chain\nllm = ChatOpenAI(model=\"gpt-4\")\nqa_chain = RetrievalQA.from_chain_type(\n    llm=llm,\n    retriever=vectorstore.as_retriever(),\n    return_source_documents=True\n)\n\n# 5. Query\nresult = qa_chain.invoke({\"query\": \"What is the refund policy?\"})\nprint(result[\"result\"])",
        "codeLanguage": "python"
      },
      "tags": [
        "rag",
        "retrieval",
        "langchain"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "llamaindex",
      "name": "LlamaIndex",
      "parentId": "rag-infrastructure",
      "sectionId": "23.12.2",
      "level": 2,
      "fullExplanation": "LlamaIndex (formerly GPT Index) is a data framework for building RAG applications with emphasis on data indexing and retrieval strategies. Provides specialized index types (vector, keyword, tree, knowledge graph), advanced query engines, and tools for structured data. Focuses on sophisticated retrieval patterns and multi-document reasoning.",
      "simpleExplanation": "Another framework for building RAG, focused on smart ways to organize and search your data. Specializes in advanced retrieval strategies like combining multiple indexes, querying across different document types, and building knowledge graphs.",
      "example": {
        "description": "",
        "code": "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.core import Settings\n\n# Configure\nSettings.llm = OpenAI(model=\"gpt-4\")\nSettings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=50)\n\n# 1. Load documents\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\n\n# 2. Create index\nindex = VectorStoreIndex.from_documents(documents)\n\n# 3. Create query engine\nquery_engine = index.as_query_engine(\n    similarity_top_k=5,\n    response_mode=\"tree_summarize\"  # Advanced response synthesis\n)\n\n# 4. Query\nresponse = query_engine.query(\"What is the refund policy?\")\nprint(response)\nprint(response.source_nodes)  # Show sources",
        "codeLanguage": "python"
      },
      "tags": [
        "rag",
        "retrieval",
        "llamaindex"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    },
    {
      "id": "rag-production",
      "name": "RAG in Production",
      "parentId": "rag-infrastructure",
      "sectionId": "23.12.3",
      "level": 2,
      "fullExplanation": "Production RAG requires considerations beyond prototype: latency optimization (async operations, caching, model selection), cost management (embedding API costs, LLM token costs), reliability (error handling, fallbacks, monitoring), scalability (vector database scaling, load balancing), and observability (logging queries, evaluating responses, detecting drift).",
      "simpleExplanation": "Making RAG work in the real world. Prototypes are easy; production is hard. Need to think about: speed (users won't wait), cost (API bills add up), reliability (can't crash), scale (handle many users), and monitoring (know when things break).",
      "example": {
        "description": "Production RAG Checklist:\n\n| Aspect | Considerations |\n|"
      },
      "tags": [
        "rag",
        "retrieval",
        "production"
      ],
      "relatedConcepts": [],
      "prerequisites": [],
      "difficulty": "advanced"
    }
  ]
}